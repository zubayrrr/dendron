{"pageProps":{"note":{"id":"f2kecna72pmc7re3wh1ugk4","title":"Apache Spark","desc":"","updated":1653318643716,"created":20211115133856116,"tags":["areas"],"custom":{},"fname":"devlog.apache spark","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"c230bd195def8a1291232025ba41ab16","links":[{"type":"frontmatterTag","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"tags.areas","alias":"areas","xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.Data Engineering","alias":"devlog.Data Engineering","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":37,"offset":37},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Data Engineering"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":65,"column":5,"offset":1878},"end":{"line":65,"column":22,"offset":1895},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":69,"column":25,"offset":1938},"end":{"line":69,"column":45,"offset":1958},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":70,"column":70,"offset":2047},"end":{"line":70,"column":85,"offset":2062},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":70,"column":105,"offset":2082},"end":{"line":70,"column":120,"offset":2097},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Kerberos","alias":"Kerberos","position":{"start":{"line":72,"column":19,"offset":2246},"end":{"line":72,"column":31,"offset":2258},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Kerberos"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Tez","alias":"Tez","position":{"start":{"line":76,"column":86,"offset":2398},"end":{"line":76,"column":93,"offset":2405},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Tez"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.scala","alias":"devlog.scala","position":{"start":{"line":78,"column":56,"offset":2578},"end":{"line":78,"column":72,"offset":2594},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.scala"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.yarn","alias":"devlog.yarn","position":{"start":{"line":79,"column":73,"offset":2668},"end":{"line":79,"column":88,"offset":2683},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.yarn"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Kerberos","alias":"Kerberos","position":{"start":{"line":79,"column":118,"offset":2713},"end":{"line":79,"column":130,"offset":2725},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Kerberos"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":85,"column":18,"offset":2782},"end":{"line":85,"column":32,"offset":2796},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":90,"column":36,"offset":3421},"end":{"line":90,"column":50,"offset":3435},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":98,"column":22,"offset":3931},"end":{"line":98,"column":39,"offset":3948},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark Core","alias":"Spark Core","position":{"start":{"line":104,"column":1,"offset":4133},"end":{"line":104,"column":15,"offset":4147},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark Core"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":106,"column":12,"offset":4160},"end":{"line":106,"column":26,"offset":4174},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"DAG","alias":"DAG","position":{"start":{"line":108,"column":11,"offset":4230},"end":{"line":108,"column":18,"offset":4237},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DAG"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark SQL","alias":"Spark SQL","position":{"start":{"line":110,"column":1,"offset":4239},"end":{"line":110,"column":14,"offset":4252},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark SQL"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark Streaming","alias":"Spark Streaming","position":{"start":{"line":115,"column":1,"offset":4428},"end":{"line":115,"column":20,"offset":4447},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark Streaming"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark MLlib","alias":"Spark MLlib","position":{"start":{"line":119,"column":1,"offset":4626},"end":{"line":119,"column":16,"offset":4641},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark MLlib"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"GraphX","alias":"GraphX","position":{"start":{"line":123,"column":1,"offset":4773},"end":{"line":123,"column":11,"offset":4783},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"GraphX"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.yarn","alias":"devlog.yarn","position":{"start":{"line":131,"column":74,"offset":5300},"end":{"line":131,"column":89,"offset":5315},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.yarn"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":147,"column":9,"offset":5983},"end":{"line":147,"column":23,"offset":5997},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":155,"column":5,"offset":6189},"end":{"line":155,"column":19,"offset":6203},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":159,"column":77,"offset":6313},"end":{"line":159,"column":92,"offset":6328},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"DAG","alias":"DAG","position":{"start":{"line":183,"column":11,"offset":8037},"end":{"line":183,"column":18,"offset":8044},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DAG"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"DAG","alias":"DAG","position":{"start":{"line":185,"column":5,"offset":8050},"end":{"line":185,"column":12,"offset":8057},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DAG"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":191,"column":123,"offset":8334},"end":{"line":191,"column":143,"offset":8354},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":193,"column":181,"offset":8564},"end":{"line":193,"column":195,"offset":8578},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark Cluster Managers","alias":"Spark Cluster Managers","position":{"start":{"line":199,"column":11,"offset":8990},"end":{"line":199,"column":37,"offset":9016},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark Cluster Managers"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":205,"column":29,"offset":9155},"end":{"line":205,"column":46,"offset":9172},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":207,"column":12,"offset":9185},"end":{"line":207,"column":29,"offset":9202},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Slicing in Spark","alias":"Slicing in Spark","position":{"start":{"line":207,"column":33,"offset":9206},"end":{"line":207,"column":53,"offset":9226},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Slicing in Spark"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"DataFrame","alias":"DataFrame","position":{"start":{"line":212,"column":4,"offset":9321},"end":{"line":212,"column":17,"offset":9334},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DataFrame"}},{"from":{"fname":"devlog.big data","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":127,"column":5,"offset":6045},"end":{"line":127,"column":28,"offset":6068},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.dag","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.data engineering roadmap","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":31,"column":3,"offset":998},"end":{"line":31,"column":26,"offset":1021},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.dataframe","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.mapreduce examples","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":93,"column":23,"offset":4115},"end":{"line":93,"column":46,"offset":4138},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.mapreduce","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":192,"column":23,"offset":8478},"end":{"line":192,"column":46,"offset":8501},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.mapreduce","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":289,"column":20,"offset":11861},"end":{"line":289,"column":43,"offset":11884},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.maven scala spark","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.rdd","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.slicing in spark","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark architecture","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark cluster managers","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark core","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark datasets","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark misc","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark mllib","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark sql","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark streaming","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"}],"anchors":{"context-of-big-data":{"type":"header","text":"Context of Big Data","value":"context-of-big-data","line":14,"column":0,"depth":3},"motivation-for-spark":{"type":"header","text":"Motivation for Spark","value":"motivation-for-spark","line":28,"column":0,"depth":3},"spark-first-principles":{"type":"header","text":"Spark First Principles","value":"spark-first-principles","line":52,"column":0,"depth":2},"devloghadoop-vs-spark":{"type":"header","text":"devlog.hadoop vs Spark","value":"devloghadoop-vs-spark","line":73,"column":0,"depth":3},"spark-features":{"type":"header","text":"Spark Features","value":"spark-features","line":89,"column":0,"depth":3},"spark-misconceptions":{"type":"header","text":"Spark Misconceptions","value":"spark-misconceptions","line":102,"column":0,"depth":3},"components-of-apache-spark":{"type":"header","text":"Components of Apache Spark","value":"components-of-apache-spark","line":108,"column":0,"depth":2},"spark-architecture":{"type":"header","text":"Spark Architecture","value":"spark-architecture","line":135,"column":0,"depth":2},"devlogrdd":{"type":"header","text":"devlog.rdd","value":"devlogrdd","line":163,"column":0,"depth":3},"transformation":{"type":"header","text":"Transformation","value":"transformation","line":179,"column":0,"depth":3},"actions":{"type":"header","text":"Actions","value":"actions","line":186,"column":0,"depth":3},"dag":{"type":"header","text":"DAG","value":"dag","line":193,"column":0,"depth":3},"spark-data-slicing":{"type":"header","text":"Spark Data Slicing","value":"spark-data-slicing","line":209,"column":0,"depth":2},"dataframe":{"type":"header","text":"DataFrame","value":"dataframe","line":220,"column":0,"depth":2},"how-dataframes-work":{"type":"header","text":"How DataFrames Work","value":"how-dataframes-work","line":252,"column":0,"depth":3},"need-to-be-distributed":{"type":"header","text":"Need to be distributed","value":"need-to-be-distributed","line":279,"column":0,"depth":3},"partitioning":{"type":"header","text":"Partitioning","value":"partitioning","line":283,"column":0,"depth":3},"transformations":{"type":"header","text":"Transformations","value":"transformations","line":290,"column":0,"depth":3},"computing-dataframes":{"type":"header","text":"Computing DataFrames","value":"computing-dataframes","line":299,"column":0,"depth":3},"transformations-vs-actions":{"type":"header","text":"Transformations vs Actions","value":"transformations-vs-actions","line":314,"column":0,"depth":3},"joins":{"type":"header","text":"Joins","value":"joins","line":323,"column":0,"depth":2}},"children":[],"parent":"9gtn7g40cvqui0sifl1s7t5","data":{}},"body":"<h1 id=\"apache-spark\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#apache-spark\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Apache Spark</h1>\n<ul>\n<li>Areas: <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">devlog.Data Engineering (Private)</a></li>\n</ul>\n<hr>\n<h3 id=\"context-of-big-data\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#context-of-big-data\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Context of Big Data</h3>\n<p>Computing vs Data</p>\n<ul>\n<li>CPUs are only incrementally faster</li>\n<li>Data storage keeps getting better and cheaper</li>\n<li>Gathering data kees getting easier and cheaper and more important</li>\n</ul>\n<p>Data needs to be distributed and processed in parallel</p>\n<p>Standard single-CPU software cannot scale up</p>\n<p>Thus Spark was born.</p>\n<h3 id=\"motivation-for-spark\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#motivation-for-spark\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Motivation for Spark</h3>\n<p>A 2009 UC Berkeley project Matei Zaharia el al</p>\n<ul>\n<li>MapReduce was the king of large distributed computation</li>\n<li>Inefficient for large applications and ML</li>\n<li>Each step required another data pass, written as separate application</li>\n</ul>\n<p>Spark phase 1</p>\n<ul>\n<li>A simple functional programming API</li>\n<li>Optimize multi-step applications</li>\n<li>In-memory computation and data-sharing aross nodes</li>\n</ul>\n<p>Spark phase 2</p>\n<ul>\n<li>Interactive data science and ad-hoc computation</li>\n<li>Spark shell and Spark SQL</li>\n</ul>\n<p>Spark phase 3</p>\n<ul>\n<li>Same engine, new libraries</li>\n<li>ML, Streaming, GraphX</li>\n</ul>\n<h2 id=\"spark-first-principles\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-first-principles\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Spark First Principles</h2>\n<p>Spark is unified computing engine and libraries for distributed data processing at scale.</p>\n<p>It is an open source data processing engine(in-memory computing engine) to store and process data in real-time across various clusters of computers using simple programming constructs.</p>\n<p>Big data = data that cannot fit on a standard computer, you'll need a cluster of computers that can process that data.(and Spark was made for this specific task)</p>\n<ul>\n<li>Spark supports a variety of data processing tasks\n<ul>\n<li>data loading</li>\n<li>SQL queries</li>\n<li>machine learning</li>\n<li>streaming</li>\n</ul>\n</li>\n<li>Unified in the sense:\n<ul>\n<li>consistent, composable APIs in multiple languages</li>\n<li>optimizations across different libraries</li>\n</ul>\n</li>\n<li>Computig engine in the sense that it is detached from data storage(where the data resides) and how the data is being fetched(I/O)</li>\n<li>Libraries\n<ul>\n<li>standard: Spark SQL, MLlib, Streaming, GraphX</li>\n<li>hundres of open-source third party libraries</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"hadoop-vs-spark\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#hadoop-vs-spark\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a><a href=\"/notes/85w31vcdf3bjnm0yxh72ygf\">Hadoop</a> vs Spark</h3>\n<p>Hadoop</p>\n<ul>\n<li>Processing data using <a href=\"/notes/j6kfx6ziqg91r356nur9cwy\">MapReduce</a> in hadoop is slow.</li>\n<li>Performs batch processing of data, intermittent data is written to <a href=\"/notes/kdddo1f7ltfsuwhexj4s535\">HDFS</a> and read back from <a href=\"/notes/kdddo1f7ltfsuwhexj4s535\">HDFS</a> which makes hadoop's MapReduce slow.</li>\n<li>hadoop has more lines of code. Since it is written in Java, it takes more time to execute.</li>\n<li>hadoop supports <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Kerberos (Private)</a> authentication, which is difficult to manage.</li>\n</ul>\n<p>Spark</p>\n<ul>\n<li>Spark processes data 100 times faster than MapReduce as it is done in-memory (like <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Tez (Private)</a>).</li>\n<li>Performs both batch processing and real-time processing of data, most use cases are around real-time processing.</li>\n<li>Spark hs fewer lines of code as it is implemented in <a href=\"/notes/caqic1zdwxl1ll1ogg1ec1f\">Scala</a>.</li>\n<li>Spark supports authentication via a shared secret. It can also run on <a href=\"/notes/6k7g4x7ws565lditv1n8hxm\">YARN</a> leveraging the capability of <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Kerberos (Private)</a>.</li>\n</ul>\n<h3 id=\"spark-features\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-features\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Spark Features</h3>\n<p>Fast processing</p>\n<ul>\n<li>Spark contains <a href=\"/notes/2vn2g9mhmi624b83rl5se0k\">RDD</a>s which saves time taken in reading and writing operations and hence, it runs almost ten to hundred times faster than hadoop.</li>\n<li>In-memory computing - In Spark data is stored in the RAM, so it can access the data quickly and accelerate the speed of analytics.\n<ul>\n<li>Caching is different from In-memory computing, caching is mainly to support \"read ahead\" mechanism, where you have your data preloaded so that it can benefit further queries.</li>\n<li>In-memory computing is more about lazy evaluation, data being loaded in memory only and only if a specific action is invoked.</li>\n</ul>\n</li>\n<li>Flexible - polyglot</li>\n<li>Fault tolerance - Spark contains <a href=\"/notes/2vn2g9mhmi624b83rl5se0k\">RDD</a>s (execution logic, temporary datasets which initially do not have any data loaded and data will only be loaded into RDDs when an execution is made) that are designed to handle the failure of any worker node in the cluster. Thus, it ensures that the loss of data reduces to zero.\n<ul>\n<li>Distributed</li>\n</ul>\n</li>\n<li>Better analytics - Spark has a rich set of SQL queries, machine learning algorithms, complex analytics etc.</li>\n</ul>\n<h3 id=\"spark-misconceptions\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-misconceptions\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Spark Misconceptions</h3>\n<p>Spark is not concerned with data sources</p>\n<p>Spark is not part of <a href=\"/notes/85w31vcdf3bjnm0yxh72ygf\">Hadoop</a>, it interacts with Hadoop and HDFS well but it is it's own thing.</p>\n<h2 id=\"components-of-apache-spark\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#components-of-apache-spark\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Components of Apache Spark</h2>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.cct1xwxfey9.png\"></p>\n<p><a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Spark Core (Private)</a></p>\n<ul>\n<li>Contains <a href=\"/notes/2vn2g9mhmi624b83rl5se0k\">RDD</a>s; core engine that takes care of processing</li>\n</ul>\n<p>See also: <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">DAG (Private)</a></p>\n<p><a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Spark SQL (Private)</a></p>\n<ul>\n<li>For working with structured data or data that can be structurized.</li>\n<li>Has internal features such as Dataframes, datasets used to process structued data in much faster way.</li>\n</ul>\n<p><a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Spark Streaming (Private)</a></p>\n<ul>\n<li>Allows you to create Spark Streaming applications which not only works on data that is being streamed/generated in but also transform the data and analyze it as it comes in.</li>\n</ul>\n<p><a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Spark MLlib (Private)</a></p>\n<ul>\n<li>For building machine learning algorithms, predictive analytics, perscriptive, descriptive, preemptive, recommendation systems.</li>\n</ul>\n<p><a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">GraphX (Private)</a></p>\n<ul>\n<li>Data naturally has a network kind of flow, data that be represented with graphs(not really pie charts) but network related data, some kind of relationships, twitter, fb, linkedin etc.</li>\n</ul>\n<h2 id=\"spark-architecture\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-architecture\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Spark Architecture</h2>\n<p>Apache Spark uses a master-slave(technically, although it can also run in standalone mode) architecture that consists of a driver, that runs on a master node and multiple executors which run across the worker nodes in the cluster.</p>\n<p>It can work with different clustering technologies such as Apache Mesos, <a href=\"/notes/6k7g4x7ws565lditv1n8hxm\">YARN</a>. It can also work as standalone.</p>\n<p>Master Node has a driver program; this driver program internally has SparkContext.</p>\n<p>The Spark code behaves as a driver program and creates a SparkContext, which is gateway to all the Spark functionalities.</p>\n<p>Driver program interacts with cluster manager(SparkContext, the entry point, takes the request to the cluster manager).</p>\n<p>Cluster manager in terms of YARN is the <code>ResourceManager</code></p>\n<p>Spark application runs as independent set of processes on a cluster.</p>\n<p>The driver program and SparkContext takes care of the job execution within the cluster.</p>\n<p>A job is split into multiple tasks that are distributed over the worker node.</p>\n<p>When an <a href=\"/notes/2vn2g9mhmi624b83rl5se0k\">RDD</a> is created in SparkContext, it can be distributed across various nodes.</p>\n<p>Worker nodes are slaves that run different tasks.</p>\n<hr>\n<p>Spark Architecture is based on 2 important abstractions:</p>\n<h3 id=\"rdd\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#rdd\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a><a href=\"/notes/2vn2g9mhmi624b83rl5se0k\">RDD</a></h3>\n<p>Resilient Distributed Datasets</p>\n<p>Spark Core is embedded with RDDs, an immutable fault-tolerant(like files in <a href=\"/notes/kdddo1f7ltfsuwhexj4s535\">HDFS</a>), distributed collection of objects that can be operated on in parallel.</p>\n<p>It where the data will be loaded(or existing for processing), it can exist for shorter amount of time.</p>\n<ul>\n<li>RDDs are the fundamental units of data in Apache Spark that are split into partitions and can be executed on different nodes of a cluster. Implicit, lazy in nature, created whenever you use a method of SparkContext or when you do a transformation on an existing RDD or a dataset.</li>\n<li>Each dataset in an RDD is divided into logical memory partitions that may be computed on different nodes of a cluster.</li>\n<li>By default every RDD has 2 partitions, which can be customized while creating RDDs.</li>\n<li>The more partitions you've the better the parallel processing.</li>\n<li>RDDs are automatically split into partitions and can be executed upon different nodes by different taks in parallel, in-memory.</li>\n</ul>\n<p>There are mainly two operations that can be peformed on an RDD</p>\n<h3 id=\"transformation\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#transformation\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Transformation</h3>\n<ul>\n<li>These are operations (such as map, filter, join, union) that are peformed on an RDD that yields as new RDD containing the result.</li>\n<li>They return a pointer to a new RDD. The original RDD cannot be changed. Spark is \"lazy\" and nothing will be executed unless an action is invoked.</li>\n<li>It isn't necessarily reproducing a new set of data or RDD, but is a new \"state\". Think of it as step(s) in a program telling Spark how to get new data and what to do with it.</li>\n<li><strong>Resilience</strong> is the ability to retrace steps from the beginning.</li>\n</ul>\n<h3 id=\"actions\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#actions\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Actions</h3>\n<ul>\n<li>These are operations (reduce, first, count, collect, count, take save-as) that return a value after running a computation on an RDD.</li>\n<li>They return values and force the transformations to actually take place.</li>\n</ul>\n<p>See also: <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">DAG (Private)</a></p>\n<h3 id=\"dag-private\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#dag-private\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a><a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">DAG (Private)</a></h3>\n<p>Directed Acyclic Graph</p>\n<p>Spark has a DAG scheduler in the background which is tracking the transformations and steps which is part of your applications.</p>\n<p>It is the scheduling layer of the Spark Architecture that implements stage-oriented scheduling and eliminates that hadoop <a href=\"/notes/j6kfx6ziqg91r356nur9cwy\">MapReduce</a> multistage execution model.</p>\n<p>If you're processing data using Spark applications in a Spark platform(whether standalone or in a hadoop cluster), for every step you do as part of your application, it creates an <a href=\"/notes/2vn2g9mhmi624b83rl5se0k\">RDD</a>, it becomes part of your DAG. The DAG scheduler is already aware of what steps are involved in DAG hence it comes up with a plan for executing the all the steps within the DAG. <span class=\"underline\">If only and only if an action is invoked.</span></p>\n<p>Check <a href=\"https://data-flair.training/blogs/dag-in-apache-spark/\">https://data-flair.training/blogs/dag-in-apache-spark/</a></p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.muzlc82nhqb.png\"></p>\n<p>See also: <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Spark Cluster Managers (Private)</a></p>\n<h2 id=\"spark-data-slicing\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-data-slicing\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Spark Data Slicing</h2>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.4ylqh1r70re.png\"></p>\n<p>in contrast with Slicing in <a href=\"/notes/85w31vcdf3bjnm0yxh72ygf\">Hadoop</a></p>\n<p>Slicing in <a href=\"/notes/85w31vcdf3bjnm0yxh72ygf\">Hadoop</a> vs <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Slicing in Spark (Private)</a>\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.83j55a3bdv5.png\"></p>\n<hr>\n<h2 id=\"dataframe-private\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#dataframe-private\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a><a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">DataFrame (Private)</a></h2>\n<p>DataFrame is a distributed collection of data organized into named columns. ... DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.</p>\n<p>Created a SparkSession(it allows creating, reading and writing DataFrames)</p>\n<pre><code>val spark = SparkSession.builder()\n    .appName(\"My Application\")\n    .config(\"spark.master\", \"local\")\n    .getOrCreate()\n</code></pre>\n<p>Reading a DataFrame from a file</p>\n<pre><code>var firstDF = spark.read()\n    .format(\"json\")\n    .option(\"inferSchema\", \"true\")\n    .load(\"path/to/file.json\")\n</code></pre>\n<p>Performing operations on a DataFrame</p>\n<pre><code>firstDF.show()\nfirstDF.printSchema()\nfirstDF.take(5)\n</code></pre>\n<p>Rows = unstructured data; and the information about the structure of the data is applied to the DataFrame in the form of a Schema</p>\n<p>Schema = description of fields aka columns and their type</p>\n<p>Spark types, they spark at runtime rather than compile time.</p>\n<hr>\n<h3 id=\"how-dataframes-work\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#how-dataframes-work\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>How DataFrames Work</h3>\n<p>Distributed spreadsheets with rows and columns, like a table that is split between multiple nodes in a Spark cluster, the information each Spark node recieves is the schema of the DataFrame anda few of the rows that compose the DataFrame.</p>\n<p>Distributed collections of Rows conforming to a schema</p>\n<p>DataFrames are:</p>\n<ul>\n<li>Immutable\n<ul>\n<li>Can't be changed once created</li>\n<li>If you want to modify them you will have to create new DataFrames using transformations</li>\n</ul>\n</li>\n</ul>\n<p>Schema = list describing the column names and types</p>\n<ul>\n<li>Types are known to Spark when the DataFrame is being used, not at compile time(to make them available at compile time with Type safe Datasets)</li>\n<li>Schema can hold arbitrary number of columns</li>\n<li>All rows have the same structure</li>\n<li>Rows do not have schema but they conform to the same structure</li>\n</ul>\n<!-- end list -->\n<pre><code>val carsSchema = StructType(Array(\n    StructField(\"Name\", StringType),\n    StructField(\"HorsePower\", IntegerType),\n    StructField(\"Acceleration\", DoubleType)\n))\n</code></pre>\n<h3 id=\"need-to-be-distributed\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#need-to-be-distributed\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Need to be distributed</h3>\n<p>These collections of rows need to be distributed, because either the data is too big for a single computer or it takes too long to process entier data on a single CPU.</p>\n<h3 id=\"partitioning\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#partitioning\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Partitioning</h3>\n<ul>\n<li>Splits the data into files, distributed between nodes in the cluster</li>\n<li>Impacts the processing parallelism</li>\n<li>More partitions may mean more parallelism but if you have 1000 partitions (1000 small files that compose your DataFrame) and a single node to process them all, parallelism will still be one because you only have one node to process all that data.</li>\n<li>Inversely, if you have 1 partition and many nodes in your Spark cluster, only one node will have access to that partition and the parallelism would still be one.</li>\n</ul>\n<h3 id=\"transformations\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#transformations\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Transformations</h3>\n<ul>\n<li>Narrow = one input partition contributes to at most one output partition(e.g map), partitioning is not changed in a DataFrame.\n<ul>\n<li>If you do a map, that would transform data row by row and so the partitioning is not changed, whereas</li>\n</ul>\n</li>\n<li>Wide = input partitions(one or more) create many output partitions, so the partitioning of the DataFrame is changed.\n<ul>\n<li>If you do a sort, that will involve exchanging data between partitions in between nodes in the cluster.</li>\n</ul>\n</li>\n<li>These operations are known as Shuffle = data exchange between cluster nodes.\n<ul>\n<li>Shuffling occurs in Wide transformations and its a massive performance topic, it can impact the time it takes for your jobs by orders of magnitude.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"computing-dataframes\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computing-dataframes\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Computing DataFrames</h3>\n<p>How DataFrames work at runtime</p>\n<p>Lazy evaluation</p>\n<ul>\n<li>Spark mechanism to wait until the last moment to execute the DF transformations</li>\n</ul>\n<p>Planning</p>\n<ul>\n<li>Spark compiles the DF transformations and dependencies into a graph before running any code, Spark will know before hand every single step that it will have to take including data exchanges between nodes before it actually starts loading or running any code.</li>\n<li>Logical plan = DF dependency graph + narrow/wide transformations sequence</li>\n<li>Physical plan = optimized seqence of steps(and it will know which node will execute which part of transformations) for nodes in the cluster.</li>\n<li>Optimizations such as: avoiding multiple passes over the data or pushing down predecates in SQL or chaining multiple predecates or where clauses into one and so on.</li>\n</ul>\n<h3 id=\"transformations-vs-actions\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#transformations-vs-actions\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Transformations vs Actions</h3>\n<ul>\n<li>A transformation descibes how new DFs are obtained (e.g map)</li>\n<li>Action actually starts executing Spark code (e.g show, count)</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.rv3o8j0t3m.png\"></p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.mymxbbtheoj.png\"></p>\n<h2 id=\"joins\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#joins\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Joins</h2>\n<p>Combine data from multiple DataFrames</p>\n<ul>\n<li>one(or more) columns from table 1 (left) is compared with one(or more) columns from table 2 (right)</li>\n<li>if the condition passes, rows are combined</li>\n<li>non-matching rows are distracted</li>\n</ul>\n<p>In Spark, Joins are Wide transformations(read: expensive!)\nIn order to compute a join, Spark scans the entire DFs from the entire clusters and the data is going to be moved around in between nodes, this involves shuffling which is expensive (in terms of performance).</p>\n<hr>\n<h2 id=\"tags\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#tags\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Tags</h2>\n<ol>\n<li><a class=\"color-tag\" style=\"--tag-color: #b79400;\" href=\"/notes/e2nal5eqj2mu8d029dnoi5c\">areas</a></li>\n</ol>\n<hr>\n<strong>Backlinks</strong>\n<ul>\n<li><a href=\"/notes/s5t9uscswvpo3p6gebui1ea\">Big Data</a></li>\n<li><a href=\"/notes/bojmgq30yfxpn44hgo0ofcf\">DAG</a></li>\n<li><a href=\"/notes/2fz1tdl3yy2s78wdmo7ql56\">Data Engineering Roadmap</a></li>\n<li><a href=\"/notes/q9l2kjgosndymzvlre2b3zs\">DataFrame</a></li>\n<li><a href=\"/notes/rttao2ttrcnyce4qro7cku2\">MapReduce Examples</a></li>\n<li><a href=\"/notes/j6kfx6ziqg91r356nur9cwy\">MapReduce</a></li>\n<li><a href=\"/notes/c9r2joc5eh6lgdxcf1tl2xh\">Maven Scala Spark</a></li>\n<li><a href=\"/notes/2vn2g9mhmi624b83rl5se0k\">RDD</a></li>\n<li><a href=\"/notes/88u97ancbt2rjk1r522bxt3\">Slicing in Spark</a></li>\n<li><a href=\"/notes/iholoz516z4p3yeo29eldm4\">Spark Architecture</a></li>\n<li><a href=\"/notes/llkslspv5gn64h1n4w6hzuc\">Spark Cluster Managers</a></li>\n<li><a href=\"/notes/uv0m9j6mm23y7zn5kte1avo\">Spark Core</a></li>\n<li><a href=\"/notes/mtez5e789o0yozjrbt75wua\">Spark Datasets</a></li>\n<li><a href=\"/notes/h0jp44pm8khupz9s02ebax4\">Spark Misc</a></li>\n<li><a href=\"/notes/wfzzq8sg88lzu3a64vpqa4p\">Spark MLlib</a></li>\n<li><a href=\"/notes/4umcol9f5n339vdmatdkrtf\">Spark SQL</a></li>\n<li><a href=\"/notes/x5x4p2nl61shtnu9ogxjrg9\">Spark Streaming</a></li>\n</ul>","noteIndex":{"id":"3nfl4nvv516muyzozhcwrw8","title":"/root","desc":"","updated":1655559901157,"created":1637610830605,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"581715455a6f0f7a699209e8521b4acf","links":[{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"my","position":{"start":{"line":4,"column":9,"offset":37},"end":{"line":4,"column":29,"offset":57},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":20,"column":111,"offset":1051},"end":{"line":20,"column":117,"offset":1057},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":21,"column":3,"offset":1198},"end":{"line":21,"column":9,"offset":1204},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes","alias":"swipes","position":{"start":{"line":27,"column":3,"offset":1724},"end":{"line":27,"column":13,"offset":1734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.quotes","alias":"quotes","position":{"start":{"line":27,"column":48,"offset":1769},"end":{"line":27,"column":72,"offset":1793},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.quotes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.excerpts","alias":"excerpts","position":{"start":{"line":27,"column":74,"offset":1795},"end":{"line":27,"column":102,"offset":1823},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.excerpts"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.sayings","alias":"sayings","position":{"start":{"line":27,"column":104,"offset":1825},"end":{"line":27,"column":130,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.sayings"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.phrases","alias":"phrases","position":{"start":{"line":27,"column":132,"offset":1853},"end":{"line":27,"column":158,"offset":1879},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.phrases"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"resources.people","alias":"others","position":{"start":{"line":27,"column":214,"offset":1935},"end":{"line":27,"column":241,"offset":1962},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"resources.people"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"inbox.webmark","alias":"webmark","position":{"start":{"line":31,"column":235,"offset":2463},"end":{"line":31,"column":260,"offset":2488},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"inbox.webmark"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"slipbox.Ontology","alias":"slipbox.Ontology","position":{"start":{"line":55,"column":3,"offset":3735},"end":{"line":55,"column":23,"offset":3755},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"slipbox.Ontology"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"About me","position":{"start":{"line":60,"column":3,"offset":3963},"end":{"line":60,"column":29,"offset":3989},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}}],"anchors":{"welcome-to-noetic-noggin":{"type":"header","text":"Welcome to Noetic Noggin","value":"welcome-to-noetic-noggin","line":8,"column":0,"depth":1},"principles":{"type":"header","text":"Principles","value":"principles","line":18,"column":0,"depth":2},"all-notes-should-be-relative-to-me":{"type":"header","text":"All notes should be relative to me.","value":"all-notes-should-be-relative-to-me","line":20,"column":0,"depth":3},"gotta-capture-em-all":{"type":"header","text":"Gotta capture 'em all","value":"gotta-capture-em-all","line":30,"column":0,"depth":3},"dont-force-evolution":{"type":"header","text":"Don't force evolution","value":"dont-force-evolution","line":40,"column":0,"depth":3},"noise--signal":{"type":"header","text":"Noise & Signal","value":"noise--signal","line":45,"column":0,"depth":3},"why-do-any-of-this":{"type":"header","text":"Why do any of this?","value":"why-do-any-of-this","line":50,"column":0,"depth":3},"structure-of-this-wiki":{"type":"header","text":"Structure of this wiki","value":"structure-of-this-wiki","line":59,"column":0,"depth":2},"quicklinks":{"type":"header","text":"Quicklinks","value":"quicklinks","line":64,"column":0,"depth":2}},"children":["0yay2om15bsg2li2p6qgux7","05c4nnjqa92zx11ld6o0ytn","9gtn7g40cvqui0sifl1s7t5","ftbd1hknsd3ocd7jao26tn3","a1kmkdbpclaz5p6sykaw6kc","z121gkmqfo09m8r7jgnpfgn","gkqrr7xbt18xhi93dmjrwzj","ja2x4lrgejr9o9wvit0bd0d","luv39odkfibx3wdosvigwvy","vtvk3bi6o72w58oima9xzf3","yy652kvqrkfn9ipk07m40h4"],"parent":null,"data":{},"body":"\n# Welcome to Noetic Noggin\n\nThis is [[my|archive.about]] personal wiki and a commonplace book; notes by me, for me.\n\nðŸš§ Permanently under construction ðŸš§\n\n![](https://res.cloudinary.com/zubayr/image/upload/v1658499909/wiki/ajevkuyebljlxiblyst2.png)\n\nThis wiki was made possible with [dendron.so](https://dendron.so) and [obisidian.md](https://obsidian.md). Stored on [Github Repository](https://github.com/zubayrrr/dendron) and hosted on [Netlify](https://netlify.com) for free.\n\n## Principles\n\n### All notes should be relative to me.\n\n- All notes in principle are written for me; what I know about a subject, how I feel about a particular thing.\n- Opinions are fine as long as I feel strong epistemic confidence in the given opinion.\n- Don't over explain a note if it's not necessary, remember, these notes are for you and are relative to whatever knowledge you posses about the subject.\n- Read books and make an dedicated notes for them.\n- Listen podcasts but capture them inside a \"subject specific\" note or \"Map of Concept\" note or a note tagged #areas. Because making notes from podcasts can be tedious as they're not as well structured as books for consumption.(Whose merit is debatable.)\n- #areas are basically \"Map of Concept\" notes but I have recently come to the realization that its better to maintain them [Nikita Voloboev style](https://wiki.nikiv.dev/) but with heavy usage of transclusion and backlinking.\n  - \"Resources\" should be first processed and then mentioned inside the note, otherwise they should be left in inbox.\n\n### Gotta capture 'em all\n\n- Hog whatever information tickles your pickle([anything that gratifies one's intellectual curiosity](https://news.ycombinator.com/newsguidelines.html)).\n- [[swipes]] are interesting/useful bits of... [[quotes|swipes.quotes]], [[excerpts|swipes.excerpts]], [[sayings|swipes.sayings]], [[phrases|swipes.phrases]]. Essentially, ideas, opinions that are swiped off from [[others|resources.people]].\n- Make no distinction between \"your\" ideas and ideas of \"others\", because if you vibe with an idea; it's already yours.\n- But also remember \"If you've time to consume, you've time to produce.\".\n- Use [raindrop.io](https://raindrop.io) to manage your URL bookmarks.\n- If you need to bookmark a webpage or an article all together, use [MarkDownload](https://chrome.google.com/webstore/detail/markdownload-markdown-web/pcmpcfapbekmbjjkdalcgopdkipoggdi?hl=en-GB) to rip the entire page. Let's call it a [[webmark|inbox.webmark]]; it belong in the `/inbox`.\n- Similar process is employed for capturing tweets using [tweet-to-markdown](https://github.com/kbravh/tweet-to-markdown) and it also belongs in the `/inbox`.\n\n### Don't force evolution\n\n- Let your second brain evolve at it's own pace.\n- The structure should never be _too_ rigid because its meant to take form by itself.\n\n### Noise & Signal\n\n- While capturing ideas left and right is recommended, make sure you're not harming your periods of focus.\n- Have impenetrable focus periods (use Pomodoro method) where you only care about the work on hand and nothing else.\n\n### Why do any of this?\n\n- Because I can't remember everything - there's a lot of information around that interests me and there isn't enough working memory installed in me.\n- So, I make notes - to remember, to create, to meditate, to think.\n- A bodybuilder's portfolio is their body - my portfolio is my wiki.\n- Not only am I making - whatever I know - tangible by writing it down. I know exactly where to look if I ever forget something.\n- I am at the beginning of my learning adventures. When I look back at it, I will know where I came from and how my thoughts evolved over time.\n- Plus, its really fun to nerd out.\n\n## Structure of this wiki\n\n- [[slipbox.Ontology]] explains the structure of this wiki and the tags, backlinks used in it.\n- Dendron takes care of the structure and hierarchy(mostly), but I insist on using tags for backwards compatibility.\n\n## Quicklinks\n\n- [[About me|archive.about]]\n- [Github](https://github.com/zubayrrr)\n- [Twitter](https://twitter.com/zoobhalu)\n- [Blog](https://zubayrali.in)\n- [Guestbook](https://www.yourworldoftext.com/~zubayrali/)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template","insertNote":{"initialValue":"templates"}},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Noetic Noggin","description":"Personal Wiki / Digital Garden","author":"Zubayr Ali","twitter":"zoobhalu"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteUrl":"localhost:3000","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}