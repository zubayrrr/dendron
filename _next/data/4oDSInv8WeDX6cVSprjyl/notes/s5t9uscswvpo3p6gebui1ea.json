{"pageProps":{"note":{"id":"s5t9uscswvpo3p6gebui1ea","title":"Big Data","desc":"","updated":1653318643709,"created":20211109142548908,"custom":{},"fname":"devlog.big data","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"cda24adf9ff453b6e0f548a083425589","links":[{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.Data Engineering","alias":"devlog.Data Engineering","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":37,"offset":37},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Data Engineering"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"CPU","alias":"CPU","position":{"start":{"line":54,"column":66,"offset":1844},"end":{"line":54,"column":73,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"CPU"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":58,"column":5,"offset":2122},"end":{"line":58,"column":22,"offset":2139},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":61,"column":46,"offset":2261},"end":{"line":61,"column":61,"offset":2276},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":64,"column":21,"offset":2405},"end":{"line":64,"column":41,"offset":2425},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.MySQL","alias":"devlog.MySQL","position":{"start":{"line":99,"column":64,"offset":4692},"end":{"line":99,"column":80,"offset":4708},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.MySQL"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":101,"column":5,"offset":4732},"end":{"line":101,"column":20,"offset":4747},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":125,"column":5,"offset":6015},"end":{"line":125,"column":20,"offset":6030},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.apache spark","alias":"devlog.apache spark","position":{"start":{"line":127,"column":5,"offset":6045},"end":{"line":127,"column":28,"offset":6068},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.apache spark"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":127,"column":32,"offset":6072},"end":{"line":127,"column":52,"offset":6092},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"HDD","alias":"HDD","position":{"start":{"line":133,"column":112,"offset":6516},"end":{"line":133,"column":119,"offset":6523},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"HDD"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"RAM","alias":"RAM","position":{"start":{"line":133,"column":145,"offset":6549},"end":{"line":133,"column":152,"offset":6556},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"RAM"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":152,"column":3,"offset":7085},"end":{"line":152,"column":17,"offset":7099},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"DataFrame","alias":"DataFrame","position":{"start":{"line":159,"column":56,"offset":7467},"end":{"line":159,"column":69,"offset":7480},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DataFrame"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"DataFrame","alias":"DataFrame","position":{"start":{"line":164,"column":11,"offset":8062},"end":{"line":164,"column":24,"offset":8075},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DataFrame"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"AWS","alias":"AWS","position":{"start":{"line":171,"column":105,"offset":8450},"end":{"line":171,"column":112,"offset":8457},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"AWS"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.linux","alias":"devlog.linux","position":{"start":{"line":172,"column":55,"offset":8513},"end":{"line":172,"column":71,"offset":8529},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.linux"}},{"from":{"fname":"devlog.data warehouse","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":39,"column":54,"offset":2039},"end":{"line":39,"column":73,"offset":2058},"indent":[]},"value":"devlog.Big Data","alias":"devlog.Big Data"},{"from":{"fname":"devlog.etl","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":38,"column":21,"offset":897},"end":{"line":38,"column":40,"offset":916},"indent":[]},"value":"devlog.Big Data","alias":"devlog.Big Data"},{"from":{"fname":"devlog.ods","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":8,"column":42,"offset":417},"end":{"line":8,"column":61,"offset":436},"indent":[]},"value":"devlog.Big Data","alias":"devlog.Big Data"},{"from":{"fname":"devlog.yarn","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":29,"offset":29},"indent":[]},"value":"devlog.Big Data","alias":"devlog.Big Data"}],"anchors":{"5-vs-of-big-data":{"type":"header","text":"5 Vs of Big Data","value":"5-vs-of-big-data","line":14,"column":0,"depth":3},"use-cases":{"type":"header","text":"Use cases:","value":"use-cases","line":22,"column":0,"depth":3},"devloghadoop":{"type":"header","text":"devlog.hadoop","value":"devloghadoop","line":64,"column":0,"depth":3},"hbase":{"type":"header","text":"HBase","value":"hbase","line":86,"column":0,"depth":3},"mahoot":{"type":"header","text":"Mahoot","value":"mahoot","line":91,"column":0,"depth":3},"flume":{"type":"header","text":"Flume","value":"flume","line":95,"column":0,"depth":3},"devloghdfs":{"type":"header","text":"devlog.hdfs","value":"devloghdfs","line":107,"column":0,"depth":3},"mapreduce":{"type":"header","text":"MapReduce","value":"mapreduce","line":115,"column":0,"depth":3},"spark":{"type":"header","text":"Spark","value":"spark","line":122,"column":0,"depth":3},"devlogapache-spark-vs-devlogmapreduce":{"type":"header","text":"devlog.apache spark VS devlog.mapreduce","value":"devlogapache-spark-vs-devlogmapreduce","line":133,"column":0,"depth":3},"spark-rdds":{"type":"header","text":"Spark RDDs","value":"spark-rdds","line":141,"column":0,"depth":3},"spark-dataframes":{"type":"header","text":"Spark DataFrames","value":"spark-dataframes","line":170,"column":0,"depth":3},"python-and-spark":{"type":"header","text":"Python and Spark","value":"python-and-spark","line":175,"column":0,"depth":3},"miscellaneous-notes":{"type":"header","text":"Miscellaneous Notes","value":"miscellaneous-notes","line":182,"column":0,"depth":2},"scalable-cluster-computing":{"type":"header","text":"Scalable Cluster Computing","value":"scalable-cluster-computing","line":184,"column":0,"depth":3},"of-servers-nodes-and-daemons":{"type":"header","text":"Of Servers, Nodes and Daemons","value":"of-servers-nodes-and-daemons","line":190,"column":0,"depth":3},"servernode":{"type":"header","text":"Server/Node","value":"servernode","line":192,"column":0,"depth":4},"daemon":{"type":"header","text":"Daemon","value":"daemon","line":199,"column":0,"depth":4},"scalable-behaviour":{"type":"header","text":"Scalable Behaviour","value":"scalable-behaviour","line":207,"column":0,"depth":3}},"children":[],"parent":"9gtn7g40cvqui0sifl1s7t5","data":{}},"body":"<h1 id=\"big-data\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#big-data\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Big Data</h1>\n<ul>\n<li>Areas: <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">devlog.Data Engineering (Private)</a></li>\n</ul>\n<hr>\n<p>Big Data is a term for collections of data sets so large and complex that it becomes difficult to process them using in-hand database management tools or traditional data processing applications.</p>\n<h3 id=\"5-vs-of-big-data\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#5-vs-of-big-data\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>5 Vs of Big Data</h3>\n<ul>\n<li>Volume - Storing and processing huge data sets</li>\n<li>Variety - Storing and processing different types of data</li>\n<li>Velocity - The speed at which data is being generated</li>\n<li>Value - Finding meaning out of data, making sense of data</li>\n<li>Veracity - Uncertainty and inconsistencies in the data</li>\n</ul>\n<h3 id=\"use-cases\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#use-cases\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Use cases:</h3>\n<ul>\n<li>Web and E-tailing\n<ul>\n<li>Recommendation Engines</li>\n<li>Ad Targeting</li>\n<li>Search Quality</li>\n</ul>\n</li>\n<li>Telecommunications\n<ul>\n<li>Abuse and Click Fraud Detection</li>\n<li>Customer Churn Prevention</li>\n<li>Network Performance Optimization</li>\n<li>Calling Data Record(CDR) Analysis</li>\n<li>Analyzing Network to Predict Failure</li>\n</ul>\n</li>\n<li>Banks and Financial Services\n<ul>\n<li>Modeling True Risk</li>\n<li>Threat Analysis</li>\n<li>Fraud Detection</li>\n<li>Trade Surveillance</li>\n<li>Credit Scoring and Analysis</li>\n</ul>\n</li>\n<li>Retail\n<ul>\n<li>Point of Sales Transaction Analysis</li>\n<li>Customer Churn Analysis</li>\n<li>Sentiment Analysis</li>\n</ul>\n</li>\n<li>Government\n<ul>\n<li>Fraud Detection and Cybersecurity</li>\n<li>Welfare Schemes</li>\n<li>Justice</li>\n</ul>\n</li>\n<li>Healthcare and Life Sciences\n<ul>\n<li>Health Information Exchange</li>\n<li>Gene Sequencing</li>\n<li>Serialization</li>\n<li>Healthcare Service Quality Improvements</li>\n<li>Drug Safety</li>\n<li>See <a href=\"https://cwiki.apache.org/confluence/display/HADOOP2/poweredby\">PoweredBy</a> for companies that are these technologies for Big Data</li>\n</ul>\n</li>\n<li>Semi-structured data</li>\n<li>Unstructured data</li>\n<li>Structured data</li>\n<li>Large amount of data cannot fit on a single machine that needs processing is processed using distributed machines.</li>\n<li>A distributed process has access to the computational resources across a number of machines connected through a network.</li>\n<li>After a certain point, it is easier to scale out to many lower <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">CPU (Private)</a> machines, than to try to scale up to a single machine with a high CPU.</li>\n<li>Distributed machines also have the advanteage of easily scaling, you can just add more machines.</li>\n<li>They also include fault tolerance, if one machines fails, the whole network can still go on.</li>\n</ul>\n<h3 id=\"hadoop\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#hadoop\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a><a href=\"/notes/85w31vcdf3bjnm0yxh72ygf\">Hadoop</a></h3>\n<ul>\n<li>Hadoop is a way to distribute very large files across multiple machines.</li>\n<li>It uses the Hadoop Distributed File System <a href=\"/notes/kdddo1f7ltfsuwhexj4s535\">HDFS</a></li>\n<li>HDFS allows a user to work with large data sets</li>\n<li>HDFS also duplicates blocks of data for fault tolerance</li>\n<li>It also then uses <a href=\"/notes/j6kfx6ziqg91r356nur9cwy\">MapReduce</a></li>\n<li>MapReduce allows computations on that data</li>\n<li>You've a NameNode(Master) and that connects to multiple DataNode(s)(Slave)</li>\n</ul>\n<p>Hadoop really only comes with HDFS and MapReduce and all the other technologies are addons, developed separately by different individuals.</p>\n<p>Java is highly preferable for MR</p>\n<ul>\n<li>Pig was invented by Yahoo, PigLatin is the language, it a scripting language that was brought about to replace SQL/Java in MR. Internally it runs on MR and converts the code to Java. Most stuff that can be done on Hive can be done on Pig.</li>\n<li>Sqoop was invented by a group, to bring data from RDBMS to Hadoop you write code in MapReduce, to avoid this, they brought about sqoop, sqoop doesn't use a programming language but commands to bring in data to Hadoop. sqoop also allows you to send data back to RDBMS from Hadoop. Import and Export. Used for Datapipeline, sqoop internally runs on Java.</li>\n<li>Oozie was invented by Yahoo, its like an XML file, its a scheduler, used for automating, scheduling jobs, like running queries on a specific time. Big Data tech stack may include non Big Data specific schedulers. It runs on Java internally.</li>\n</ul>\n<p>All of above technologies discussed are abstractions of MapReduce.</p>\n<p>Big Data tech stack don't only consist of Big Data tools/technologies, MySQL for example is not a Big Data specific tool/technologies.</p>\n<h3 id=\"hbase\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#hbase\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a><a href=\"#HBase\">HBase</a></h3>\n<p>Storage part, Database for Hadoop but NoSQL, invented by Facebook, HBase shell commands, works on top of Hadoop, all the modern databases are NoSQL database.\nIt is different from Hive.</p>\n<h3 id=\"mahoot\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mahoot\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Mahoot</h3>\n<p>Framework for Data Science</p>\n<h3 id=\"flume\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#flume\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Flume</h3>\n<p>Similar to sqoop, used in datapipelines, flume can retrieve data in realtime. Messaging queue. sqoop is only for RDBMS unlike Flume which can be used for a fetching data from many other sources.(like twitter). Flume only retrieves data to Hadoop but cannot send it back to any other targets, only incoming.</p>\n<p>There are other MQs such as Kafka, Scribe.</p>\n<p>Hadoop is a losely couple framework, you can swap out/in components like oozie and it would still work, unlike tightly coupled frameworks(like .NET frameworks)</p>\n<p>Integrations - Hadoop can be integrated well with Spark or any other Big Data frameworks.</p>\n<p>Hadoop can also be connected with non Big Data components like <a href=\"/notes/ypszfixe0p3s5k0inqs5g08\">MySql</a> or other RDBMSes.</p>\n<h3 id=\"hdfs\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#hdfs\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a><a href=\"/notes/kdddo1f7ltfsuwhexj4s535\">HDFS</a></h3>\n<ul>\n<li>HDFS will use blocks of data, with a size of 128 MB by default.</li>\n<li>Each of these blocks is replicated 3 times.</li>\n<li>The blocks are distributed in a way to support fault tolerance.</li>\n<li>Smaller blocks provide more parallelization during processing.</li>\n<li>Multiple copies of a block prevents loss of data due to a failure of a node.</li>\n</ul>\n<h3 id=\"mapreduce\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mapreduce\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>MapReduce</h3>\n<ul>\n<li>MapReduce is a way of splitting a computation task to a distributed set of files (such as HDFS).</li>\n<li>It consists of a Job Tracker and multiple Task Trackers.</li>\n<li>The Job Tracker sends code to run on the Task Trackers.</li>\n<li>The Task Trackers allocate CPU and memory for the tasks and monitor the tasks on the worker nodes.</li>\n</ul>\n<h3 id=\"spark\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Spark</h3>\n<ul>\n<li>One of the latest and open source technologies built to quickly and easily handle Big Data.</li>\n<li>It is a Framework for dealing with large data, distributing it and doing calculations across distributed network. It is written in Scala. Scala gets the Spark's latest features, Scala is written in Java so Java can also be used for Spark's lastest features.</li>\n<li>Created at AMPLab at UC Berkeley, first released in Feb of 2013.</li>\n<li>It is a flexible alternative to MapReduce. (doesn't necessarily replaces Hadoop but MapReduce)</li>\n<li>Spark can use data stored in a variety of formats\n<ul>\n<li>Cassandra</li>\n<li>AWS S3</li>\n<li><a href=\"/notes/kdddo1f7ltfsuwhexj4s535\">HDFS</a> and more</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"apache-spark-vs-mapreduce\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#apache-spark-vs-mapreduce\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a><a href=\"/notes/f2kecna72pmc7re3wh1ugk4\">Apache Spark</a> VS <a href=\"/notes/j6kfx6ziqg91r356nur9cwy\">MapReduce</a></h3>\n<ul>\n<li>MapReduce requires files to be stored in HDFS, Spark does not!</li>\n<li>Spark also can perform operations up to 100x faster than MapReduce.</li>\n<li>It is not really Hadoop VS MapReduce, people sometime make it as such because MapReduce uses HDFS.</li>\n<li>MapReduce writes most data to disk after each Map and Reduce operation.</li>\n<li>Spark keeps most of the data in memory something like RAM after each Transformation. Spark can spill over to <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">HDD (Private)</a> if you don't have enough <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">RAM (Private)</a>.</li>\n</ul>\n<h3 id=\"spark-rdds\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-rdds\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Spark RDDs</h3>\n<ul>\n<li>At the core of Spark is the idea of a Resilient Distributed Dataset(RDD).</li>\n<li>Resilient Distributed Dataset (RDD) has 4 main features:\n<ul>\n<li>Distrbuted Collections of Data</li>\n<li>Fault-tolerant</li>\n<li>Parallel operation - ability to be partitioned</li>\n<li>Ability to use many data sources</li>\n</ul>\n</li>\n</ul>\n<p>At the abstract level it looks like:</p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.ewfeqm3kqzs.png\"></p>\n<p>At physical level:</p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.3f0c49iwkep.png\"></p>\n<ul>\n<li><a href=\"/notes/2vn2g9mhmi624b83rl5se0k\">RDD</a>s are immutable, lazily evaluated and cacheable</li>\n<li>There are two types of Spark operations:\n<ul>\n<li>Transformations</li>\n<li>Actions</li>\n</ul>\n</li>\n<li>Transformations are basically a recipe to follow.</li>\n<li>Actions actually perform what the recipe says to do and returns something back.</li>\n<li>This behavior carries over to the syntax when coding.</li>\n<li>A lot of times you will write a method call off of a <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">DataFrame (Private)</a> you won't see anything as a result until you call the action.</li>\n<li>This makes sense because with a large dataset, you don't want to calculate all the transformations until you are sure you want to perform them!</li>\n<li>When discussing Spark syntax you'll often see RDD versus DataFrame syntax show up.</li>\n<li>With the release of Spark 2.0, Spark is moving towards a DataFrame based syntax, but keep in mind that the way files are being distributed can still be thought of as RDDs(it is still happening in RDD at the pyhsical level), it is just the typed out syntax that is changing.</li>\n</ul>\n<h3 id=\"spark-dataframe-privates\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-dataframe-privates\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Spark <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">DataFrame (Private)</a>s</h3>\n<ul>\n<li>Spark DataFrames are also now the standard way of using Spark's Machine Learning capabilities.</li>\n<li>Spark DataFrame documentation is still pretty new and can be sparse, it can be found on <a href=\"https://spark.apache.org/docs/latest/\">spark.apache.org</a></li>\n</ul>\n<h3 id=\"python-and-spark\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#python-and-spark\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Python and Spark</h3>\n<ul>\n<li>Realistically Spark won't be running on a single machine, it will run on a cluster on a service, like <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">AWS (Private)</a>.</li>\n<li>These cluster services will pretty much always be a <a href=\"/notes/owoutsv5dicylguol2odc3e\">Linux</a>) based system.</li>\n</ul>\n<hr>\n<h2 id=\"miscellaneous-notes\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#miscellaneous-notes\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Miscellaneous Notes</h2>\n<h3 id=\"scalable-cluster-computing\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#scalable-cluster-computing\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Scalable Cluster Computing</h3>\n<p>Scalable Cluster Computing is a method of combining discrete computing resources(e.g servers) to increase the performance of a single application.</p>\n<p>Does not work for applications! (i.e., not all applications \"scale\")</p>\n<h3 id=\"of-servers-nodes-and-daemons\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#of-servers-nodes-and-daemons\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Of Servers, Nodes and Daemons</h3>\n<h4 id=\"servernode\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#servernode\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Server/Node</h4>\n<p>A Cluster is a collection of servers working together on one problem.</p>\n<ul>\n<li>Servers come with cores, memory, storage and network</li>\n<li>Servers can be called a \"<strong>NODE</strong>\" in a cluster of servers</li>\n</ul>\n<h4 id=\"daemon\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#daemon\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Daemon</h4>\n<p>A daemon is a program that is constantly running and responding to requests (e.g., web server)</p>\n<ul>\n<li>Servers can have one or multiple daemons running on them</li>\n<li>Daemons can communicate with other daemons and form a network within and between servers</li>\n<li>Each daemon can also be called a \"<strong>NODE</strong>\"</li>\n</ul>\n<h3 id=\"scalable-behaviour\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#scalable-behaviour\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Scalable Behaviour</h3>\n<p>of Scalable Systems:\nThe ability to apply resources (servers, VM instances) to a problem as a means to increase performance(green line) over a single server(red line).</p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.t6kfn1dy0om.png\"></p>\n<p>But, initially performance may lag due to overhead(the setup of our infrastructure).</p>\n<hr>\n<strong>Backlinks</strong>\n<ul>\n<li><a href=\"/notes/lz322a84xc913sor5r5c71u\">Data Warehouse</a></li>\n<li><a href=\"/notes/ffhm4kotv5wbd139ml9ch44\">ETL</a></li>\n<li><a href=\"/notes/hr5l5ukjr3mfsnsn0fkw6w5\">ODS</a></li>\n<li><a href=\"/notes/6k7g4x7ws565lditv1n8hxm\">YARN</a></li>\n</ul>","noteIndex":{"id":"3nfl4nvv516muyzozhcwrw8","title":"/root","desc":"","updated":1655559901157,"created":1637610830605,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"581715455a6f0f7a699209e8521b4acf","links":[{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"my","position":{"start":{"line":4,"column":9,"offset":37},"end":{"line":4,"column":29,"offset":57},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":20,"column":111,"offset":1051},"end":{"line":20,"column":117,"offset":1057},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":21,"column":3,"offset":1198},"end":{"line":21,"column":9,"offset":1204},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes","alias":"swipes","position":{"start":{"line":27,"column":3,"offset":1724},"end":{"line":27,"column":13,"offset":1734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.quotes","alias":"quotes","position":{"start":{"line":27,"column":48,"offset":1769},"end":{"line":27,"column":72,"offset":1793},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.quotes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.excerpts","alias":"excerpts","position":{"start":{"line":27,"column":74,"offset":1795},"end":{"line":27,"column":102,"offset":1823},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.excerpts"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.sayings","alias":"sayings","position":{"start":{"line":27,"column":104,"offset":1825},"end":{"line":27,"column":130,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.sayings"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.phrases","alias":"phrases","position":{"start":{"line":27,"column":132,"offset":1853},"end":{"line":27,"column":158,"offset":1879},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.phrases"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"resources.people","alias":"others","position":{"start":{"line":27,"column":214,"offset":1935},"end":{"line":27,"column":241,"offset":1962},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"resources.people"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"inbox.webmark","alias":"webmark","position":{"start":{"line":31,"column":235,"offset":2463},"end":{"line":31,"column":260,"offset":2488},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"inbox.webmark"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"slipbox.Ontology","alias":"slipbox.Ontology","position":{"start":{"line":55,"column":3,"offset":3735},"end":{"line":55,"column":23,"offset":3755},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"slipbox.Ontology"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"About me","position":{"start":{"line":60,"column":3,"offset":3963},"end":{"line":60,"column":29,"offset":3989},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}}],"anchors":{"welcome-to-noetic-noggin":{"type":"header","text":"Welcome to Noetic Noggin","value":"welcome-to-noetic-noggin","line":8,"column":0,"depth":1},"principles":{"type":"header","text":"Principles","value":"principles","line":18,"column":0,"depth":2},"all-notes-should-be-relative-to-me":{"type":"header","text":"All notes should be relative to me.","value":"all-notes-should-be-relative-to-me","line":20,"column":0,"depth":3},"gotta-capture-em-all":{"type":"header","text":"Gotta capture 'em all","value":"gotta-capture-em-all","line":30,"column":0,"depth":3},"dont-force-evolution":{"type":"header","text":"Don't force evolution","value":"dont-force-evolution","line":40,"column":0,"depth":3},"noise--signal":{"type":"header","text":"Noise & Signal","value":"noise--signal","line":45,"column":0,"depth":3},"why-do-any-of-this":{"type":"header","text":"Why do any of this?","value":"why-do-any-of-this","line":50,"column":0,"depth":3},"structure-of-this-wiki":{"type":"header","text":"Structure of this wiki","value":"structure-of-this-wiki","line":59,"column":0,"depth":2},"quicklinks":{"type":"header","text":"Quicklinks","value":"quicklinks","line":64,"column":0,"depth":2}},"children":["0yay2om15bsg2li2p6qgux7","05c4nnjqa92zx11ld6o0ytn","9gtn7g40cvqui0sifl1s7t5","ftbd1hknsd3ocd7jao26tn3","a1kmkdbpclaz5p6sykaw6kc","z121gkmqfo09m8r7jgnpfgn","gkqrr7xbt18xhi93dmjrwzj","ja2x4lrgejr9o9wvit0bd0d","luv39odkfibx3wdosvigwvy","vtvk3bi6o72w58oima9xzf3","yy652kvqrkfn9ipk07m40h4"],"parent":null,"data":{},"body":"\n# Welcome to Noetic Noggin\n\nThis is [[my|archive.about]] personal wiki and a commonplace book; notes by me, for me.\n\nðŸš§ Permanently under construction ðŸš§\n\n![](https://res.cloudinary.com/zubayr/image/upload/v1658499909/wiki/ajevkuyebljlxiblyst2.png)\n\nThis wiki was made possible with [dendron.so](https://dendron.so) and [obisidian.md](https://obsidian.md). Stored on [Github Repository](https://github.com/zubayrrr/dendron) and hosted on [Netlify](https://netlify.com) for free.\n\n## Principles\n\n### All notes should be relative to me.\n\n- All notes in principle are written for me; what I know about a subject, how I feel about a particular thing.\n- Opinions are fine as long as I feel strong epistemic confidence in the given opinion.\n- Don't over explain a note if it's not necessary, remember, these notes are for you and are relative to whatever knowledge you posses about the subject.\n- Read books and make an dedicated notes for them.\n- Listen podcasts but capture them inside a \"subject specific\" note or \"Map of Concept\" note or a note tagged #areas. Because making notes from podcasts can be tedious as they're not as well structured as books for consumption.(Whose merit is debatable.)\n- #areas are basically \"Map of Concept\" notes but I have recently come to the realization that its better to maintain them [Nikita Voloboev style](https://wiki.nikiv.dev/) but with heavy usage of transclusion and backlinking.\n  - \"Resources\" should be first processed and then mentioned inside the note, otherwise they should be left in inbox.\n\n### Gotta capture 'em all\n\n- Hog whatever information tickles your pickle([anything that gratifies one's intellectual curiosity](https://news.ycombinator.com/newsguidelines.html)).\n- [[swipes]] are interesting/useful bits of... [[quotes|swipes.quotes]], [[excerpts|swipes.excerpts]], [[sayings|swipes.sayings]], [[phrases|swipes.phrases]]. Essentially, ideas, opinions that are swiped off from [[others|resources.people]].\n- Make no distinction between \"your\" ideas and ideas of \"others\", because if you vibe with an idea; it's already yours.\n- But also remember \"If you've time to consume, you've time to produce.\".\n- Use [raindrop.io](https://raindrop.io) to manage your URL bookmarks.\n- If you need to bookmark a webpage or an article all together, use [MarkDownload](https://chrome.google.com/webstore/detail/markdownload-markdown-web/pcmpcfapbekmbjjkdalcgopdkipoggdi?hl=en-GB) to rip the entire page. Let's call it a [[webmark|inbox.webmark]]; it belong in the `/inbox`.\n- Similar process is employed for capturing tweets using [tweet-to-markdown](https://github.com/kbravh/tweet-to-markdown) and it also belongs in the `/inbox`.\n\n### Don't force evolution\n\n- Let your second brain evolve at it's own pace.\n- The structure should never be _too_ rigid because its meant to take form by itself.\n\n### Noise & Signal\n\n- While capturing ideas left and right is recommended, make sure you're not harming your periods of focus.\n- Have impenetrable focus periods (use Pomodoro method) where you only care about the work on hand and nothing else.\n\n### Why do any of this?\n\n- Because I can't remember everything - there's a lot of information around that interests me and there isn't enough working memory installed in me.\n- So, I make notes - to remember, to create, to meditate, to think.\n- A bodybuilder's portfolio is their body - my portfolio is my wiki.\n- Not only am I making - whatever I know - tangible by writing it down. I know exactly where to look if I ever forget something.\n- I am at the beginning of my learning adventures. When I look back at it, I will know where I came from and how my thoughts evolved over time.\n- Plus, its really fun to nerd out.\n\n## Structure of this wiki\n\n- [[slipbox.Ontology]] explains the structure of this wiki and the tags, backlinks used in it.\n- Dendron takes care of the structure and hierarchy(mostly), but I insist on using tags for backwards compatibility.\n\n## Quicklinks\n\n- [[About me|archive.about]]\n- [Github](https://github.com/zubayrrr)\n- [Twitter](https://twitter.com/zoobhalu)\n- [Blog](https://zubayrali.in)\n- [Guestbook](https://www.yourworldoftext.com/~zubayrali/)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template","insertNote":{"initialValue":"templates"}},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Noetic Noggin","description":"Personal Wiki / Digital Garden","author":"Zubayr Ali","twitter":"zoobhalu"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteUrl":"localhost:3000","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}