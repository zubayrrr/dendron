{"pageProps":{"note":{"id":"j6kfx6ziqg91r356nur9cwy","title":"MapReduce","desc":"","updated":1653305310654,"created":20211122214632356,"custom":{},"fname":"devlog.mapreduce","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"d4f29c70a2f34823f3bf9eb5c082f7d3","links":[{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":15,"column":67,"offset":524},"end":{"line":15,"column":82,"offset":539},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":57,"column":3,"offset":2308},"end":{"line":57,"column":20,"offset":2325},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":57,"column":81,"offset":2386},"end":{"line":57,"column":96,"offset":2401},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.yarn","alias":"devlog.yarn","position":{"start":{"line":57,"column":98,"offset":2403},"end":{"line":57,"column":113,"offset":2418},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.yarn"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":57,"column":118,"offset":2423},"end":{"line":57,"column":138,"offset":2443},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hive","alias":"devlog.hive","position":{"start":{"line":58,"column":78,"offset":2522},"end":{"line":58,"column":93,"offset":2537},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hive"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"Tez","alias":"Tez","position":{"start":{"line":60,"column":26,"offset":2653},"end":{"line":60,"column":33,"offset":2660},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Tez"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"Tez","alias":"Tez","position":{"start":{"line":76,"column":47,"offset":3285},"end":{"line":76,"column":54,"offset":3292},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Tez"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hive","alias":"devlog.hive","position":{"start":{"line":79,"column":33,"offset":3463},"end":{"line":79,"column":48,"offset":3478},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hive"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":192,"column":1,"offset":8456},"end":{"line":192,"column":18,"offset":8473},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.apache spark","alias":"devlog.apache spark","position":{"start":{"line":192,"column":23,"offset":8478},"end":{"line":192,"column":46,"offset":8501},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.apache spark"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.Redirecting stdin & stderr","alias":"devlog.Redirecting stdin & stderr","position":{"start":{"line":202,"column":77,"offset":9252},"end":{"line":202,"column":114,"offset":9289},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Redirecting stdin & stderr"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.stdout","alias":"devlog.stdout","position":{"start":{"line":202,"column":119,"offset":9294},"end":{"line":202,"column":136,"offset":9311},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.stdout"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"Tez","alias":"Tez","position":{"start":{"line":289,"column":11,"offset":11852},"end":{"line":289,"column":18,"offset":11859},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Tez"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.apache spark","alias":"devlog.apache spark","position":{"start":{"line":289,"column":20,"offset":11861},"end":{"line":289,"column":43,"offset":11884},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.apache spark"}},{"from":{"fname":"devlog.apache spark","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":69,"column":25,"offset":1938},"end":{"line":69,"column":45,"offset":1958},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.apache spark","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":191,"column":123,"offset":8334},"end":{"line":191,"column":143,"offset":8354},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.big data","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":64,"column":21,"offset":2405},"end":{"line":64,"column":41,"offset":2425},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.big data","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":127,"column":32,"offset":6072},"end":{"line":127,"column":52,"offset":6092},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.dag","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":10,"column":123,"offset":315},"end":{"line":10,"column":143,"offset":335},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.data engineering roadmap","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":18,"column":3,"offset":558},"end":{"line":18,"column":23,"offset":578},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.hive","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":18,"column":39,"offset":1003},"end":{"line":18,"column":59,"offset":1023},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.hive","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":20,"column":56,"offset":1080},"end":{"line":20,"column":76,"offset":1100},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.hql","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":54,"column":22,"offset":1608},"end":{"line":54,"column":42,"offset":1628},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.tez","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":21,"offset":21},"end":{"line":2,"column":41,"offset":41},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"}],"anchors":{"example-of-mapreduce-but-non-parallel":{"type":"header","text":"Example of MapReduce (but non parallel)","value":"example-of-mapreduce-but-non-parallel","line":12,"column":0,"depth":3},"mapreduce-design-principles":{"type":"header","text":"MapReduce Design Principles","value":"mapreduce-design-principles","line":16,"column":0,"depth":3},"mapreduce-big-picture":{"type":"header","text":"MapReduce Big Picture","value":"mapreduce-big-picture","line":27,"column":0,"depth":3},"slices-and-splits":{"type":"header","text":"Slices and Splits","value":"slices-and-splits","line":37,"column":0,"depth":3},"scalable-mapping-step":{"type":"header","text":"Scalable Mapping Step","value":"scalable-mapping-step","line":47,"column":0,"depth":3},"reduction-step":{"type":"header","text":"Reduction Step","value":"reduction-step","line":55,"column":0,"depth":3},"summary":{"type":"header","text":"Summary","value":"summary","line":61,"column":0,"depth":3},"advantages-of-hadoop-mapreduce":{"type":"header","text":"Advantages of Hadoop MapReduce","value":"advantages-of-hadoop-mapreduce","line":69,"column":0,"depth":3},"programming-hadoop-mapreduce":{"type":"header","text":"Programming Hadoop MapReduce","value":"programming-hadoop-mapreduce","line":80,"column":0,"depth":3},"examples-mapreduce-in-parallel":{"type":"header","text":"Examples (MapReduce in Parallel)","value":"examples-mapreduce-in-parallel","line":93,"column":0,"depth":2},"word-count":{"type":"header","text":"Word Count","value":"word-count","line":95,"column":0,"depth":3},"working-example-of-word-count":{"type":"header","text":"Working Example of Word Count","value":"working-example-of-word-count","line":111,"column":0,"depth":3},"calculate-pi":{"type":"header","text":"Calculate pi","value":"calculate-pi","line":125,"column":0,"depth":3},"the-terasort-benchmark":{"type":"header","text":"The Terasort Benchmark","value":"the-terasort-benchmark","line":140,"column":0,"depth":3},"miscellaneous-notes":{"type":"header","text":"Miscellaneous notes","value":"miscellaneous-notes","line":196,"column":0,"depth":3},"simulating-map-and-reduce-using-python-and-bash-scripts":{"type":"header","text":"Simulating Map and Reduce using Python and Bash scripts","value":"simulating-map-and-reduce-using-python-and-bash-scripts","line":200,"column":0,"depth":2},"developing-a-hadoop-streaming-word-counting-application":{"type":"header","text":"Developing a Hadoop Streaming word counting application.","value":"developing-a-hadoop-streaming-word-counting-application","line":210,"column":0,"depth":3}},"children":[],"parent":"9gtn7g40cvqui0sifl1s7t5","data":{}},"body":"<h1 id=\"mapreduce\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mapreduce\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>MapReduce</h1>\n<ul>\n<li>MapReduce is a simple algorithm</li>\n<li>It uses distinct steps and one-way communication</li>\n<li><strong>Map</strong> first then <strong>Reduce</strong>, can be combined into multiple layers</li>\n</ul>\n<h3 id=\"example-of-mapreduce-but-non-parallel\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#example-of-mapreduce-but-non-parallel\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Example of MapReduce (but non parallel)</h3>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.uyhzcdyhtl.png\"></p>\n<h3 id=\"mapreduce-design-principles\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mapreduce-design-principles\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>MapReduce Design Principles</h3>\n<p><strong>\"Can we read parts(<em>slices</em>) of the file at the same time on different systems?\"</strong></p>\n<ul>\n<li>For certain types of problems the answer is \"Yes\"</li>\n<li>Slice data and spread across multiple HDDs on multiple servers (<a href=\"/notes/kdddo1f7ltfsuwhexj4s535\">HDFS</a>)</li>\n<li>Obtain performance from reading slices at the same time</li>\n<li>Moving computation to data, if possible(cheaper than moving data to computation)</li>\n<li>Allows scalable software that can hide (most) execution details from the user</li>\n<li>The ability to be fault tolerant (map steps crashing, restarting) which can be extended to reducers.</li>\n</ul>\n<h3 id=\"mapreduce-big-picture\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mapreduce-big-picture\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>MapReduce Big Picture</h3>\n<ul>\n<li>Slice data and place each part on a different server(node)?</li>\n<li>Improve performance by mapping (processing) each slice independently</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.2eclq7p550p.png\"></p>\n<ul>\n<li>Slicing happens automatically, the administrator can change the size of the slice</li>\n<li>Slice is blind to the data that is inside the file(s), we're not slicing based on structure but based on size.</li>\n</ul>\n<h3 id=\"slices-and-splits\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#slices-and-splits\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Slices and Splits</h3>\n<ul>\n<li>Each slice has programmatically defined data splits that independent of slice boundaries.</li>\n<li><strong>A map is applied to each split in the slice.</strong></li>\n<li>Edge splits may span two slices.</li>\n<li>There can be multiple maps occuring on the same slice.\n<ul>\n<li>If a split doesn't fit on a slice, edge splits may span to other slices(not a lot of data and MapReduce algo will account for it automagically)</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.wstyvft6d0s.png\"></p>\n<h3 id=\"scalable-mapping-step\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#scalable-mapping-step\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Scalable Mapping Step</h3>\n<ol>\n<li>Slice data into parts</li>\n<li>Apply the same Mapping function to <strong>Input list</strong> (user defined splits)</li>\n<li>Each step provides a separate set of results(<strong>Output list</strong>)</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.2glgo0k8cyt.png\"></p>\n<h3 id=\"reduction-step\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#reduction-step\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Reduction Step</h3>\n<ul>\n<li>Results from each map(as applied to each split) are combined and reduced to a single Output value.</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.w1uoldg3if.png\"></p>\n<h3 id=\"summary\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#summary\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Summary</h3>\n<ul>\n<li><a href=\"/notes/85w31vcdf3bjnm0yxh72ygf\">Hadoop</a> provides a MapReduce engine as part of the core components (<a href=\"/notes/kdddo1f7ltfsuwhexj4s535\">HDFS</a>, <a href=\"/notes/6k7g4x7ws565lditv1n8hxm\">YARN</a> and <a href=\"/notes/j6kfx6ziqg91r356nur9cwy\">MapReduce</a>)</li>\n<li>Hadoop MapReduce can be used directly or by higher level applications(e.g. <a href=\"/notes/30isnfzmqrvmmiii03d0chj\">Hive</a>)</li>\n<li>Original MapReduce was somewhat slow because intermediate results are written to disk.</li>\n<li>Current MapReduce uses <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Tez (Private)</a> acceleration to keep intermediate results in memory (very fast)</li>\n<li>Operate on \"Bigger than Database\" amounts of data.</li>\n</ul>\n<h3 id=\"advantages-of-hadoop-mapreduce\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#advantages-of-hadoop-mapreduce\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Advantages of Hadoop MapReduce</h3>\n<ul>\n<li>Uses a functional approach (original data is unchanged)</li>\n<li>Restricted to one-way communication path</li>\n<li>Provides the following features:\n<ul>\n<li>Highly scalable(same user code)</li>\n<li>Easily managed workflow</li>\n<li>Hardware fault tolerant</li>\n</ul>\n</li>\n<li>MapReduce is powerful paradigm for solving problems</li>\n<li>Often referred to as a \"Data Parallel\" or <strong>S</strong>ingle <strong>I</strong>nstrution <strong>M</strong>ultiple <strong>D</strong>ata problem (SIMD)</li>\n</ul>\n<h3 id=\"programming-hadoop-mapreduce\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#programming-hadoop-mapreduce\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Programming Hadoop MapReduce</h3>\n<ul>\n<li><strong>Program Natively</strong> using Java API (Either <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Tez (Private)</a> or MapReduce)</li>\n<li>Uses other languages like Python with the Streaming interface(stdin, stdout and text only)</li>\n<li>Use the C++ Pipes interface.</li>\n<li>Use higher level tools (e.g., <a href=\"/notes/30isnfzmqrvmmiii03d0chj\">Hive</a>, Pig) on top of MapReduce or Tez acceleration</li>\n<li>MapReduce applications can scale with no change to the application</li>\n<li>Computation is moved close to the data in HDFS</li>\n<li>There is no need to manage side-effects or process state</li>\n<li>SW/HW faults in HDFS and YARN are handled by automatic restart of tasks, transparent to the application.</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.bnsj6fk5m18.png\"></p>\n<h2 id=\"examples-mapreduce-in-parallel\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#examples-mapreduce-in-parallel\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Examples (MapReduce in Parallel)</h2>\n<h3 id=\"word-count\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#word-count\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Word Count</h3>\n<p>Determine how many times unique words are used in the following text.</p>\n<ul>\n<li><code>see spot run</code></li>\n<li><code>run spot run</code></li>\n<li><code>see the cat</code></li>\n</ul>\n<p>A trivial task. How does MapReduce perform this at scale when a collection of document has 50 billion words?</p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.upf9zn7wm2.png\"></p>\n<p>We can add improvements to our mapper using a combiner</p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.x9crxdaozq.png\"></p>\n<h3 id=\"working-example-of-word-count\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#working-example-of-word-count\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Working Example of Word Count</h3>\n<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar > words.txt</code></p>\n<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar wordcount wordcount/in wordcount/out</code></p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.vkmgyisvvbc.png\"></p>\n<p>Check the output that was created</p>\n<p><code>hdfs dfs -cat wordcount/out/part-r-00000</code></p>\n<p><img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.8velr6e7whs.png\"></p>\n<h3 id=\"calculate-pi\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#calculate-pi\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Calculate pi</h3>\n<ul>\n<li>Simple MapReduce program to run, good for quick tests.</li>\n<li>The following is run on the 4-node cluster</li>\n</ul>\n<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar pi 16 100000</code><br>\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.ip1wh7ittuc.png\">\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.op5t2hnmh7l.png\"></p>\n<ul>\n<li>The following is a short test that will run on the LHM-VM (If you increase</li>\n<li>the number of maps (8) or the number of guesses (1000) then the</li>\n<li>application will take longer.</li>\n</ul>\n<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar pi 8 1000</code></p>\n<h3 id=\"the-terasort-benchmark\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#the-terasort-benchmark\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>The Terasort Benchmark</h3>\n<p><strong>IMPORTANT: The test will not run on the LHM-VM. Indeed, as run below,\nthe Teragen portion will create a 50GB file (by default the LHM-VM has\n70 GB of file space. The actual Terasort step will create another\n50 GB file. You can try to reduce the file size from 500000000 to\n5000 so the application can finish.</strong>\nTerasort is used to measure the raw sorting power of Hadoop MapReduce, though it is of no practical use, it can provide an indication of how fast your cluster can process data.</p>\n<p>The following steps are for the the 4-node cluster mentioned above, using the HDFS account <code>/user/deadline</code>.</p>\n<p>There are three steps required to run the complete test:</p>\n<ol>\n<li>Generate the table</li>\n<li>Sort the table</li>\n<li>Validate the sort</li>\n</ol>\n<!-- end list -->\n<ul>\n<li>\n<p>rows are 100 bytes long, thus the total amount of data written is 100 times the</p>\n</li>\n<li>\n<p>number of rows (i.e. to write a 100 GBytes of data, use 1000000000 rows). You</p>\n</li>\n<li>\n<p>will also need to specify input and output directories in HDFS.</p>\n</li>\n<li>\n<p>1. Run teragen to generate 500 MB of data, 500,000 rows of random data to sort</p>\n</li>\n</ul>\n<p><code>yarn jar $$EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar teragen 5000000 /user/hands-on/TeraGen-500MB</code>\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.dhhwu16cazc.png\"> * Check if the data was created; this will be your input for the next step in the terasort benchmark\n<code>hdfs dfs -ls TeraGen-500MB</code>\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.brz6odys9hv.png\"></p>\n<ul>\n<li>2. Run terasort to sort the database</li>\n</ul>\n<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar terasort /user/hands-on/TeraGen-500MB /user/hands-on/TeraSort-500MB</code>\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.idartp3zxj.png\">\nCheck the output that was created for <code>TeraSort-500MB</code> (which was the output dir)\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.bohj41tce1a.png\"></p>\n<p>A combined single file was created</p>\n<ul>\n<li>3. Run teravalidate to validate the sort Teragen</li>\n</ul>\n<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar teravalidate /user/hands-on/TeraSort-500MB /user/hands-on/TeraValid-500MB</code>\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.m6uaknqoch.png\">\nCheck the results that was created and the checksum of the file\n<img src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.4ch3o9j2lo7.png\"> * Interpret the results: * Measure the time it takes to complete the terasort application. Results are * usually reported in Database Size in Seconds (or Minutes).</p>\n<ul>\n<li>The performance can be increased by increasing the number of reducers (default is one)</li>\n<li>add the option -Dmapred.reduce.tasks=NUMBER_OF_REDUCERS</li>\n<li>The command below uses 4 reducers.</li>\n</ul>\n<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar terasort -Dmapred.reduce.tasks=4 /user/hands-on/TeraGen-500MB /user/hands-on/TeraSort-500MB</code></p>\n<ul>\n<li>Don't for get to delete you files in HDFS before the next run!</li>\n</ul>\n<p><code>hdfs dfs -rm -r -skipTrash Tera*</code></p>\n<h3 id=\"miscellaneous-notes\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#miscellaneous-notes\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Miscellaneous notes</h3>\n<p><a href=\"/notes/85w31vcdf3bjnm0yxh72ygf\">Hadoop</a> and <a href=\"/notes/f2kecna72pmc7re3wh1ugk4\">Apache Spark</a> usually work with \"INPUT DIRECTORIES\" not files. Note that the argument given to word count in the above example was a directory NOT the \"words.txt\" file. We can add more files to the input directory and <code>wordcount</code> will ingest all the files in finds.</p>\n<h2 id=\"simulating-map-and-reduce-using-python-and-bash-scripts\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#simulating-map-and-reduce-using-python-and-bash-scripts\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Simulating Map and Reduce using Python and Bash scripts</h2>\n<p>Source: <a href=\"https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\">https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></p>\n<p>There are several methods to program a Hadoop MapReduce. The most direct and lowest level is using the Java Hadoop API or the C++ Pipes library.</p>\n<p>Many users prefer the flexibility of the Streams Interface that allows a variety of programming language to be used.</p>\n<p>This method will work with any program that can read and write TEXT DATA to <a href=\"/notes/n4ernfsrloq7zjrscm8t5rc\">Redirecting stdin &#x26; stderr</a> and <a href=\"/notes/do68y261xeu98bpq3vl5cn9\">stdout</a>.</p>\n<h3 id=\"developing-a-hadoop-streaming-word-counting-application\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#developing-a-hadoop-streaming-word-counting-application\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Developing a Hadoop Streaming word counting application.</h3>\n<ul>\n<li>pymapper.py - a python word count mapper</li>\n<li>shuffer.sh - a simulated shuffle step using bash script</li>\n<li>pyreducer.py - a python word count reducer</li>\n</ul>\n<!-- end list -->\n<pre><code>#!/usr/bin/env python\n\"\"\"pymapper.py\"\"\"\n\nimport sys\n\n# input comes from STDIN (standard input)\nfor line in sys.stdin:\n    # remove leading and trailing whitespace\n    line = line.strip()\n    # split the line into words\n    words = line.split()\n    # increase counters\n    for word in words:\n        # write the results to STDOUT (standard output);\n        # what we output here will be the input for the\n        # Reduce step, i.e. the input for reducer.py\n        #\n        # tab-delimited; the trivial word count is 1\n        print '%s\\t%s' % (word, 1)\n\n#!/bin/bash\n# shuffle.sh\nsort -k1,1\n\n#!/usr/bin/env python\n\"\"\"pyreducer.py\"\"\"\n\nfrom operator import itemgetter\nimport sys\n\ncurrent_word = None\ncurrent_count = 0\nword = None\n\n# input comes from STDIN\nfor line in sys.stdin:\n    # remove leading and trailing whitespace\n    line = line.strip()\n\n    # parse the input we got from mapper.py\n    word, count = line.split('\\t', 1)\n\n    # convert count (currently a string) to int\n    try:\n        count = int(count)\n    except ValueError:\n        # count was not a number, so silently\n        # ignore/discard this line\n        continue\n\n    # this IF-switch only works because Hadoop sorts map output\n    # by key (here: word) before it is passed to the reducer\n    if current_word == word:\n        current_count += count\n    else:\n        if current_word:\n            # write result to STDOUT\n            print ('%s\\t%s' % (current_word, current_count))\n        current_count = count\n        current_word = word\n\n# do not forget to output the last word if needed!\nif current_word == word:\n    print ('%s\\t%s' % (current_word, current_count))\n</code></pre>\n<p>1. Run the <code>pymapper.py</code> with some simple data and change the input words to key value pairs.</p>\n<pre><code>echo \"see spot run run spot run see the cat\" | ./pymapper.py\n</code></pre>\n<p>2. Sort the data passed through <code>pymapper.py</code></p>\n<pre><code>echo \"see spot run run spot run see the cat\" | ./pymapper.py | ./shuffle.sh\n</code></pre>\n<p>3. Add the reducer stage and count all the same keys</p>\n<pre><code>echo \"see spot run run spot run see the cat\" | ./pymapper.py | ./shuffle.sh | ./pyreducer.py\n</code></pre>\n<p>See also: <a title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\">Tez (Private)</a>, <a href=\"/notes/f2kecna72pmc7re3wh1ugk4\">Apache Spark</a></p>\n<hr>\n<strong>Backlinks</strong>\n<ul>\n<li><a href=\"/notes/f2kecna72pmc7re3wh1ugk4\">Apache Spark</a></li>\n<li><a href=\"/notes/s5t9uscswvpo3p6gebui1ea\">Big Data</a></li>\n<li><a href=\"/notes/bojmgq30yfxpn44hgo0ofcf\">DAG</a></li>\n<li><a href=\"/notes/2fz1tdl3yy2s78wdmo7ql56\">Data Engineering Roadmap</a></li>\n<li><a href=\"/notes/30isnfzmqrvmmiii03d0chj\">Hive</a></li>\n<li><a href=\"/notes/e92qvq2zwoa9wyn0p0187l3\">HQL</a></li>\n<li><a href=\"/notes/7v21vh269qxy069is0fkrzn\">Tez</a></li>\n</ul>","noteIndex":{"id":"3nfl4nvv516muyzozhcwrw8","title":"/root","desc":"","updated":1655559901157,"created":1637610830605,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"581715455a6f0f7a699209e8521b4acf","links":[{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"my","position":{"start":{"line":4,"column":9,"offset":37},"end":{"line":4,"column":29,"offset":57},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":20,"column":111,"offset":1051},"end":{"line":20,"column":117,"offset":1057},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":21,"column":3,"offset":1198},"end":{"line":21,"column":9,"offset":1204},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes","alias":"swipes","position":{"start":{"line":27,"column":3,"offset":1724},"end":{"line":27,"column":13,"offset":1734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.quotes","alias":"quotes","position":{"start":{"line":27,"column":48,"offset":1769},"end":{"line":27,"column":72,"offset":1793},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.quotes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.excerpts","alias":"excerpts","position":{"start":{"line":27,"column":74,"offset":1795},"end":{"line":27,"column":102,"offset":1823},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.excerpts"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.sayings","alias":"sayings","position":{"start":{"line":27,"column":104,"offset":1825},"end":{"line":27,"column":130,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.sayings"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.phrases","alias":"phrases","position":{"start":{"line":27,"column":132,"offset":1853},"end":{"line":27,"column":158,"offset":1879},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.phrases"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"resources.people","alias":"others","position":{"start":{"line":27,"column":214,"offset":1935},"end":{"line":27,"column":241,"offset":1962},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"resources.people"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"inbox.webmark","alias":"webmark","position":{"start":{"line":31,"column":235,"offset":2463},"end":{"line":31,"column":260,"offset":2488},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"inbox.webmark"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"slipbox.Ontology","alias":"slipbox.Ontology","position":{"start":{"line":55,"column":3,"offset":3735},"end":{"line":55,"column":23,"offset":3755},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"slipbox.Ontology"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"About me","position":{"start":{"line":60,"column":3,"offset":3963},"end":{"line":60,"column":29,"offset":3989},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}}],"anchors":{"welcome-to-noetic-noggin":{"type":"header","text":"Welcome to Noetic Noggin","value":"welcome-to-noetic-noggin","line":8,"column":0,"depth":1},"principles":{"type":"header","text":"Principles","value":"principles","line":18,"column":0,"depth":2},"all-notes-should-be-relative-to-me":{"type":"header","text":"All notes should be relative to me.","value":"all-notes-should-be-relative-to-me","line":20,"column":0,"depth":3},"gotta-capture-em-all":{"type":"header","text":"Gotta capture 'em all","value":"gotta-capture-em-all","line":30,"column":0,"depth":3},"dont-force-evolution":{"type":"header","text":"Don't force evolution","value":"dont-force-evolution","line":40,"column":0,"depth":3},"noise--signal":{"type":"header","text":"Noise & Signal","value":"noise--signal","line":45,"column":0,"depth":3},"why-do-any-of-this":{"type":"header","text":"Why do any of this?","value":"why-do-any-of-this","line":50,"column":0,"depth":3},"structure-of-this-wiki":{"type":"header","text":"Structure of this wiki","value":"structure-of-this-wiki","line":59,"column":0,"depth":2},"quicklinks":{"type":"header","text":"Quicklinks","value":"quicklinks","line":64,"column":0,"depth":2}},"children":["0yay2om15bsg2li2p6qgux7","05c4nnjqa92zx11ld6o0ytn","9gtn7g40cvqui0sifl1s7t5","ftbd1hknsd3ocd7jao26tn3","a1kmkdbpclaz5p6sykaw6kc","z121gkmqfo09m8r7jgnpfgn","gkqrr7xbt18xhi93dmjrwzj","ja2x4lrgejr9o9wvit0bd0d","luv39odkfibx3wdosvigwvy","vtvk3bi6o72w58oima9xzf3","yy652kvqrkfn9ipk07m40h4"],"parent":null,"data":{},"body":"\n# Welcome to Noetic Noggin\n\nThis is [[my|archive.about]] personal wiki and a commonplace book; notes by me, for me.\n\n🚧 Permanently under construction 🚧\n\n![](https://res.cloudinary.com/zubayr/image/upload/v1658499909/wiki/ajevkuyebljlxiblyst2.png)\n\nThis wiki was made possible with [dendron.so](https://dendron.so) and [obisidian.md](https://obsidian.md). Stored on [Github Repository](https://github.com/zubayrrr/dendron) and hosted on [Netlify](https://netlify.com) for free.\n\n## Principles\n\n### All notes should be relative to me.\n\n- All notes in principle are written for me; what I know about a subject, how I feel about a particular thing.\n- Opinions are fine as long as I feel strong epistemic confidence in the given opinion.\n- Don't over explain a note if it's not necessary, remember, these notes are for you and are relative to whatever knowledge you posses about the subject.\n- Read books and make an dedicated notes for them.\n- Listen podcasts but capture them inside a \"subject specific\" note or \"Map of Concept\" note or a note tagged #areas. Because making notes from podcasts can be tedious as they're not as well structured as books for consumption.(Whose merit is debatable.)\n- #areas are basically \"Map of Concept\" notes but I have recently come to the realization that its better to maintain them [Nikita Voloboev style](https://wiki.nikiv.dev/) but with heavy usage of transclusion and backlinking.\n  - \"Resources\" should be first processed and then mentioned inside the note, otherwise they should be left in inbox.\n\n### Gotta capture 'em all\n\n- Hog whatever information tickles your pickle([anything that gratifies one's intellectual curiosity](https://news.ycombinator.com/newsguidelines.html)).\n- [[swipes]] are interesting/useful bits of... [[quotes|swipes.quotes]], [[excerpts|swipes.excerpts]], [[sayings|swipes.sayings]], [[phrases|swipes.phrases]]. Essentially, ideas, opinions that are swiped off from [[others|resources.people]].\n- Make no distinction between \"your\" ideas and ideas of \"others\", because if you vibe with an idea; it's already yours.\n- But also remember \"If you've time to consume, you've time to produce.\".\n- Use [raindrop.io](https://raindrop.io) to manage your URL bookmarks.\n- If you need to bookmark a webpage or an article all together, use [MarkDownload](https://chrome.google.com/webstore/detail/markdownload-markdown-web/pcmpcfapbekmbjjkdalcgopdkipoggdi?hl=en-GB) to rip the entire page. Let's call it a [[webmark|inbox.webmark]]; it belong in the `/inbox`.\n- Similar process is employed for capturing tweets using [tweet-to-markdown](https://github.com/kbravh/tweet-to-markdown) and it also belongs in the `/inbox`.\n\n### Don't force evolution\n\n- Let your second brain evolve at it's own pace.\n- The structure should never be _too_ rigid because its meant to take form by itself.\n\n### Noise & Signal\n\n- While capturing ideas left and right is recommended, make sure you're not harming your periods of focus.\n- Have impenetrable focus periods (use Pomodoro method) where you only care about the work on hand and nothing else.\n\n### Why do any of this?\n\n- Because I can't remember everything - there's a lot of information around that interests me and there isn't enough working memory installed in me.\n- So, I make notes - to remember, to create, to meditate, to think.\n- A bodybuilder's portfolio is their body - my portfolio is my wiki.\n- Not only am I making - whatever I know - tangible by writing it down. I know exactly where to look if I ever forget something.\n- I am at the beginning of my learning adventures. When I look back at it, I will know where I came from and how my thoughts evolved over time.\n- Plus, its really fun to nerd out.\n\n## Structure of this wiki\n\n- [[slipbox.Ontology]] explains the structure of this wiki and the tags, backlinks used in it.\n- Dendron takes care of the structure and hierarchy(mostly), but I insist on using tags for backwards compatibility.\n\n## Quicklinks\n\n- [[About me|archive.about]]\n- [Github](https://github.com/zubayrrr)\n- [Twitter](https://twitter.com/zoobhalu)\n- [Blog](https://zubayrali.in)\n- [Guestbook](https://www.yourworldoftext.com/~zubayrali/)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template","insertNote":{"initialValue":"templates"}},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Noetic Noggin","description":"Personal Wiki / Digital Garden","author":"Zubayr Ali","twitter":"zoobhalu"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteUrl":"localhost:3000","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}