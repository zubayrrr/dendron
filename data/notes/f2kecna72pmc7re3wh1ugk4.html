<h1 id="apache-spark"><a aria-hidden="true" class="anchor-heading" href="#apache-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Apache Spark</h1>
<ul>
<li>Areas: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">devlog.Data Engineering (Private)</a></li>
</ul>
<hr>
<h3 id="context-of-big-data"><a aria-hidden="true" class="anchor-heading" href="#context-of-big-data"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Context of Big Data</h3>
<p>Computing vs Data</p>
<ul>
<li>CPUs are only incrementally faster</li>
<li>Data storage keeps getting better and cheaper</li>
<li>Gathering data kees getting easier and cheaper and more important</li>
</ul>
<p>Data needs to be distributed and processed in parallel</p>
<p>Standard single-CPU software cannot scale up</p>
<p>Thus Spark was born.</p>
<h3 id="motivation-for-spark"><a aria-hidden="true" class="anchor-heading" href="#motivation-for-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Motivation for Spark</h3>
<p>A 2009 UC Berkeley project Matei Zaharia el al</p>
<ul>
<li>MapReduce was the king of large distributed computation</li>
<li>Inefficient for large applications and ML</li>
<li>Each step required another data pass, written as separate application</li>
</ul>
<p>Spark phase 1</p>
<ul>
<li>A simple functional programming API</li>
<li>Optimize multi-step applications</li>
<li>In-memory computation and data-sharing aross nodes</li>
</ul>
<p>Spark phase 2</p>
<ul>
<li>Interactive data science and ad-hoc computation</li>
<li>Spark shell and Spark SQL</li>
</ul>
<p>Spark phase 3</p>
<ul>
<li>Same engine, new libraries</li>
<li>ML, Streaming, GraphX</li>
</ul>
<h2 id="spark-first-principles"><a aria-hidden="true" class="anchor-heading" href="#spark-first-principles"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark First Principles</h2>
<p>Spark is unified computing engine and libraries for distributed data processing at scale.</p>
<p>It is an open source data processing engine(in-memory computing engine) to store and process data in real-time across various clusters of computers using simple programming constructs.</p>
<p>Big data = data that cannot fit on a standard computer, you'll need a cluster of computers that can process that data.(and Spark was made for this specific task)</p>
<ul>
<li>Spark supports a variety of data processing tasks
<ul>
<li>data loading</li>
<li>SQL queries</li>
<li>machine learning</li>
<li>streaming</li>
</ul>
</li>
<li>Unified in the sense:
<ul>
<li>consistent, composable APIs in multiple languages</li>
<li>optimizations across different libraries</li>
</ul>
</li>
<li>Computig engine in the sense that it is detached from data storage(where the data resides) and how the data is being fetched(I/O)</li>
<li>Libraries
<ul>
<li>standard: Spark SQL, MLlib, Streaming, GraphX</li>
<li>hundres of open-source third party libraries</li>
</ul>
</li>
</ul>
<h3 id="hadoop-vs-spark"><a aria-hidden="true" class="anchor-heading" href="#hadoop-vs-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a> vs Spark</h3>
<p>Hadoop</p>
<ul>
<li>Processing data using <a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a> in hadoop is slow.</li>
<li>Performs batch processing of data, intermittent data is written to <a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a> and read back from <a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a> which makes hadoop's MapReduce slow.</li>
<li>hadoop has more lines of code. Since it is written in Java, it takes more time to execute.</li>
<li>hadoop supports <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Kerberos (Private)</a> authentication, which is difficult to manage.</li>
</ul>
<p>Spark</p>
<ul>
<li>Spark processes data 100 times faster than MapReduce as it is done in-memory (like <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Tez (Private)</a>).</li>
<li>Performs both batch processing and real-time processing of data, most use cases are around real-time processing.</li>
<li>Spark hs fewer lines of code as it is implemented in <a href="/notes/caqic1zdwxl1ll1ogg1ec1f">Scala</a>.</li>
<li>Spark supports authentication via a shared secret. It can also run on <a href="/notes/6k7g4x7ws565lditv1n8hxm">YARN</a> leveraging the capability of <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Kerberos (Private)</a>.</li>
</ul>
<h3 id="spark-features"><a aria-hidden="true" class="anchor-heading" href="#spark-features"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark Features</h3>
<p>Fast processing</p>
<ul>
<li>Spark contains <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>s which saves time taken in reading and writing operations and hence, it runs almost ten to hundred times faster than hadoop.</li>
<li>In-memory computing - In Spark data is stored in the RAM, so it can access the data quickly and accelerate the speed of analytics.
<ul>
<li>Caching is different from In-memory computing, caching is mainly to support "read ahead" mechanism, where you have your data preloaded so that it can benefit further queries.</li>
<li>In-memory computing is more about lazy evaluation, data being loaded in memory only and only if a specific action is invoked.</li>
</ul>
</li>
<li>Flexible - polyglot</li>
<li>Fault tolerance - Spark contains <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>s (execution logic, temporary datasets which initially do not have any data loaded and data will only be loaded into RDDs when an execution is made) that are designed to handle the failure of any worker node in the cluster. Thus, it ensures that the loss of data reduces to zero.
<ul>
<li>Distributed</li>
</ul>
</li>
<li>Better analytics - Spark has a rich set of SQL queries, machine learning algorithms, complex analytics etc.</li>
</ul>
<h3 id="spark-misconceptions"><a aria-hidden="true" class="anchor-heading" href="#spark-misconceptions"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark Misconceptions</h3>
<p>Spark is not concerned with data sources</p>
<p>Spark is not part of <a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a>, it interacts with Hadoop and HDFS well but it is it's own thing.</p>
<h2 id="components-of-apache-spark"><a aria-hidden="true" class="anchor-heading" href="#components-of-apache-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Components of Apache Spark</h2>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.cct1xwxfey9.png"></p>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark Core (Private)</a></p>
<ul>
<li>Contains <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>s; core engine that takes care of processing</li>
</ul>
<p>See also: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DAG (Private)</a></p>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark SQL (Private)</a></p>
<ul>
<li>For working with structured data or data that can be structurized.</li>
<li>Has internal features such as Dataframes, datasets used to process structued data in much faster way.</li>
</ul>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark Streaming (Private)</a></p>
<ul>
<li>Allows you to create Spark Streaming applications which not only works on data that is being streamed/generated in but also transform the data and analyze it as it comes in.</li>
</ul>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark MLlib (Private)</a></p>
<ul>
<li>For building machine learning algorithms, predictive analytics, perscriptive, descriptive, preemptive, recommendation systems.</li>
</ul>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">GraphX (Private)</a></p>
<ul>
<li>Data naturally has a network kind of flow, data that be represented with graphs(not really pie charts) but network related data, some kind of relationships, twitter, fb, linkedin etc.</li>
</ul>
<h2 id="spark-architecture"><a aria-hidden="true" class="anchor-heading" href="#spark-architecture"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark Architecture</h2>
<p>Apache Spark uses a master-slave(technically, although it can also run in standalone mode) architecture that consists of a driver, that runs on a master node and multiple executors which run across the worker nodes in the cluster.</p>
<p>It can work with different clustering technologies such as Apache Mesos, <a href="/notes/6k7g4x7ws565lditv1n8hxm">YARN</a>. It can also work as standalone.</p>
<p>Master Node has a driver program; this driver program internally has SparkContext.</p>
<p>The Spark code behaves as a driver program and creates a SparkContext, which is gateway to all the Spark functionalities.</p>
<p>Driver program interacts with cluster manager(SparkContext, the entry point, takes the request to the cluster manager).</p>
<p>Cluster manager in terms of YARN is the <code>ResourceManager</code></p>
<p>Spark application runs as independent set of processes on a cluster.</p>
<p>The driver program and SparkContext takes care of the job execution within the cluster.</p>
<p>A job is split into multiple tasks that are distributed over the worker node.</p>
<p>When an <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a> is created in SparkContext, it can be distributed across various nodes.</p>
<p>Worker nodes are slaves that run different tasks.</p>
<hr>
<p>Spark Architecture is based on 2 important abstractions:</p>
<h3 id="rdd"><a aria-hidden="true" class="anchor-heading" href="#rdd"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a></h3>
<p>Resilient Distributed Datasets</p>
<p>Spark Core is embedded with RDDs, an immutable fault-tolerant(like files in <a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a>), distributed collection of objects that can be operated on in parallel.</p>
<p>It where the data will be loaded(or existing for processing), it can exist for shorter amount of time.</p>
<ul>
<li>RDDs are the fundamental units of data in Apache Spark that are split into partitions and can be executed on different nodes of a cluster. Implicit, lazy in nature, created whenever you use a method of SparkContext or when you do a transformation on an existing RDD or a dataset.</li>
<li>Each dataset in an RDD is divided into logical memory partitions that may be computed on different nodes of a cluster.</li>
<li>By default every RDD has 2 partitions, which can be customized while creating RDDs.</li>
<li>The more partitions you've the better the parallel processing.</li>
<li>RDDs are automatically split into partitions and can be executed upon different nodes by different taks in parallel, in-memory.</li>
</ul>
<p>There are mainly two operations that can be peformed on an RDD</p>
<h3 id="transformation"><a aria-hidden="true" class="anchor-heading" href="#transformation"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Transformation</h3>
<ul>
<li>These are operations (such as map, filter, join, union) that are peformed on an RDD that yields as new RDD containing the result.</li>
<li>They return a pointer to a new RDD. The original RDD cannot be changed. Spark is "lazy" and nothing will be executed unless an action is invoked.</li>
<li>It isn't necessarily reproducing a new set of data or RDD, but is a new "state". Think of it as step(s) in a program telling Spark how to get new data and what to do with it.</li>
<li><strong>Resilience</strong> is the ability to retrace steps from the beginning.</li>
</ul>
<h3 id="actions"><a aria-hidden="true" class="anchor-heading" href="#actions"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Actions</h3>
<ul>
<li>These are operations (reduce, first, count, collect, count, take save-as) that return a value after running a computation on an RDD.</li>
<li>They return values and force the transformations to actually take place.</li>
</ul>
<p>See also: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DAG (Private)</a></p>
<h3 id="dag-private"><a aria-hidden="true" class="anchor-heading" href="#dag-private"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DAG (Private)</a></h3>
<p>Directed Acyclic Graph</p>
<p>Spark has a DAG scheduler in the background which is tracking the transformations and steps which is part of your applications.</p>
<p>It is the scheduling layer of the Spark Architecture that implements stage-oriented scheduling and eliminates that hadoop <a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a> multistage execution model.</p>
<p>If you're processing data using Spark applications in a Spark platform(whether standalone or in a hadoop cluster), for every step you do as part of your application, it creates an <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>, it becomes part of your DAG. The DAG scheduler is already aware of what steps are involved in DAG hence it comes up with a plan for executing the all the steps within the DAG. <span class="underline">If only and only if an action is invoked.</span></p>
<p>Check <a href="https://data-flair.training/blogs/dag-in-apache-spark/">https://data-flair.training/blogs/dag-in-apache-spark/</a></p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.muzlc82nhqb.png"></p>
<p>See also: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark Cluster Managers (Private)</a></p>
<h2 id="spark-data-slicing"><a aria-hidden="true" class="anchor-heading" href="#spark-data-slicing"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark Data Slicing</h2>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.4ylqh1r70re.png"></p>
<p>in contrast with Slicing in <a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a></p>
<p>Slicing in <a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a> vs <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Slicing in Spark (Private)</a>
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.83j55a3bdv5.png"></p>
<hr>
<h2 id="dataframe-private"><a aria-hidden="true" class="anchor-heading" href="#dataframe-private"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DataFrame (Private)</a></h2>
<p>DataFrame is a distributed collection of data organized into named columns. ... DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.</p>
<p>Created a SparkSession(it allows creating, reading and writing DataFrames)</p>
<pre><code>val spark = SparkSession.builder()
    .appName("My Application")
    .config("spark.master", "local")
    .getOrCreate()
</code></pre>
<p>Reading a DataFrame from a file</p>
<pre><code>var firstDF = spark.read()
    .format("json")
    .option("inferSchema", "true")
    .load("path/to/file.json")
</code></pre>
<p>Performing operations on a DataFrame</p>
<pre><code>firstDF.show()
firstDF.printSchema()
firstDF.take(5)
</code></pre>
<p>Rows = unstructured data; and the information about the structure of the data is applied to the DataFrame in the form of a Schema</p>
<p>Schema = description of fields aka columns and their type</p>
<p>Spark types, they spark at runtime rather than compile time.</p>
<hr>
<h3 id="how-dataframes-work"><a aria-hidden="true" class="anchor-heading" href="#how-dataframes-work"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>How DataFrames Work</h3>
<p>Distributed spreadsheets with rows and columns, like a table that is split between multiple nodes in a Spark cluster, the information each Spark node recieves is the schema of the DataFrame anda few of the rows that compose the DataFrame.</p>
<p>Distributed collections of Rows conforming to a schema</p>
<p>DataFrames are:</p>
<ul>
<li>Immutable
<ul>
<li>Can't be changed once created</li>
<li>If you want to modify them you will have to create new DataFrames using transformations</li>
</ul>
</li>
</ul>
<p>Schema = list describing the column names and types</p>
<ul>
<li>Types are known to Spark when the DataFrame is being used, not at compile time(to make them available at compile time with Type safe Datasets)</li>
<li>Schema can hold arbitrary number of columns</li>
<li>All rows have the same structure</li>
<li>Rows do not have schema but they conform to the same structure</li>
</ul>
<!-- end list -->
<pre><code>val carsSchema = StructType(Array(
    StructField("Name", StringType),
    StructField("HorsePower", IntegerType),
    StructField("Acceleration", DoubleType)
))
</code></pre>
<h3 id="need-to-be-distributed"><a aria-hidden="true" class="anchor-heading" href="#need-to-be-distributed"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Need to be distributed</h3>
<p>These collections of rows need to be distributed, because either the data is too big for a single computer or it takes too long to process entier data on a single CPU.</p>
<h3 id="partitioning"><a aria-hidden="true" class="anchor-heading" href="#partitioning"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Partitioning</h3>
<ul>
<li>Splits the data into files, distributed between nodes in the cluster</li>
<li>Impacts the processing parallelism</li>
<li>More partitions may mean more parallelism but if you have 1000 partitions (1000 small files that compose your DataFrame) and a single node to process them all, parallelism will still be one because you only have one node to process all that data.</li>
<li>Inversely, if you have 1 partition and many nodes in your Spark cluster, only one node will have access to that partition and the parallelism would still be one.</li>
</ul>
<h3 id="transformations"><a aria-hidden="true" class="anchor-heading" href="#transformations"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Transformations</h3>
<ul>
<li>Narrow = one input partition contributes to at most one output partition(e.g map), partitioning is not changed in a DataFrame.
<ul>
<li>If you do a map, that would transform data row by row and so the partitioning is not changed, whereas</li>
</ul>
</li>
<li>Wide = input partitions(one or more) create many output partitions, so the partitioning of the DataFrame is changed.
<ul>
<li>If you do a sort, that will involve exchanging data between partitions in between nodes in the cluster.</li>
</ul>
</li>
<li>These operations are known as Shuffle = data exchange between cluster nodes.
<ul>
<li>Shuffling occurs in Wide transformations and its a massive performance topic, it can impact the time it takes for your jobs by orders of magnitude.</li>
</ul>
</li>
</ul>
<h3 id="computing-dataframes"><a aria-hidden="true" class="anchor-heading" href="#computing-dataframes"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Computing DataFrames</h3>
<p>How DataFrames work at runtime</p>
<p>Lazy evaluation</p>
<ul>
<li>Spark mechanism to wait until the last moment to execute the DF transformations</li>
</ul>
<p>Planning</p>
<ul>
<li>Spark compiles the DF transformations and dependencies into a graph before running any code, Spark will know before hand every single step that it will have to take including data exchanges between nodes before it actually starts loading or running any code.</li>
<li>Logical plan = DF dependency graph + narrow/wide transformations sequence</li>
<li>Physical plan = optimized seqence of steps(and it will know which node will execute which part of transformations) for nodes in the cluster.</li>
<li>Optimizations such as: avoiding multiple passes over the data or pushing down predecates in SQL or chaining multiple predecates or where clauses into one and so on.</li>
</ul>
<h3 id="transformations-vs-actions"><a aria-hidden="true" class="anchor-heading" href="#transformations-vs-actions"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Transformations vs Actions</h3>
<ul>
<li>A transformation descibes how new DFs are obtained (e.g map)</li>
<li>Action actually starts executing Spark code (e.g show, count)</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.rv3o8j0t3m.png"></p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.mymxbbtheoj.png"></p>
<h2 id="joins"><a aria-hidden="true" class="anchor-heading" href="#joins"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Joins</h2>
<p>Combine data from multiple DataFrames</p>
<ul>
<li>one(or more) columns from table 1 (left) is compared with one(or more) columns from table 2 (right)</li>
<li>if the condition passes, rows are combined</li>
<li>non-matching rows are distracted</li>
</ul>
<p>In Spark, Joins are Wide transformations(read: expensive!)
In order to compute a join, Spark scans the entire DFs from the entire clusters and the data is going to be moved around in between nodes, this involves shuffling which is expensive (in terms of performance).</p>
<hr>
<h2 id="tags"><a aria-hidden="true" class="anchor-heading" href="#tags"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Tags</h2>
<ol>
<li><a class="color-tag" style="--tag-color: #b79400;" href="/notes/e2nal5eqj2mu8d029dnoi5c">areas</a></li>
</ol>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/s5t9uscswvpo3p6gebui1ea">Big Data</a></li>
<li><a href="/notes/bojmgq30yfxpn44hgo0ofcf">DAG</a></li>
<li><a href="/notes/2fz1tdl3yy2s78wdmo7ql56">Data Engineering Roadmap</a></li>
<li><a href="/notes/q9l2kjgosndymzvlre2b3zs">DataFrame</a></li>
<li><a href="/notes/rttao2ttrcnyce4qro7cku2">MapReduce Examples</a></li>
<li><a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a></li>
<li><a href="/notes/c9r2joc5eh6lgdxcf1tl2xh">Maven Scala Spark</a></li>
<li><a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a></li>
<li><a href="/notes/88u97ancbt2rjk1r522bxt3">Slicing in Spark</a></li>
<li><a href="/notes/iholoz516z4p3yeo29eldm4">Spark Architecture</a></li>
<li><a href="/notes/llkslspv5gn64h1n4w6hzuc">Spark Cluster Managers</a></li>
<li><a href="/notes/uv0m9j6mm23y7zn5kte1avo">Spark Core</a></li>
<li><a href="/notes/mtez5e789o0yozjrbt75wua">Spark Datasets</a></li>
<li><a href="/notes/h0jp44pm8khupz9s02ebax4">Spark Misc</a></li>
<li><a href="/notes/wfzzq8sg88lzu3a64vpqa4p">Spark MLlib</a></li>
<li><a href="/notes/4umcol9f5n339vdmatdkrtf">Spark SQL</a></li>
<li><a href="/notes/x5x4p2nl61shtnu9ogxjrg9">Spark Streaming</a></li>
</ul>