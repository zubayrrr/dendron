<h1 id="hdfs"><a aria-hidden="true" class="anchor-heading" href="#hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>HDFS</h1>
<ul>
<li>Traditional filesystems are not designed for large files and fast streaming reads hence HDFS was introduced.</li>
<li>It is designed for "write once/read many" uses cases.</li>
<li>It is a distributed/spread out filesystem.</li>
<li>Requires low coherency/concurrency overhead
<ul>
<li>No random file writes, one user writing at a time. (can be a restriction/constraint)</li>
</ul>
</li>
<li>HDFS takes files and stripes them across different servers and then allows us to scan those files in parallel at the same time. Each chunk/slice is on a different server.</li>
<li>The goal is to <strong>move computation to data</strong>.</li>
<li>Uses a converged data-computation model; slice data file and place pieces on multiple computational nodes/servers.</li>
</ul>
<h3 id="namenodes-and-datanodes"><a aria-hidden="true" class="anchor-heading" href="#namenodes-and-datanodes"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>NameNodes and Datanodes</h3>
<ul>
<li>HDFS uses a director/worker model.</li>
<li>The director or the daemon is called the NameNode and acts as a metadata server (data about data) or "data traffic cop".</li>
<li>NameNode keeps metadata in memory for performance.</li>
<li>Provides a single filesystem namespace that is managed by the NameNode.</li>
<li>The workers are DataNode(s)(at least one); Data is stored on these nodes.</li>
<li>A secondary NameNode checkpoints NameNode metadata to disk, but does not provide failover.</li>
</ul>
<h3 id="hdfs-roles"><a aria-hidden="true" class="anchor-heading" href="#hdfs-roles"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>HDFS Roles</h3>
<ul>
<li>
<p>User client-layer talks with NameNode(transparent to user). Often called as <strong>Edge Node</strong></p>
</li>
<li>
<p>NameNode keeps track of metadata and uses DataNodes for actual data storage.</p>
</li>
<li>
<p>Data flows from client node directly to/from DataNodes.</p>
</li>
<li>
<p>Secondary NameNode checkpoints the metadata to disk but is not failover.</p>
</li>
<li>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.zt918wn73hk.png"></p>
</li>
</ul>
<h3 id="the-hdfs-file-system-namespace"><a aria-hidden="true" class="anchor-heading" href="#the-hdfs-file-system-namespace"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>The HDFS File System Namespace</h3>
<ul>
<li>Provides a hierarchical file system with files and directories.</li>
<li>Namespace path starts with <code>/</code> and provides user directories.</li>
<li>Users can create, remove, move, rename and copy files.</li>
<li>There are no <span class="underline">random reads or writes only appends</span>.</li>
<li>User must access HDFS through <a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a> client layer, not directly on DataNodes or NFSv3</li>
<li>Features include: High Availability(multiple NameNodes), Federation(spread namespace across multiple NameNodes), Snapshots(instant read-only point-in-time copies), and NFSv3 mounts.</li>
</ul>
<h3 id="replication-in-hdfs"><a aria-hidden="true" class="anchor-heading" href="#replication-in-hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Replication in HDFS</h3>
<ul>
<li>Data is replicated (default is 3 copies).</li>
<li>Replication is used for both redundancy and better availability on busy clusters.</li>
<li>At 3x repliaction a 320 MB file requires 1 GB of space in HDFS.</li>
<li>HDFS V3 supports erasure coding for achived data (reduces storage space).</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.1eqf63vf4xu.png"></p>
<h3 id="how-the-user-sees-hdfs"><a aria-hidden="true" class="anchor-heading" href="#how-the-user-sees-hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>How The User "Sees" HDFS</h3>
<ul>
<li>HDFS is a separate file system from the host server(edge node).</li>
<li>All Hadoop processing happens in HDFS, but first:
<ul>
<li>1. Data must be moved to edge node/server from user's source (local or web/cloud).</li>
<li>2. Data must be moved from edge node to HDFS using put and get client commands.</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.yxr0gacpr2.png"></p>
<hr>
<h2 id="hdfs-cli"><a aria-hidden="true" class="anchor-heading" href="#hdfs-cli"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>HDFS cli</h2>
<h4 id="make-a-dir-in-hdfs"><a aria-hidden="true" class="anchor-heading" href="#make-a-dir-in-hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Make a dir in HDFS</h4>
<p><code>hdfs dfs -mkdir stuff</code> will create a dir in home dir(since no path was supplied, /users/hands-on) named "stuff"</p>
<h4 id="copy-files-to-hdfs"><a aria-hidden="true" class="anchor-heading" href="#copy-files-to-hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Copy files to HDFS</h4>
<p><code>hdfs dfs -put war-and-peace.txt stuff</code></p>
<h4 id="copy-files-from-hdfs"><a aria-hidden="true" class="anchor-heading" href="#copy-files-from-hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Copy files from HDFS</h4>
<p>Copy files back to your local file system</p>
<p><code>hdfs dfs -get stuff/war-and-peace.txt war-and-peace-copy.txt</code></p>
<h4 id="copy-files-within-hdfs"><a aria-hidden="true" class="anchor-heading" href="#copy-files-within-hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Copy files within HDFS</h4>
<p><code>hdfs dfs -cp stuff/war-and-peace.txt copy-of-war-and-peace.txt</code></p>
<h4 id="delete-files-within-hdfs"><a aria-hidden="true" class="anchor-heading" href="#delete-files-within-hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Delete files within HDFS</h4>
<p><code>hdfs dfs -rm copy-of-war-and-peace.txt</code></p>
<h4 id="list-files-on-hdfs-userhands-on"><a aria-hidden="true" class="anchor-heading" href="#list-files-on-hdfs-userhands-on"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>List files on HDFS (/user/hands-on)</h4>
<p><code>hdfs dfs -ls</code></p>
<h4 id="delete-a-dir-in-hdfs"><a aria-hidden="true" class="anchor-heading" href="#delete-a-dir-in-hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Delete a dir in HDFS</h4>
<p><code>hdfs dfs -rm -r stuff</code></p>
<p>to skip the trash(if trash collection is enabled use <code>-skipTrash</code>)</p>
<h2 id="hdfs-web-interface"><a aria-hidden="true" class="anchor-heading" href="#hdfs-web-interface"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>HDFS Web Interface</h2>
<p>The HDFS web interaface: <code>http://127.0.0.1:50070</code>
The <a href="/notes/6k7g4x7ws565lditv1n8hxm">YARN</a> Jobs web interface: <code>http://127.0.0.1:8088</code>
Zeppelin Web Notebook: <code>http://127.0.0.9995</code></p>
<p>The HDFS web interace also features "Browse the file system" under the "Utilities" dropdown menu.</p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/f2kecna72pmc7re3wh1ugk4">Apache Spark</a></li>
<li><a href="/notes/s5t9uscswvpo3p6gebui1ea">Big Data</a></li>
<li><a href="/notes/2fz1tdl3yy2s78wdmo7ql56">Data Engineering Roadmap</a></li>
<li><a href="/notes/lz322a84xc913sor5r5c71u">Data Warehouse</a></li>
<li><a href="/notes/ffhm4kotv5wbd139ml9ch44">ETL</a></li>
<li><a href="/notes/30isnfzmqrvmmiii03d0chj">Hive</a></li>
<li><a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a></li>
<li><a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a></li>
</ul>