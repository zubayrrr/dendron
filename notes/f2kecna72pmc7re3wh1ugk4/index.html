<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Apache Spark</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Wiki / Digital Garden"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="zoobhalu"/><meta name="twitter:creator" content="zoobhalu"/><meta property="og:title" content="Apache Spark"/><meta property="og:description" content="Personal Wiki / Digital Garden"/><meta property="og:url" content="localhost:3000/notes/f2kecna72pmc7re3wh1ugk4/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="Invalid DateTime"/><meta property="article:modified_time" content="5/23/2022"/><meta property="article:tag" content="areas"/><link rel="canonical" href="localhost:3000/notes/f2kecna72pmc7re3wh1ugk4/"/><meta name="next-head-count" content="18"/><link rel="preload" href="/_next/static/css/cc2307f6fd3a2fec.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cc2307f6fd3a2fec.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-709abf7ab5f510de.js" defer=""></script><script src="/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/_next/static/chunks/main-c4b0e551a2150d17.js" defer=""></script><script src="/_next/static/chunks/pages/_app-62c5b93605efada7.js" defer=""></script><script src="/_next/static/chunks/78-13ae6acd5ce7ca5b.js" defer=""></script><script src="/_next/static/chunks/373-2f3879190a46a3d9.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-69449972c2a725d8.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_buildManifest.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_ssgManifest.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout site-layout" style="margin-top:64px"><section class="ant-layout site-layout"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="apache-spark"><a aria-hidden="true" class="anchor-heading" href="#apache-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Apache Spark</h1>
<ul>
<li>Areas: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">devlog.Data Engineering (Private)</a></li>
</ul>
<hr>
<h3 id="context-of-big-data"><a aria-hidden="true" class="anchor-heading" href="#context-of-big-data"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Context of Big Data</h3>
<p>Computing vs Data</p>
<ul>
<li>CPUs are only incrementally faster</li>
<li>Data storage keeps getting better and cheaper</li>
<li>Gathering data kees getting easier and cheaper and more important</li>
</ul>
<p>Data needs to be distributed and processed in parallel</p>
<p>Standard single-CPU software cannot scale up</p>
<p>Thus Spark was born.</p>
<h3 id="motivation-for-spark"><a aria-hidden="true" class="anchor-heading" href="#motivation-for-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Motivation for Spark</h3>
<p>A 2009 UC Berkeley project Matei Zaharia el al</p>
<ul>
<li>MapReduce was the king of large distributed computation</li>
<li>Inefficient for large applications and ML</li>
<li>Each step required another data pass, written as separate application</li>
</ul>
<p>Spark phase 1</p>
<ul>
<li>A simple functional programming API</li>
<li>Optimize multi-step applications</li>
<li>In-memory computation and data-sharing aross nodes</li>
</ul>
<p>Spark phase 2</p>
<ul>
<li>Interactive data science and ad-hoc computation</li>
<li>Spark shell and Spark SQL</li>
</ul>
<p>Spark phase 3</p>
<ul>
<li>Same engine, new libraries</li>
<li>ML, Streaming, GraphX</li>
</ul>
<h2 id="spark-first-principles"><a aria-hidden="true" class="anchor-heading" href="#spark-first-principles"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark First Principles</h2>
<p>Spark is unified computing engine and libraries for distributed data processing at scale.</p>
<p>It is an open source data processing engine(in-memory computing engine) to store and process data in real-time across various clusters of computers using simple programming constructs.</p>
<p>Big data = data that cannot fit on a standard computer, you'll need a cluster of computers that can process that data.(and Spark was made for this specific task)</p>
<ul>
<li>Spark supports a variety of data processing tasks
<ul>
<li>data loading</li>
<li>SQL queries</li>
<li>machine learning</li>
<li>streaming</li>
</ul>
</li>
<li>Unified in the sense:
<ul>
<li>consistent, composable APIs in multiple languages</li>
<li>optimizations across different libraries</li>
</ul>
</li>
<li>Computig engine in the sense that it is detached from data storage(where the data resides) and how the data is being fetched(I/O)</li>
<li>Libraries
<ul>
<li>standard: Spark SQL, MLlib, Streaming, GraphX</li>
<li>hundres of open-source third party libraries</li>
</ul>
</li>
</ul>
<h3 id="hadoop-vs-spark"><a aria-hidden="true" class="anchor-heading" href="#hadoop-vs-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a> vs Spark</h3>
<p>Hadoop</p>
<ul>
<li>Processing data using <a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a> in hadoop is slow.</li>
<li>Performs batch processing of data, intermittent data is written to <a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a> and read back from <a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a> which makes hadoop's MapReduce slow.</li>
<li>hadoop has more lines of code. Since it is written in Java, it takes more time to execute.</li>
<li>hadoop supports <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Kerberos (Private)</a> authentication, which is difficult to manage.</li>
</ul>
<p>Spark</p>
<ul>
<li>Spark processes data 100 times faster than MapReduce as it is done in-memory (like <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Tez (Private)</a>).</li>
<li>Performs both batch processing and real-time processing of data, most use cases are around real-time processing.</li>
<li>Spark hs fewer lines of code as it is implemented in <a href="/notes/caqic1zdwxl1ll1ogg1ec1f">Scala</a>.</li>
<li>Spark supports authentication via a shared secret. It can also run on <a href="/notes/6k7g4x7ws565lditv1n8hxm">YARN</a> leveraging the capability of <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Kerberos (Private)</a>.</li>
</ul>
<h3 id="spark-features"><a aria-hidden="true" class="anchor-heading" href="#spark-features"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark Features</h3>
<p>Fast processing</p>
<ul>
<li>Spark contains <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>s which saves time taken in reading and writing operations and hence, it runs almost ten to hundred times faster than hadoop.</li>
<li>In-memory computing - In Spark data is stored in the RAM, so it can access the data quickly and accelerate the speed of analytics.
<ul>
<li>Caching is different from In-memory computing, caching is mainly to support "read ahead" mechanism, where you have your data preloaded so that it can benefit further queries.</li>
<li>In-memory computing is more about lazy evaluation, data being loaded in memory only and only if a specific action is invoked.</li>
</ul>
</li>
<li>Flexible - polyglot</li>
<li>Fault tolerance - Spark contains <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>s (execution logic, temporary datasets which initially do not have any data loaded and data will only be loaded into RDDs when an execution is made) that are designed to handle the failure of any worker node in the cluster. Thus, it ensures that the loss of data reduces to zero.
<ul>
<li>Distributed</li>
</ul>
</li>
<li>Better analytics - Spark has a rich set of SQL queries, machine learning algorithms, complex analytics etc.</li>
</ul>
<h3 id="spark-misconceptions"><a aria-hidden="true" class="anchor-heading" href="#spark-misconceptions"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark Misconceptions</h3>
<p>Spark is not concerned with data sources</p>
<p>Spark is not part of <a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a>, it interacts with Hadoop and HDFS well but it is it's own thing.</p>
<h2 id="components-of-apache-spark"><a aria-hidden="true" class="anchor-heading" href="#components-of-apache-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Components of Apache Spark</h2>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.cct1xwxfey9.png"></p>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark Core (Private)</a></p>
<ul>
<li>Contains <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>s; core engine that takes care of processing</li>
</ul>
<p>See also: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DAG (Private)</a></p>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark SQL (Private)</a></p>
<ul>
<li>For working with structured data or data that can be structurized.</li>
<li>Has internal features such as Dataframes, datasets used to process structued data in much faster way.</li>
</ul>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark Streaming (Private)</a></p>
<ul>
<li>Allows you to create Spark Streaming applications which not only works on data that is being streamed/generated in but also transform the data and analyze it as it comes in.</li>
</ul>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark MLlib (Private)</a></p>
<ul>
<li>For building machine learning algorithms, predictive analytics, perscriptive, descriptive, preemptive, recommendation systems.</li>
</ul>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">GraphX (Private)</a></p>
<ul>
<li>Data naturally has a network kind of flow, data that be represented with graphs(not really pie charts) but network related data, some kind of relationships, twitter, fb, linkedin etc.</li>
</ul>
<h2 id="spark-architecture"><a aria-hidden="true" class="anchor-heading" href="#spark-architecture"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark Architecture</h2>
<p>Apache Spark uses a master-slave(technically, although it can also run in standalone mode) architecture that consists of a driver, that runs on a master node and multiple executors which run across the worker nodes in the cluster.</p>
<p>It can work with different clustering technologies such as Apache Mesos, <a href="/notes/6k7g4x7ws565lditv1n8hxm">YARN</a>. It can also work as standalone.</p>
<p>Master Node has a driver program; this driver program internally has SparkContext.</p>
<p>The Spark code behaves as a driver program and creates a SparkContext, which is gateway to all the Spark functionalities.</p>
<p>Driver program interacts with cluster manager(SparkContext, the entry point, takes the request to the cluster manager).</p>
<p>Cluster manager in terms of YARN is the <code>ResourceManager</code></p>
<p>Spark application runs as independent set of processes on a cluster.</p>
<p>The driver program and SparkContext takes care of the job execution within the cluster.</p>
<p>A job is split into multiple tasks that are distributed over the worker node.</p>
<p>When an <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a> is created in SparkContext, it can be distributed across various nodes.</p>
<p>Worker nodes are slaves that run different tasks.</p>
<hr>
<p>Spark Architecture is based on 2 important abstractions:</p>
<h3 id="rdd"><a aria-hidden="true" class="anchor-heading" href="#rdd"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a></h3>
<p>Resilient Distributed Datasets</p>
<p>Spark Core is embedded with RDDs, an immutable fault-tolerant(like files in <a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a>), distributed collection of objects that can be operated on in parallel.</p>
<p>It where the data will be loaded(or existing for processing), it can exist for shorter amount of time.</p>
<ul>
<li>RDDs are the fundamental units of data in Apache Spark that are split into partitions and can be executed on different nodes of a cluster. Implicit, lazy in nature, created whenever you use a method of SparkContext or when you do a transformation on an existing RDD or a dataset.</li>
<li>Each dataset in an RDD is divided into logical memory partitions that may be computed on different nodes of a cluster.</li>
<li>By default every RDD has 2 partitions, which can be customized while creating RDDs.</li>
<li>The more partitions you've the better the parallel processing.</li>
<li>RDDs are automatically split into partitions and can be executed upon different nodes by different taks in parallel, in-memory.</li>
</ul>
<p>There are mainly two operations that can be peformed on an RDD</p>
<h3 id="transformation"><a aria-hidden="true" class="anchor-heading" href="#transformation"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Transformation</h3>
<ul>
<li>These are operations (such as map, filter, join, union) that are peformed on an RDD that yields as new RDD containing the result.</li>
<li>They return a pointer to a new RDD. The original RDD cannot be changed. Spark is "lazy" and nothing will be executed unless an action is invoked.</li>
<li>It isn't necessarily reproducing a new set of data or RDD, but is a new "state". Think of it as step(s) in a program telling Spark how to get new data and what to do with it.</li>
<li><strong>Resilience</strong> is the ability to retrace steps from the beginning.</li>
</ul>
<h3 id="actions"><a aria-hidden="true" class="anchor-heading" href="#actions"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Actions</h3>
<ul>
<li>These are operations (reduce, first, count, collect, count, take save-as) that return a value after running a computation on an RDD.</li>
<li>They return values and force the transformations to actually take place.</li>
</ul>
<p>See also: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DAG (Private)</a></p>
<h3 id="dag-private"><a aria-hidden="true" class="anchor-heading" href="#dag-private"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DAG (Private)</a></h3>
<p>Directed Acyclic Graph</p>
<p>Spark has a DAG scheduler in the background which is tracking the transformations and steps which is part of your applications.</p>
<p>It is the scheduling layer of the Spark Architecture that implements stage-oriented scheduling and eliminates that hadoop <a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a> multistage execution model.</p>
<p>If you're processing data using Spark applications in a Spark platform(whether standalone or in a hadoop cluster), for every step you do as part of your application, it creates an <a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>, it becomes part of your DAG. The DAG scheduler is already aware of what steps are involved in DAG hence it comes up with a plan for executing the all the steps within the DAG. <span class="underline">If only and only if an action is invoked.</span></p>
<p>Check <a href="https://data-flair.training/blogs/dag-in-apache-spark/">https://data-flair.training/blogs/dag-in-apache-spark/</a></p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.muzlc82nhqb.png"></p>
<p>See also: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Spark Cluster Managers (Private)</a></p>
<h2 id="spark-data-slicing"><a aria-hidden="true" class="anchor-heading" href="#spark-data-slicing"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark Data Slicing</h2>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.4ylqh1r70re.png"></p>
<p>in contrast with Slicing in <a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a></p>
<p>Slicing in <a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a> vs <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Slicing in Spark (Private)</a>
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.83j55a3bdv5.png"></p>
<hr>
<h2 id="dataframe-private"><a aria-hidden="true" class="anchor-heading" href="#dataframe-private"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DataFrame (Private)</a></h2>
<p>DataFrame is a distributed collection of data organized into named columns. ... DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.</p>
<p>Created a SparkSession(it allows creating, reading and writing DataFrames)</p>
<pre><code>val spark = SparkSession.builder()
    .appName("My Application")
    .config("spark.master", "local")
    .getOrCreate()
</code></pre>
<p>Reading a DataFrame from a file</p>
<pre><code>var firstDF = spark.read()
    .format("json")
    .option("inferSchema", "true")
    .load("path/to/file.json")
</code></pre>
<p>Performing operations on a DataFrame</p>
<pre><code>firstDF.show()
firstDF.printSchema()
firstDF.take(5)
</code></pre>
<p>Rows = unstructured data; and the information about the structure of the data is applied to the DataFrame in the form of a Schema</p>
<p>Schema = description of fields aka columns and their type</p>
<p>Spark types, they spark at runtime rather than compile time.</p>
<hr>
<h3 id="how-dataframes-work"><a aria-hidden="true" class="anchor-heading" href="#how-dataframes-work"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>How DataFrames Work</h3>
<p>Distributed spreadsheets with rows and columns, like a table that is split between multiple nodes in a Spark cluster, the information each Spark node recieves is the schema of the DataFrame anda few of the rows that compose the DataFrame.</p>
<p>Distributed collections of Rows conforming to a schema</p>
<p>DataFrames are:</p>
<ul>
<li>Immutable
<ul>
<li>Can't be changed once created</li>
<li>If you want to modify them you will have to create new DataFrames using transformations</li>
</ul>
</li>
</ul>
<p>Schema = list describing the column names and types</p>
<ul>
<li>Types are known to Spark when the DataFrame is being used, not at compile time(to make them available at compile time with Type safe Datasets)</li>
<li>Schema can hold arbitrary number of columns</li>
<li>All rows have the same structure</li>
<li>Rows do not have schema but they conform to the same structure</li>
</ul>
<!-- end list -->
<pre><code>val carsSchema = StructType(Array(
    StructField("Name", StringType),
    StructField("HorsePower", IntegerType),
    StructField("Acceleration", DoubleType)
))
</code></pre>
<h3 id="need-to-be-distributed"><a aria-hidden="true" class="anchor-heading" href="#need-to-be-distributed"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Need to be distributed</h3>
<p>These collections of rows need to be distributed, because either the data is too big for a single computer or it takes too long to process entier data on a single CPU.</p>
<h3 id="partitioning"><a aria-hidden="true" class="anchor-heading" href="#partitioning"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Partitioning</h3>
<ul>
<li>Splits the data into files, distributed between nodes in the cluster</li>
<li>Impacts the processing parallelism</li>
<li>More partitions may mean more parallelism but if you have 1000 partitions (1000 small files that compose your DataFrame) and a single node to process them all, parallelism will still be one because you only have one node to process all that data.</li>
<li>Inversely, if you have 1 partition and many nodes in your Spark cluster, only one node will have access to that partition and the parallelism would still be one.</li>
</ul>
<h3 id="transformations"><a aria-hidden="true" class="anchor-heading" href="#transformations"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Transformations</h3>
<ul>
<li>Narrow = one input partition contributes to at most one output partition(e.g map), partitioning is not changed in a DataFrame.
<ul>
<li>If you do a map, that would transform data row by row and so the partitioning is not changed, whereas</li>
</ul>
</li>
<li>Wide = input partitions(one or more) create many output partitions, so the partitioning of the DataFrame is changed.
<ul>
<li>If you do a sort, that will involve exchanging data between partitions in between nodes in the cluster.</li>
</ul>
</li>
<li>These operations are known as Shuffle = data exchange between cluster nodes.
<ul>
<li>Shuffling occurs in Wide transformations and its a massive performance topic, it can impact the time it takes for your jobs by orders of magnitude.</li>
</ul>
</li>
</ul>
<h3 id="computing-dataframes"><a aria-hidden="true" class="anchor-heading" href="#computing-dataframes"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Computing DataFrames</h3>
<p>How DataFrames work at runtime</p>
<p>Lazy evaluation</p>
<ul>
<li>Spark mechanism to wait until the last moment to execute the DF transformations</li>
</ul>
<p>Planning</p>
<ul>
<li>Spark compiles the DF transformations and dependencies into a graph before running any code, Spark will know before hand every single step that it will have to take including data exchanges between nodes before it actually starts loading or running any code.</li>
<li>Logical plan = DF dependency graph + narrow/wide transformations sequence</li>
<li>Physical plan = optimized seqence of steps(and it will know which node will execute which part of transformations) for nodes in the cluster.</li>
<li>Optimizations such as: avoiding multiple passes over the data or pushing down predecates in SQL or chaining multiple predecates or where clauses into one and so on.</li>
</ul>
<h3 id="transformations-vs-actions"><a aria-hidden="true" class="anchor-heading" href="#transformations-vs-actions"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Transformations vs Actions</h3>
<ul>
<li>A transformation descibes how new DFs are obtained (e.g map)</li>
<li>Action actually starts executing Spark code (e.g show, count)</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.rv3o8j0t3m.png"></p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.mymxbbtheoj.png"></p>
<h2 id="joins"><a aria-hidden="true" class="anchor-heading" href="#joins"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Joins</h2>
<p>Combine data from multiple DataFrames</p>
<ul>
<li>one(or more) columns from table 1 (left) is compared with one(or more) columns from table 2 (right)</li>
<li>if the condition passes, rows are combined</li>
<li>non-matching rows are distracted</li>
</ul>
<p>In Spark, Joins are Wide transformations(read: expensive!)
In order to compute a join, Spark scans the entire DFs from the entire clusters and the data is going to be moved around in between nodes, this involves shuffling which is expensive (in terms of performance).</p>
<hr>
<h2 id="tags"><a aria-hidden="true" class="anchor-heading" href="#tags"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Tags</h2>
<ol>
<li><a class="color-tag" style="--tag-color: #b79400;" href="/notes/e2nal5eqj2mu8d029dnoi5c">areas</a></li>
</ol>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/s5t9uscswvpo3p6gebui1ea">Big Data</a></li>
<li><a href="/notes/bojmgq30yfxpn44hgo0ofcf">DAG</a></li>
<li><a href="/notes/2fz1tdl3yy2s78wdmo7ql56">Data Engineering Roadmap</a></li>
<li><a href="/notes/q9l2kjgosndymzvlre2b3zs">DataFrame</a></li>
<li><a href="/notes/rttao2ttrcnyce4qro7cku2">MapReduce Examples</a></li>
<li><a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a></li>
<li><a href="/notes/c9r2joc5eh6lgdxcf1tl2xh">Maven Scala Spark</a></li>
<li><a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a></li>
<li><a href="/notes/88u97ancbt2rjk1r522bxt3">Slicing in Spark</a></li>
<li><a href="/notes/iholoz516z4p3yeo29eldm4">Spark Architecture</a></li>
<li><a href="/notes/llkslspv5gn64h1n4w6hzuc">Spark Cluster Managers</a></li>
<li><a href="/notes/uv0m9j6mm23y7zn5kte1avo">Spark Core</a></li>
<li><a href="/notes/mtez5e789o0yozjrbt75wua">Spark Datasets</a></li>
<li><a href="/notes/h0jp44pm8khupz9s02ebax4">Spark Misc</a></li>
<li><a href="/notes/wfzzq8sg88lzu3a64vpqa4p">Spark MLlib</a></li>
<li><a href="/notes/4umcol9f5n339vdmatdkrtf">Spark SQL</a></li>
<li><a href="/notes/x5x4p2nl61shtnu9ogxjrg9">Spark Streaming</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#context-of-big-data" title="Context of Big Data">Context of Big Data</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#motivation-for-spark" title="Motivation for Spark">Motivation for Spark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark-first-principles" title="Spark First Principles">Spark First Principles</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#devloghadoop-vs-spark" title="devlog.hadoop vs Spark">devlog.hadoop vs Spark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark-features" title="Spark Features">Spark Features</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark-misconceptions" title="Spark Misconceptions">Spark Misconceptions</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#components-of-apache-spark" title="Components of Apache Spark">Components of Apache Spark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark-architecture" title="Spark Architecture">Spark Architecture</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#devlogrdd" title="devlog.rdd">devlog.rdd</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#transformation" title="Transformation">Transformation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#actions" title="Actions">Actions</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dag" title="DAG">DAG</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark-data-slicing" title="Spark Data Slicing">Spark Data Slicing</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dataframe" title="DataFrame">DataFrame</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#how-dataframes-work" title="How DataFrames Work">How DataFrames Work</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#need-to-be-distributed" title="Need to be distributed">Need to be distributed</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#partitioning" title="Partitioning">Partitioning</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#transformations" title="Transformations">Transformations</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#computing-dataframes" title="Computing DataFrames">Computing DataFrames</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#transformations-vs-actions" title="Transformations vs Actions">Transformations vs Actions</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#joins" title="Joins">Joins</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"f2kecna72pmc7re3wh1ugk4","title":"Apache Spark","desc":"","updated":1653318643716,"created":20211115133856116,"tags":["areas"],"custom":{},"fname":"devlog.apache spark","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"c230bd195def8a1291232025ba41ab16","links":[{"type":"frontmatterTag","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"tags.areas","alias":"areas","xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.Data Engineering","alias":"devlog.Data Engineering","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":37,"offset":37},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Data Engineering"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":65,"column":5,"offset":1878},"end":{"line":65,"column":22,"offset":1895},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":69,"column":25,"offset":1938},"end":{"line":69,"column":45,"offset":1958},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":70,"column":70,"offset":2047},"end":{"line":70,"column":85,"offset":2062},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":70,"column":105,"offset":2082},"end":{"line":70,"column":120,"offset":2097},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Kerberos","alias":"Kerberos","position":{"start":{"line":72,"column":19,"offset":2246},"end":{"line":72,"column":31,"offset":2258},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Kerberos"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Tez","alias":"Tez","position":{"start":{"line":76,"column":86,"offset":2398},"end":{"line":76,"column":93,"offset":2405},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Tez"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.scala","alias":"devlog.scala","position":{"start":{"line":78,"column":56,"offset":2578},"end":{"line":78,"column":72,"offset":2594},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.scala"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.yarn","alias":"devlog.yarn","position":{"start":{"line":79,"column":73,"offset":2668},"end":{"line":79,"column":88,"offset":2683},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.yarn"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Kerberos","alias":"Kerberos","position":{"start":{"line":79,"column":118,"offset":2713},"end":{"line":79,"column":130,"offset":2725},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Kerberos"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":85,"column":18,"offset":2782},"end":{"line":85,"column":32,"offset":2796},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":90,"column":36,"offset":3421},"end":{"line":90,"column":50,"offset":3435},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":98,"column":22,"offset":3931},"end":{"line":98,"column":39,"offset":3948},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark Core","alias":"Spark Core","position":{"start":{"line":104,"column":1,"offset":4133},"end":{"line":104,"column":15,"offset":4147},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark Core"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":106,"column":12,"offset":4160},"end":{"line":106,"column":26,"offset":4174},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"DAG","alias":"DAG","position":{"start":{"line":108,"column":11,"offset":4230},"end":{"line":108,"column":18,"offset":4237},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DAG"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark SQL","alias":"Spark SQL","position":{"start":{"line":110,"column":1,"offset":4239},"end":{"line":110,"column":14,"offset":4252},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark SQL"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark Streaming","alias":"Spark Streaming","position":{"start":{"line":115,"column":1,"offset":4428},"end":{"line":115,"column":20,"offset":4447},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark Streaming"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark MLlib","alias":"Spark MLlib","position":{"start":{"line":119,"column":1,"offset":4626},"end":{"line":119,"column":16,"offset":4641},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark MLlib"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"GraphX","alias":"GraphX","position":{"start":{"line":123,"column":1,"offset":4773},"end":{"line":123,"column":11,"offset":4783},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"GraphX"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.yarn","alias":"devlog.yarn","position":{"start":{"line":131,"column":74,"offset":5300},"end":{"line":131,"column":89,"offset":5315},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.yarn"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":147,"column":9,"offset":5983},"end":{"line":147,"column":23,"offset":5997},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":155,"column":5,"offset":6189},"end":{"line":155,"column":19,"offset":6203},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":159,"column":77,"offset":6313},"end":{"line":159,"column":92,"offset":6328},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"DAG","alias":"DAG","position":{"start":{"line":183,"column":11,"offset":8037},"end":{"line":183,"column":18,"offset":8044},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DAG"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"DAG","alias":"DAG","position":{"start":{"line":185,"column":5,"offset":8050},"end":{"line":185,"column":12,"offset":8057},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DAG"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":191,"column":123,"offset":8334},"end":{"line":191,"column":143,"offset":8354},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":193,"column":181,"offset":8564},"end":{"line":193,"column":195,"offset":8578},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Spark Cluster Managers","alias":"Spark Cluster Managers","position":{"start":{"line":199,"column":11,"offset":8990},"end":{"line":199,"column":37,"offset":9016},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Spark Cluster Managers"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":205,"column":29,"offset":9155},"end":{"line":205,"column":46,"offset":9172},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":207,"column":12,"offset":9185},"end":{"line":207,"column":29,"offset":9202},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"Slicing in Spark","alias":"Slicing in Spark","position":{"start":{"line":207,"column":33,"offset":9206},"end":{"line":207,"column":53,"offset":9226},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Slicing in Spark"}},{"type":"wiki","from":{"fname":"devlog.apache spark","id":"f2kecna72pmc7re3wh1ugk4","vaultName":"Dendron"},"value":"DataFrame","alias":"DataFrame","position":{"start":{"line":212,"column":4,"offset":9321},"end":{"line":212,"column":17,"offset":9334},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DataFrame"}},{"from":{"fname":"devlog.big data","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":127,"column":5,"offset":6045},"end":{"line":127,"column":28,"offset":6068},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.dag","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.data engineering roadmap","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":31,"column":3,"offset":998},"end":{"line":31,"column":26,"offset":1021},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.dataframe","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.mapreduce examples","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":93,"column":23,"offset":4115},"end":{"line":93,"column":46,"offset":4138},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.mapreduce","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":192,"column":23,"offset":8478},"end":{"line":192,"column":46,"offset":8501},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.mapreduce","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":289,"column":20,"offset":11861},"end":{"line":289,"column":43,"offset":11884},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.maven scala spark","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.rdd","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.slicing in spark","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark architecture","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark cluster managers","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark core","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark datasets","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark misc","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark mllib","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark sql","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"},{"from":{"fname":"devlog.spark streaming","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":33,"offset":33},"indent":[]},"value":"devlog.apache spark","alias":"devlog.apache spark"}],"anchors":{"context-of-big-data":{"type":"header","text":"Context of Big Data","value":"context-of-big-data","line":14,"column":0,"depth":3},"motivation-for-spark":{"type":"header","text":"Motivation for Spark","value":"motivation-for-spark","line":28,"column":0,"depth":3},"spark-first-principles":{"type":"header","text":"Spark First Principles","value":"spark-first-principles","line":52,"column":0,"depth":2},"devloghadoop-vs-spark":{"type":"header","text":"devlog.hadoop vs Spark","value":"devloghadoop-vs-spark","line":73,"column":0,"depth":3},"spark-features":{"type":"header","text":"Spark Features","value":"spark-features","line":89,"column":0,"depth":3},"spark-misconceptions":{"type":"header","text":"Spark Misconceptions","value":"spark-misconceptions","line":102,"column":0,"depth":3},"components-of-apache-spark":{"type":"header","text":"Components of Apache Spark","value":"components-of-apache-spark","line":108,"column":0,"depth":2},"spark-architecture":{"type":"header","text":"Spark Architecture","value":"spark-architecture","line":135,"column":0,"depth":2},"devlogrdd":{"type":"header","text":"devlog.rdd","value":"devlogrdd","line":163,"column":0,"depth":3},"transformation":{"type":"header","text":"Transformation","value":"transformation","line":179,"column":0,"depth":3},"actions":{"type":"header","text":"Actions","value":"actions","line":186,"column":0,"depth":3},"dag":{"type":"header","text":"DAG","value":"dag","line":193,"column":0,"depth":3},"spark-data-slicing":{"type":"header","text":"Spark Data Slicing","value":"spark-data-slicing","line":209,"column":0,"depth":2},"dataframe":{"type":"header","text":"DataFrame","value":"dataframe","line":220,"column":0,"depth":2},"how-dataframes-work":{"type":"header","text":"How DataFrames Work","value":"how-dataframes-work","line":252,"column":0,"depth":3},"need-to-be-distributed":{"type":"header","text":"Need to be distributed","value":"need-to-be-distributed","line":279,"column":0,"depth":3},"partitioning":{"type":"header","text":"Partitioning","value":"partitioning","line":283,"column":0,"depth":3},"transformations":{"type":"header","text":"Transformations","value":"transformations","line":290,"column":0,"depth":3},"computing-dataframes":{"type":"header","text":"Computing DataFrames","value":"computing-dataframes","line":299,"column":0,"depth":3},"transformations-vs-actions":{"type":"header","text":"Transformations vs Actions","value":"transformations-vs-actions","line":314,"column":0,"depth":3},"joins":{"type":"header","text":"Joins","value":"joins","line":323,"column":0,"depth":2}},"children":[],"parent":"9gtn7g40cvqui0sifl1s7t5","data":{}},"body":"\u003ch1 id=\"apache-spark\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#apache-spark\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eApache Spark\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eAreas: \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003edevlog.Data Engineering (Private)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"context-of-big-data\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#context-of-big-data\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eContext of Big Data\u003c/h3\u003e\n\u003cp\u003eComputing vs Data\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPUs are only incrementally faster\u003c/li\u003e\n\u003cli\u003eData storage keeps getting better and cheaper\u003c/li\u003e\n\u003cli\u003eGathering data kees getting easier and cheaper and more important\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eData needs to be distributed and processed in parallel\u003c/p\u003e\n\u003cp\u003eStandard single-CPU software cannot scale up\u003c/p\u003e\n\u003cp\u003eThus Spark was born.\u003c/p\u003e\n\u003ch3 id=\"motivation-for-spark\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#motivation-for-spark\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMotivation for Spark\u003c/h3\u003e\n\u003cp\u003eA 2009 UC Berkeley project Matei Zaharia el al\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMapReduce was the king of large distributed computation\u003c/li\u003e\n\u003cli\u003eInefficient for large applications and ML\u003c/li\u003e\n\u003cli\u003eEach step required another data pass, written as separate application\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSpark phase 1\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple functional programming API\u003c/li\u003e\n\u003cli\u003eOptimize multi-step applications\u003c/li\u003e\n\u003cli\u003eIn-memory computation and data-sharing aross nodes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSpark phase 2\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInteractive data science and ad-hoc computation\u003c/li\u003e\n\u003cli\u003eSpark shell and Spark SQL\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSpark phase 3\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSame engine, new libraries\u003c/li\u003e\n\u003cli\u003eML, Streaming, GraphX\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"spark-first-principles\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-first-principles\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSpark First Principles\u003c/h2\u003e\n\u003cp\u003eSpark is unified computing engine and libraries for distributed data processing at scale.\u003c/p\u003e\n\u003cp\u003eIt is an open source data processing engine(in-memory computing engine) to store and process data in real-time across various clusters of computers using simple programming constructs.\u003c/p\u003e\n\u003cp\u003eBig data = data that cannot fit on a standard computer, you'll need a cluster of computers that can process that data.(and Spark was made for this specific task)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpark supports a variety of data processing tasks\n\u003cul\u003e\n\u003cli\u003edata loading\u003c/li\u003e\n\u003cli\u003eSQL queries\u003c/li\u003e\n\u003cli\u003emachine learning\u003c/li\u003e\n\u003cli\u003estreaming\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUnified in the sense:\n\u003cul\u003e\n\u003cli\u003econsistent, composable APIs in multiple languages\u003c/li\u003e\n\u003cli\u003eoptimizations across different libraries\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eComputig engine in the sense that it is detached from data storage(where the data resides) and how the data is being fetched(I/O)\u003c/li\u003e\n\u003cli\u003eLibraries\n\u003cul\u003e\n\u003cli\u003estandard: Spark SQL, MLlib, Streaming, GraphX\u003c/li\u003e\n\u003cli\u003ehundres of open-source third party libraries\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hadoop-vs-spark\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#hadoop-vs-spark\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003ca href=\"/notes/85w31vcdf3bjnm0yxh72ygf\"\u003eHadoop\u003c/a\u003e vs Spark\u003c/h3\u003e\n\u003cp\u003eHadoop\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProcessing data using \u003ca href=\"/notes/j6kfx6ziqg91r356nur9cwy\"\u003eMapReduce\u003c/a\u003e in hadoop is slow.\u003c/li\u003e\n\u003cli\u003ePerforms batch processing of data, intermittent data is written to \u003ca href=\"/notes/kdddo1f7ltfsuwhexj4s535\"\u003eHDFS\u003c/a\u003e and read back from \u003ca href=\"/notes/kdddo1f7ltfsuwhexj4s535\"\u003eHDFS\u003c/a\u003e which makes hadoop's MapReduce slow.\u003c/li\u003e\n\u003cli\u003ehadoop has more lines of code. Since it is written in Java, it takes more time to execute.\u003c/li\u003e\n\u003cli\u003ehadoop supports \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eKerberos (Private)\u003c/a\u003e authentication, which is difficult to manage.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSpark\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpark processes data 100 times faster than MapReduce as it is done in-memory (like \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eTez (Private)\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003ePerforms both batch processing and real-time processing of data, most use cases are around real-time processing.\u003c/li\u003e\n\u003cli\u003eSpark hs fewer lines of code as it is implemented in \u003ca href=\"/notes/caqic1zdwxl1ll1ogg1ec1f\"\u003eScala\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eSpark supports authentication via a shared secret. It can also run on \u003ca href=\"/notes/6k7g4x7ws565lditv1n8hxm\"\u003eYARN\u003c/a\u003e leveraging the capability of \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eKerberos (Private)\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"spark-features\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-features\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSpark Features\u003c/h3\u003e\n\u003cp\u003eFast processing\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpark contains \u003ca href=\"/notes/2vn2g9mhmi624b83rl5se0k\"\u003eRDD\u003c/a\u003es which saves time taken in reading and writing operations and hence, it runs almost ten to hundred times faster than hadoop.\u003c/li\u003e\n\u003cli\u003eIn-memory computing - In Spark data is stored in the RAM, so it can access the data quickly and accelerate the speed of analytics.\n\u003cul\u003e\n\u003cli\u003eCaching is different from In-memory computing, caching is mainly to support \"read ahead\" mechanism, where you have your data preloaded so that it can benefit further queries.\u003c/li\u003e\n\u003cli\u003eIn-memory computing is more about lazy evaluation, data being loaded in memory only and only if a specific action is invoked.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFlexible - polyglot\u003c/li\u003e\n\u003cli\u003eFault tolerance - Spark contains \u003ca href=\"/notes/2vn2g9mhmi624b83rl5se0k\"\u003eRDD\u003c/a\u003es (execution logic, temporary datasets which initially do not have any data loaded and data will only be loaded into RDDs when an execution is made) that are designed to handle the failure of any worker node in the cluster. Thus, it ensures that the loss of data reduces to zero.\n\u003cul\u003e\n\u003cli\u003eDistributed\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBetter analytics - Spark has a rich set of SQL queries, machine learning algorithms, complex analytics etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"spark-misconceptions\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-misconceptions\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSpark Misconceptions\u003c/h3\u003e\n\u003cp\u003eSpark is not concerned with data sources\u003c/p\u003e\n\u003cp\u003eSpark is not part of \u003ca href=\"/notes/85w31vcdf3bjnm0yxh72ygf\"\u003eHadoop\u003c/a\u003e, it interacts with Hadoop and HDFS well but it is it's own thing.\u003c/p\u003e\n\u003ch2 id=\"components-of-apache-spark\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#components-of-apache-spark\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eComponents of Apache Spark\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.cct1xwxfey9.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eSpark Core (Private)\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eContains \u003ca href=\"/notes/2vn2g9mhmi624b83rl5se0k\"\u003eRDD\u003c/a\u003es; core engine that takes care of processing\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee also: \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eDAG (Private)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eSpark SQL (Private)\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor working with structured data or data that can be structurized.\u003c/li\u003e\n\u003cli\u003eHas internal features such as Dataframes, datasets used to process structued data in much faster way.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eSpark Streaming (Private)\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAllows you to create Spark Streaming applications which not only works on data that is being streamed/generated in but also transform the data and analyze it as it comes in.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eSpark MLlib (Private)\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor building machine learning algorithms, predictive analytics, perscriptive, descriptive, preemptive, recommendation systems.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eGraphX (Private)\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eData naturally has a network kind of flow, data that be represented with graphs(not really pie charts) but network related data, some kind of relationships, twitter, fb, linkedin etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"spark-architecture\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-architecture\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSpark Architecture\u003c/h2\u003e\n\u003cp\u003eApache Spark uses a master-slave(technically, although it can also run in standalone mode) architecture that consists of a driver, that runs on a master node and multiple executors which run across the worker nodes in the cluster.\u003c/p\u003e\n\u003cp\u003eIt can work with different clustering technologies such as Apache Mesos, \u003ca href=\"/notes/6k7g4x7ws565lditv1n8hxm\"\u003eYARN\u003c/a\u003e. It can also work as standalone.\u003c/p\u003e\n\u003cp\u003eMaster Node has a driver program; this driver program internally has SparkContext.\u003c/p\u003e\n\u003cp\u003eThe Spark code behaves as a driver program and creates a SparkContext, which is gateway to all the Spark functionalities.\u003c/p\u003e\n\u003cp\u003eDriver program interacts with cluster manager(SparkContext, the entry point, takes the request to the cluster manager).\u003c/p\u003e\n\u003cp\u003eCluster manager in terms of YARN is the \u003ccode\u003eResourceManager\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSpark application runs as independent set of processes on a cluster.\u003c/p\u003e\n\u003cp\u003eThe driver program and SparkContext takes care of the job execution within the cluster.\u003c/p\u003e\n\u003cp\u003eA job is split into multiple tasks that are distributed over the worker node.\u003c/p\u003e\n\u003cp\u003eWhen an \u003ca href=\"/notes/2vn2g9mhmi624b83rl5se0k\"\u003eRDD\u003c/a\u003e is created in SparkContext, it can be distributed across various nodes.\u003c/p\u003e\n\u003cp\u003eWorker nodes are slaves that run different tasks.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eSpark Architecture is based on 2 important abstractions:\u003c/p\u003e\n\u003ch3 id=\"rdd\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#rdd\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003ca href=\"/notes/2vn2g9mhmi624b83rl5se0k\"\u003eRDD\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eResilient Distributed Datasets\u003c/p\u003e\n\u003cp\u003eSpark Core is embedded with RDDs, an immutable fault-tolerant(like files in \u003ca href=\"/notes/kdddo1f7ltfsuwhexj4s535\"\u003eHDFS\u003c/a\u003e), distributed collection of objects that can be operated on in parallel.\u003c/p\u003e\n\u003cp\u003eIt where the data will be loaded(or existing for processing), it can exist for shorter amount of time.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRDDs are the fundamental units of data in Apache Spark that are split into partitions and can be executed on different nodes of a cluster. Implicit, lazy in nature, created whenever you use a method of SparkContext or when you do a transformation on an existing RDD or a dataset.\u003c/li\u003e\n\u003cli\u003eEach dataset in an RDD is divided into logical memory partitions that may be computed on different nodes of a cluster.\u003c/li\u003e\n\u003cli\u003eBy default every RDD has 2 partitions, which can be customized while creating RDDs.\u003c/li\u003e\n\u003cli\u003eThe more partitions you've the better the parallel processing.\u003c/li\u003e\n\u003cli\u003eRDDs are automatically split into partitions and can be executed upon different nodes by different taks in parallel, in-memory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are mainly two operations that can be peformed on an RDD\u003c/p\u003e\n\u003ch3 id=\"transformation\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#transformation\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eTransformation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThese are operations (such as map, filter, join, union) that are peformed on an RDD that yields as new RDD containing the result.\u003c/li\u003e\n\u003cli\u003eThey return a pointer to a new RDD. The original RDD cannot be changed. Spark is \"lazy\" and nothing will be executed unless an action is invoked.\u003c/li\u003e\n\u003cli\u003eIt isn't necessarily reproducing a new set of data or RDD, but is a new \"state\". Think of it as step(s) in a program telling Spark how to get new data and what to do with it.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResilience\u003c/strong\u003e is the ability to retrace steps from the beginning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"actions\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#actions\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eActions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThese are operations (reduce, first, count, collect, count, take save-as) that return a value after running a computation on an RDD.\u003c/li\u003e\n\u003cli\u003eThey return values and force the transformations to actually take place.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee also: \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eDAG (Private)\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"dag-private\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#dag-private\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eDAG (Private)\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eDirected Acyclic Graph\u003c/p\u003e\n\u003cp\u003eSpark has a DAG scheduler in the background which is tracking the transformations and steps which is part of your applications.\u003c/p\u003e\n\u003cp\u003eIt is the scheduling layer of the Spark Architecture that implements stage-oriented scheduling and eliminates that hadoop \u003ca href=\"/notes/j6kfx6ziqg91r356nur9cwy\"\u003eMapReduce\u003c/a\u003e multistage execution model.\u003c/p\u003e\n\u003cp\u003eIf you're processing data using Spark applications in a Spark platform(whether standalone or in a hadoop cluster), for every step you do as part of your application, it creates an \u003ca href=\"/notes/2vn2g9mhmi624b83rl5se0k\"\u003eRDD\u003c/a\u003e, it becomes part of your DAG. The DAG scheduler is already aware of what steps are involved in DAG hence it comes up with a plan for executing the all the steps within the DAG. \u003cspan class=\"underline\"\u003eIf only and only if an action is invoked.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eCheck \u003ca href=\"https://data-flair.training/blogs/dag-in-apache-spark/\"\u003ehttps://data-flair.training/blogs/dag-in-apache-spark/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.muzlc82nhqb.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSee also: \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eSpark Cluster Managers (Private)\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"spark-data-slicing\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-data-slicing\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSpark Data Slicing\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.4ylqh1r70re.png\"\u003e\u003c/p\u003e\n\u003cp\u003ein contrast with Slicing in \u003ca href=\"/notes/85w31vcdf3bjnm0yxh72ygf\"\u003eHadoop\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSlicing in \u003ca href=\"/notes/85w31vcdf3bjnm0yxh72ygf\"\u003eHadoop\u003c/a\u003e vs \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eSlicing in Spark (Private)\u003c/a\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.83j55a3bdv5.png\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"dataframe-private\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#dataframe-private\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eDataFrame (Private)\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eDataFrame is a distributed collection of data organized into named columns. ... DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\u003c/p\u003e\n\u003cp\u003eCreated a SparkSession(it allows creating, reading and writing DataFrames)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval spark = SparkSession.builder()\n    .appName(\"My Application\")\n    .config(\"spark.master\", \"local\")\n    .getOrCreate()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReading a DataFrame from a file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003evar firstDF = spark.read()\n    .format(\"json\")\n    .option(\"inferSchema\", \"true\")\n    .load(\"path/to/file.json\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePerforming operations on a DataFrame\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efirstDF.show()\nfirstDF.printSchema()\nfirstDF.take(5)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRows = unstructured data; and the information about the structure of the data is applied to the DataFrame in the form of a Schema\u003c/p\u003e\n\u003cp\u003eSchema = description of fields aka columns and their type\u003c/p\u003e\n\u003cp\u003eSpark types, they spark at runtime rather than compile time.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-dataframes-work\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#how-dataframes-work\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eHow DataFrames Work\u003c/h3\u003e\n\u003cp\u003eDistributed spreadsheets with rows and columns, like a table that is split between multiple nodes in a Spark cluster, the information each Spark node recieves is the schema of the DataFrame anda few of the rows that compose the DataFrame.\u003c/p\u003e\n\u003cp\u003eDistributed collections of Rows conforming to a schema\u003c/p\u003e\n\u003cp\u003eDataFrames are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImmutable\n\u003cul\u003e\n\u003cli\u003eCan't be changed once created\u003c/li\u003e\n\u003cli\u003eIf you want to modify them you will have to create new DataFrames using transformations\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSchema = list describing the column names and types\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTypes are known to Spark when the DataFrame is being used, not at compile time(to make them available at compile time with Type safe Datasets)\u003c/li\u003e\n\u003cli\u003eSchema can hold arbitrary number of columns\u003c/li\u003e\n\u003cli\u003eAll rows have the same structure\u003c/li\u003e\n\u003cli\u003eRows do not have schema but they conform to the same structure\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- end list --\u003e\n\u003cpre\u003e\u003ccode\u003eval carsSchema = StructType(Array(\n    StructField(\"Name\", StringType),\n    StructField(\"HorsePower\", IntegerType),\n    StructField(\"Acceleration\", DoubleType)\n))\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"need-to-be-distributed\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#need-to-be-distributed\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eNeed to be distributed\u003c/h3\u003e\n\u003cp\u003eThese collections of rows need to be distributed, because either the data is too big for a single computer or it takes too long to process entier data on a single CPU.\u003c/p\u003e\n\u003ch3 id=\"partitioning\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#partitioning\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePartitioning\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSplits the data into files, distributed between nodes in the cluster\u003c/li\u003e\n\u003cli\u003eImpacts the processing parallelism\u003c/li\u003e\n\u003cli\u003eMore partitions may mean more parallelism but if you have 1000 partitions (1000 small files that compose your DataFrame) and a single node to process them all, parallelism will still be one because you only have one node to process all that data.\u003c/li\u003e\n\u003cli\u003eInversely, if you have 1 partition and many nodes in your Spark cluster, only one node will have access to that partition and the parallelism would still be one.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"transformations\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#transformations\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eTransformations\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNarrow = one input partition contributes to at most one output partition(e.g map), partitioning is not changed in a DataFrame.\n\u003cul\u003e\n\u003cli\u003eIf you do a map, that would transform data row by row and so the partitioning is not changed, whereas\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWide = input partitions(one or more) create many output partitions, so the partitioning of the DataFrame is changed.\n\u003cul\u003e\n\u003cli\u003eIf you do a sort, that will involve exchanging data between partitions in between nodes in the cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThese operations are known as Shuffle = data exchange between cluster nodes.\n\u003cul\u003e\n\u003cli\u003eShuffling occurs in Wide transformations and its a massive performance topic, it can impact the time it takes for your jobs by orders of magnitude.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"computing-dataframes\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computing-dataframes\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eComputing DataFrames\u003c/h3\u003e\n\u003cp\u003eHow DataFrames work at runtime\u003c/p\u003e\n\u003cp\u003eLazy evaluation\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpark mechanism to wait until the last moment to execute the DF transformations\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePlanning\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSpark compiles the DF transformations and dependencies into a graph before running any code, Spark will know before hand every single step that it will have to take including data exchanges between nodes before it actually starts loading or running any code.\u003c/li\u003e\n\u003cli\u003eLogical plan = DF dependency graph + narrow/wide transformations sequence\u003c/li\u003e\n\u003cli\u003ePhysical plan = optimized seqence of steps(and it will know which node will execute which part of transformations) for nodes in the cluster.\u003c/li\u003e\n\u003cli\u003eOptimizations such as: avoiding multiple passes over the data or pushing down predecates in SQL or chaining multiple predecates or where clauses into one and so on.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"transformations-vs-actions\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#transformations-vs-actions\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eTransformations vs Actions\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA transformation descibes how new DFs are obtained (e.g map)\u003c/li\u003e\n\u003cli\u003eAction actually starts executing Spark code (e.g show, count)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.rv3o8j0t3m.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.mymxbbtheoj.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"joins\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#joins\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eJoins\u003c/h2\u003e\n\u003cp\u003eCombine data from multiple DataFrames\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eone(or more) columns from table 1 (left) is compared with one(or more) columns from table 2 (right)\u003c/li\u003e\n\u003cli\u003eif the condition passes, rows are combined\u003c/li\u003e\n\u003cli\u003enon-matching rows are distracted\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn Spark, Joins are Wide transformations(read: expensive!)\nIn order to compute a join, Spark scans the entire DFs from the entire clusters and the data is going to be moved around in between nodes, this involves shuffling which is expensive (in terms of performance).\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"tags\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#tags\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eTags\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca class=\"color-tag\" style=\"--tag-color: #b79400;\" href=\"/notes/e2nal5eqj2mu8d029dnoi5c\"\u003eareas\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/s5t9uscswvpo3p6gebui1ea\"\u003eBig Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/bojmgq30yfxpn44hgo0ofcf\"\u003eDAG\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/2fz1tdl3yy2s78wdmo7ql56\"\u003eData Engineering Roadmap\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/q9l2kjgosndymzvlre2b3zs\"\u003eDataFrame\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/rttao2ttrcnyce4qro7cku2\"\u003eMapReduce Examples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/j6kfx6ziqg91r356nur9cwy\"\u003eMapReduce\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/c9r2joc5eh6lgdxcf1tl2xh\"\u003eMaven Scala Spark\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/2vn2g9mhmi624b83rl5se0k\"\u003eRDD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/88u97ancbt2rjk1r522bxt3\"\u003eSlicing in Spark\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/iholoz516z4p3yeo29eldm4\"\u003eSpark Architecture\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/llkslspv5gn64h1n4w6hzuc\"\u003eSpark Cluster Managers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/uv0m9j6mm23y7zn5kte1avo\"\u003eSpark Core\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/mtez5e789o0yozjrbt75wua\"\u003eSpark Datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/h0jp44pm8khupz9s02ebax4\"\u003eSpark Misc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/wfzzq8sg88lzu3a64vpqa4p\"\u003eSpark MLlib\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/4umcol9f5n339vdmatdkrtf\"\u003eSpark SQL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/x5x4p2nl61shtnu9ogxjrg9\"\u003eSpark Streaming\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"3nfl4nvv516muyzozhcwrw8","title":"/root","desc":"","updated":1655559901157,"created":1637610830605,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"581715455a6f0f7a699209e8521b4acf","links":[{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"my","position":{"start":{"line":4,"column":9,"offset":37},"end":{"line":4,"column":29,"offset":57},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":20,"column":111,"offset":1051},"end":{"line":20,"column":117,"offset":1057},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":21,"column":3,"offset":1198},"end":{"line":21,"column":9,"offset":1204},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes","alias":"swipes","position":{"start":{"line":27,"column":3,"offset":1724},"end":{"line":27,"column":13,"offset":1734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.quotes","alias":"quotes","position":{"start":{"line":27,"column":48,"offset":1769},"end":{"line":27,"column":72,"offset":1793},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.quotes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.excerpts","alias":"excerpts","position":{"start":{"line":27,"column":74,"offset":1795},"end":{"line":27,"column":102,"offset":1823},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.excerpts"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.sayings","alias":"sayings","position":{"start":{"line":27,"column":104,"offset":1825},"end":{"line":27,"column":130,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.sayings"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.phrases","alias":"phrases","position":{"start":{"line":27,"column":132,"offset":1853},"end":{"line":27,"column":158,"offset":1879},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.phrases"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"resources.people","alias":"others","position":{"start":{"line":27,"column":214,"offset":1935},"end":{"line":27,"column":241,"offset":1962},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"resources.people"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"inbox.webmark","alias":"webmark","position":{"start":{"line":31,"column":235,"offset":2463},"end":{"line":31,"column":260,"offset":2488},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"inbox.webmark"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"slipbox.Ontology","alias":"slipbox.Ontology","position":{"start":{"line":55,"column":3,"offset":3735},"end":{"line":55,"column":23,"offset":3755},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"slipbox.Ontology"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"About me","position":{"start":{"line":60,"column":3,"offset":3963},"end":{"line":60,"column":29,"offset":3989},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}}],"anchors":{"welcome-to-noetic-noggin":{"type":"header","text":"Welcome to Noetic Noggin","value":"welcome-to-noetic-noggin","line":8,"column":0,"depth":1},"principles":{"type":"header","text":"Principles","value":"principles","line":18,"column":0,"depth":2},"all-notes-should-be-relative-to-me":{"type":"header","text":"All notes should be relative to me.","value":"all-notes-should-be-relative-to-me","line":20,"column":0,"depth":3},"gotta-capture-em-all":{"type":"header","text":"Gotta capture 'em all","value":"gotta-capture-em-all","line":30,"column":0,"depth":3},"dont-force-evolution":{"type":"header","text":"Don't force evolution","value":"dont-force-evolution","line":40,"column":0,"depth":3},"noise--signal":{"type":"header","text":"Noise \u0026 Signal","value":"noise--signal","line":45,"column":0,"depth":3},"why-do-any-of-this":{"type":"header","text":"Why do any of this?","value":"why-do-any-of-this","line":50,"column":0,"depth":3},"structure-of-this-wiki":{"type":"header","text":"Structure of this wiki","value":"structure-of-this-wiki","line":59,"column":0,"depth":2},"quicklinks":{"type":"header","text":"Quicklinks","value":"quicklinks","line":64,"column":0,"depth":2}},"children":["0yay2om15bsg2li2p6qgux7","05c4nnjqa92zx11ld6o0ytn","9gtn7g40cvqui0sifl1s7t5","ftbd1hknsd3ocd7jao26tn3","a1kmkdbpclaz5p6sykaw6kc","z121gkmqfo09m8r7jgnpfgn","gkqrr7xbt18xhi93dmjrwzj","ja2x4lrgejr9o9wvit0bd0d","luv39odkfibx3wdosvigwvy","vtvk3bi6o72w58oima9xzf3","yy652kvqrkfn9ipk07m40h4"],"parent":null,"data":{},"body":"\n# Welcome to Noetic Noggin\n\nThis is [[my|archive.about]] personal wiki and a commonplace book; notes by me, for me.\n\n Permanently under construction \n\n![](https://res.cloudinary.com/zubayr/image/upload/v1658499909/wiki/ajevkuyebljlxiblyst2.png)\n\nThis wiki was made possible with [dendron.so](https://dendron.so) and [obisidian.md](https://obsidian.md). Stored on [Github Repository](https://github.com/zubayrrr/dendron) and hosted on [Netlify](https://netlify.com) for free.\n\n## Principles\n\n### All notes should be relative to me.\n\n- All notes in principle are written for me; what I know about a subject, how I feel about a particular thing.\n- Opinions are fine as long as I feel strong epistemic confidence in the given opinion.\n- Don't over explain a note if it's not necessary, remember, these notes are for you and are relative to whatever knowledge you posses about the subject.\n- Read books and make an dedicated notes for them.\n- Listen podcasts but capture them inside a \"subject specific\" note or \"Map of Concept\" note or a note tagged #areas. Because making notes from podcasts can be tedious as they're not as well structured as books for consumption.(Whose merit is debatable.)\n- #areas are basically \"Map of Concept\" notes but I have recently come to the realization that its better to maintain them [Nikita Voloboev style](https://wiki.nikiv.dev/) but with heavy usage of transclusion and backlinking.\n  - \"Resources\" should be first processed and then mentioned inside the note, otherwise they should be left in inbox.\n\n### Gotta capture 'em all\n\n- Hog whatever information tickles your pickle([anything that gratifies one's intellectual curiosity](https://news.ycombinator.com/newsguidelines.html)).\n- [[swipes]] are interesting/useful bits of... [[quotes|swipes.quotes]], [[excerpts|swipes.excerpts]], [[sayings|swipes.sayings]], [[phrases|swipes.phrases]]. Essentially, ideas, opinions that are swiped off from [[others|resources.people]].\n- Make no distinction between \"your\" ideas and ideas of \"others\", because if you vibe with an idea; it's already yours.\n- But also remember \"If you've time to consume, you've time to produce.\".\n- Use [raindrop.io](https://raindrop.io) to manage your URL bookmarks.\n- If you need to bookmark a webpage or an article all together, use [MarkDownload](https://chrome.google.com/webstore/detail/markdownload-markdown-web/pcmpcfapbekmbjjkdalcgopdkipoggdi?hl=en-GB) to rip the entire page. Let's call it a [[webmark|inbox.webmark]]; it belong in the `/inbox`.\n- Similar process is employed for capturing tweets using [tweet-to-markdown](https://github.com/kbravh/tweet-to-markdown) and it also belongs in the `/inbox`.\n\n### Don't force evolution\n\n- Let your second brain evolve at it's own pace.\n- The structure should never be _too_ rigid because its meant to take form by itself.\n\n### Noise \u0026 Signal\n\n- While capturing ideas left and right is recommended, make sure you're not harming your periods of focus.\n- Have impenetrable focus periods (use Pomodoro method) where you only care about the work on hand and nothing else.\n\n### Why do any of this?\n\n- Because I can't remember everything - there's a lot of information around that interests me and there isn't enough working memory installed in me.\n- So, I make notes - to remember, to create, to meditate, to think.\n- A bodybuilder's portfolio is their body - my portfolio is my wiki.\n- Not only am I making - whatever I know - tangible by writing it down. I know exactly where to look if I ever forget something.\n- I am at the beginning of my learning adventures. When I look back at it, I will know where I came from and how my thoughts evolved over time.\n- Plus, its really fun to nerd out.\n\n## Structure of this wiki\n\n- [[slipbox.Ontology]] explains the structure of this wiki and the tags, backlinks used in it.\n- Dendron takes care of the structure and hierarchy(mostly), but I insist on using tags for backwards compatibility.\n\n## Quicklinks\n\n- [[About me|archive.about]]\n- [Github](https://github.com/zubayrrr)\n- [Twitter](https://twitter.com/zoobhalu)\n- [Blog](https://zubayrali.in)\n- [Guestbook](https://www.yourworldoftext.com/~zubayrali/)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template","insertNote":{"initialValue":"templates"}},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Noetic Noggin","description":"Personal Wiki / Digital Garden","author":"Zubayr Ali","twitter":"zoobhalu"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteUrl":"localhost:3000","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"f2kecna72pmc7re3wh1ugk4"},"buildId":"4oDSInv8WeDX6cVSprjyl","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>