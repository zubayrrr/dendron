<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Devops Interview Questions</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Wiki / Digital Garden"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="zoobhalu"/><meta name="twitter:creator" content="zoobhalu"/><meta property="og:title" content="Devops Interview Questions"/><meta property="og:description" content="Personal Wiki / Digital Garden"/><meta property="og:url" content="localhost:3000/notes/5eybx5vd339tjnkglx8igde/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="6/22/2022"/><meta property="article:modified_time" content="6/25/2022"/><link rel="canonical" href="localhost:3000/notes/5eybx5vd339tjnkglx8igde/"/><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/cc2307f6fd3a2fec.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cc2307f6fd3a2fec.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-709abf7ab5f510de.js" defer=""></script><script src="/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/_next/static/chunks/main-c4b0e551a2150d17.js" defer=""></script><script src="/_next/static/chunks/pages/_app-62c5b93605efada7.js" defer=""></script><script src="/_next/static/chunks/78-13ae6acd5ce7ca5b.js" defer=""></script><script src="/_next/static/chunks/373-2f3879190a46a3d9.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-69449972c2a725d8.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_buildManifest.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_ssgManifest.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout site-layout" style="margin-top:64px"><section class="ant-layout site-layout"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="devops-interview-questions"><a aria-hidden="true" class="anchor-heading" href="#devops-interview-questions"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Devops Interview Questions</h1>
<ol>
<li><strong>EC2 instance is running out of disk space. What actions will you take to mitigate the issue?</strong></li>
</ol>
<ul>
<li><a href="/notes/1h5mmmv2b68di6siwa31jvh">AWS EC2</a> disk space typically refers to <a href="/notes/fsmsti0qns6uixhjwtqk7p6">AWS EBS</a> volume.</li>
<li>We'll first check if its a root volume or any other volume
<ul>
<li>/root - OS</li>
<li>/application - for all our applications</li>
</ul>
</li>
<li>If its root volume then we'll first try to check logs(<code>/var/logs</code>) and clear some space if not the instance might shut down.</li>
<li>If its the application volume(learn the reason of high disk usage), then we'll use EBS feature to take the snapshot and increase disk space for the EC2 instance.</li>
</ul>
<ol start="2">
<li><strong>Explain different ways in which</strong> <a href="/notes/9fa6uf6xbnv00fv944o1y1o">Prometheus</a> <strong>can get metrics?</strong></li>
</ol>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">Prometheus</span></div>
<a href="/notes/9fa6uf6xbnv00fv944o1y1o" class="portal-arrow">Go to text <span class="right-arrow">→</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><h2 id="getting-metrics"><a aria-hidden="true" class="anchor-heading" href="#getting-metrics"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Getting metrics</h2>
<p>How does Prometheus collect metrics from the targets?</p>
<p><strong>Pulling</strong></p>
<p>Your application(regardless of technology) will have to expose a metrics HTTP endpoint and Prometheus will scrape from the endpoint. By default is is: <code>hostaddress/metrics</code>.</p>
<p>Data available in the <code>/metrics</code> endpoint should be in the correct format that Prometheus understands.</p>
<p>Some servers expose Prometheus endpoints by default so you don't really have to do extra work for it. But many services don't have native Prometheus endpoints in which case you'd need an <strong>Exporter</strong></p>
<p><strong>Exporter</strong></p>
<p>It basically a script/service that fetches metrics from your target and converts them in format Prometheus understands and exposes it's converted data at it's own <code>/metrics</code> endpoint where Prometheus can scrape them.</p>
<p>Prometheus has a list of exporters for different services like <a href="/notes/ypszfixe0p3s5k0inqs5g08">MySql</a>, <a href="/notes/boyz5i8gtwc9aoj4zfz556y">Elasticsearch</a>, <a href="/notes/owoutsv5dicylguol2odc3e">Linux</a> servers, <a href="/notes/qcaw5ht4vcnucydq104gh7v">Build Tools</a>, Cloud Platforms and so on.</p>
<p>If you want to monitor a Linux server, see: <a href="https://prometheus.io/docs/guides/node-exporter/">Monitoring Linux host metrics with the Node Exporter | Prometheus</a></p>
<p><img src="https://res.cloudinary.com/zubayr/image/upload/v1656150261/wiki/xfe8c37gmzogdoin3wtx.png"></p>
<p>Exporters are also available as Docker images. SO</p>
<p>If you want to monitor <a href="/notes/ypszfixe0p3s5k0inqs5g08">MySql</a> container in a <a href="/notes/gbeh61d6hvbmxxy63chp81b">Kubernetes</a> cluster, you can deploy  a sidecar container of MySQL exporter that will run inside the pod with MySQL container, connect to it and start sending MySQL metrics for Prometheus and making them available at it’s own <code>/metrics</code> endpoint.</p>
<p><strong>Monitoring  your own applications?</strong></p>
<ul>
<li>
<p>How many requests your applications are receiving.</p>
</li>
<li>
<p>How many exceptions are occurring.</p>
</li>
<li>
<p>How many server resources your application is using.</p>
<p> For this you can use Client Libraries for different languages using which you can expose <code>/metrics</code> endpoint for metrics that are relevant to you.
<a href="https://prometheus.io/docs/instrumenting/clientlibs/">Client libraries | Prometheus</a></p>
</li>
</ul>
<h2 id="push-based-vs-pull-based"><a aria-hidden="true" class="anchor-heading" href="#push-based-vs-pull-based"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Push based VS Pull based</h2>
<p>Most monitoring systems like <a href="/notes/42qbhsb3tdd4z7w7ynfmjj1">AWS CloudWatch</a> or <a href="/notes/g8txkrlv8lggex7u6372sd1">New Relic</a> etc use a Push system. Applications and servers are responsible for pushing their metric data to a centralized collection platform of that monitoring tool.</p>
<p>In large microservices based system this approach can create a bottleneck for your infrastructure as all of these microservices constantly make push request to your monitoring tool thus flooding your system.</p>
<p>Plus, you’ll also need to install additional software(daemons) on each of your targets to push the metrics to the monitoring server. In contrast with Prometheus which only requires a scraping endpoint.</p>
<p>Multiple Prometheus instances can collect/pull metrics. Using pull, Prometheus can easily detect whether a service is up and running or not.</p>
<p>Pushing can be ambiguous when checking if the service is up or not when compared to pull mechanism. Because there can be many reasons for a push request to fail. </p>
<p><strong>Pushing</strong></p>
<p>Pushgateway can be utilized when a target only runs for a short time.
Eg: A batch job, scheduled job etc. For such jobs, Prometheus offers Pushgateway component. So these services can push metrics directly to Prometheus DB.</p>
<p><img src="https://res.cloudinary.com/zubayr/image/upload/v1655885235/wiki/yguspabejdgcr4qbzegm.png"></p>
<h2 id="configuring-prometheus"><a aria-hidden="true" class="anchor-heading" href="#configuring-prometheus"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Configuring Prometheus</h2>
<p><code>prometheus.yaml</code> file contains all the info needed for Prometheus to know what(targets) to scrape and when(intervals). </p>
<p>Prometheus then uses <strong>Service Discovery</strong>  mechanism to find those target endpoints.</p>
<p>You can find the sample config files with default values which comes with your first Prometheus installation.</p>
<p><img src="https://res.cloudinary.com/zubayr/image/upload/v1656153062/wiki/p2l9ubydx8h1viv4pafn.png"></p>
<p>Under <code>global:</code> you define how often Prometheus will scrape it’s targets.</p>
<p>Rules are for aggregating metric values or creating alerts when conditions
are met.</p>
<p><code>scrape_configs:</code> define what resources Prometheus monitors; essentially targets. You can define your own jobs and default values for each job(overwrite global interval values).</p>
<p>Since Prometheus has it’s own <code>/metrics</code> endpoint, it can monitor it’s own health.</p>
<h2 id="alertmanager"><a aria-hidden="true" class="anchor-heading" href="#alertmanager"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>AlertManager</h2>
<p>How does Prometheus trigger alerts that are defined by rules in <code>prometheus.yaml</code> and who receives these alerts?</p>
<p>Prometheus has a component called AlertManger that is responsible for firing alerts via different channels (Emails, Slack channel or other notification clients).</p>
<p>Prometheus server will read alert rules and if the conditions under rules is met an alert is fired. </p>
<h2 id="prometheus-data-storage"><a aria-hidden="true" class="anchor-heading" href="#prometheus-data-storage"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Prometheus Data Storage</h2>
<p>Where does Prometheus store all the data that it collects/aggregates? How can other systems use this data?</p>
<p>Prometheus stores metric data on disks, includes Local on disk <strong>Time Series DB</strong> but also optionally integrates with remote storage system. It is stored in custom Time Series format. Because of this you cannot directly write this data on a relational DB or something else.</p>
<p>Once collected, Prometheus lets you query the data through it’s server API using it’s query language called <strong>PromQL</strong>.</p>
<h2 id="promql"><a aria-hidden="true" class="anchor-heading" href="#promql"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>PromQL</h2>
<p>You can use Prometheus dashboard UI to ask Prometheus server via PromQL to for example show the status of a target right now.</p>
<p>Or use more powerful data visualization tools like <strong>Grafana</strong> to display the data which uses PromQL under the hood to get data out of Prometheus .</p>
<p>Example PromQL query to:</p>
<p>Query all HTTP status codes except <code>4xx</code> ones</p>
<pre class="language-sql"><code class="language-sql">http_requests_total{<span class="token keyword">status</span><span class="token operator">!</span><span class="token operator">~</span><span class="token string">"4.."</span>}
</code></pre>
<p>This query does some subquery:</p>
<p>Returns the 5 minute rate of the <em>http_requests_total</em> metric for the past 30 minutes.</p>
<pre class="language-sql"><code class="language-sql">rate<span class="token punctuation">(</span>http_requests_total<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">[</span><span class="token number">30</span>m:<span class="token punctuation">]</span>
</code></pre>
<p><img src="https://res.cloudinary.com/zubayr/image/upload/v1656153983/wiki/qu5c4x2hrwuozqeqebxy.png"></p>
<h2 id="sailent-characteristics-of-prometheus"><a aria-hidden="true" class="anchor-heading" href="#sailent-characteristics-of-prometheus"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Sailent Characteristics of Prometheus</h2>
<p>It is designed to be reliable even when other systems have an outage so you can diagnose the problems and fix them.</p>
<p>Each Prometheus server is standalone and self-contained. It doesn’t depend on network storage or other remote services. It is meant to be still working when other parts of the infrastructure are broken.</p>
<p>It doesn’t require extensive setup needed.</p>
<p><strong>Drawbacks:</strong></p>
<p>It can be difficult to scale, when you have hundreds of servers that you want to use multiple Prometheus instances for aggregation of metrics setting them up can get complicated.</p>
<p>A workaround this would be to increase the capacity of your Prometheus server, limit the number of metrics Prometheus collects from applications.</p>
<h2 id="prometheus-federation"><a aria-hidden="true" class="anchor-heading" href="#prometheus-federation"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Prometheus Federation</h2>
<p>To scale monitoring with scalable cloud apps.</p>
<p>Prometheus Federation allows one Prometheus server to scrape data from another Prometheus server. This will allow you to scale your Prometheus setup with your multi-node applications.</p>
<h2 id="prometheus-with-docker--kubernetes"><a aria-hidden="true" class="anchor-heading" href="#prometheus-with-docker--kubernetes"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Prometheus with Docker &#x26; Kubernetes</h2>
<p>It is fully compatible with both.</p>
<p>Prometheus components are available as Docker images and therefore can be deployed on Kubernetes or other container environments.</p>
<p>It provides monitoring of K8s Cluster Node Resource out of the box! Once deployed on K8s, it starts gather metrics data on each Kubernetes node server without any extra configuration.</p></div></div><p></p><p></p>
<ol start="3">
<li><strong>What is Kubernetes kOps?</strong></li>
</ol>
<p><a href="/notes/gbeh61d6hvbmxxy63chp81b">Kubernetes</a> <a href="/notes/fhi59s6yqplah2w1hfx9i5x">kOps</a> is an automation tool used to setup Kubernetes cluster. It is an alternative to <code>kubeadmin</code>.</p>
<p>kOps can help you spin a test cluster or a small dev cluster quickly. It is not something that will help you setup managed Kubernetes cluster(<a href="/notes/ervusa6qabwyx7sbb1a4vcg">AWS EKS</a>). It can create, destroy, upgrade, maintain production-grade, high availability clusters and also provision necessary cloud infrastructure(only recommended if you cannot afford managed service).</p>
<p>It supports many cloud providers.</p>
<ol start="4">
<li><strong>What is instance fleet in AWS?</strong></li>
</ol>
<p>Instance fleet in <a href="/notes/gvpkbgglehtr9ej5e0uj44j">AWS</a> refers to a configuration - information to launch a fleet or a group of <a href="/notes/1h5mmmv2b68di6siwa31jvh">AWS EC2</a> instances, in a single API call. A fleet can launch multiple instance(mixed set) types across multiple Availability Zones using On-Demand Instance, Reserved Instance and Spot Instance purchasing options together.</p>
<p>In the configuration you can define:</p>
<ul>
<li>Separate capacity targets and maximum amount you're willing to pay per hour for On-Demand, Spot instances.</li>
<li>Specify the instance types that work best for your applications.</li>
<li>Specify how EC2 should distribute your fleet capacity within each purchasing options.</li>
</ul>
<p>The instance fleet configuration for <a href="/notes/w0ukvte4c5p8inzz5kf7zf6">AWS EMR</a> lets you select wide variety of provisioning options for Amazon EC2 instances and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster.</p>
<ol start="5">
<li><strong>How do you pass “message” for your git commit</strong></li>
</ol>
<p>Use the flag <code>-m [message]</code> for your <a href="/notes/qkp5jc6pta8a9f35evsbvod">Git</a> commit. Although you can commit without passing a message.</p>
<p><code>git commit -m "🔥 commit message"</code></p>
<ol start="6">
<li><strong>What application server are you familiar with?</strong></li>
</ol>
<p><a href="/notes/jew452yotysgzsmcmgmzj6j">Web Server &#x26; Application Server</a></p>
<p>For Java applications, we have <a href="/notes/w2frd082bpmiik2nqaxf8wc">Tomcat</a> but there are different application servers for applications based on different technologies.</p>
<ol start="7">
<li><strong>How to check logs of a Docker container/filter last 200 lines from the logs.</strong></li>
</ol>
<p><code>docker container logs &#x3C;container_name></code>
<code>docker container logs --tail 200 &#x3C;container_name></code></p>
<ol start="8">
<li><strong>What happens to container logs if it is restarted?</strong></li>
</ol>
<p>You won’t lose any logs for restarting a container but since containers are stateless, you will lose the logs if a container is <strong>deleted</strong>. If you want to persist logs you can use external persistent storage. You can also push the container logs to something like <a href="/notes/002tn8rmi02kqtdd6xzllg2">AWS CloudTrail</a> or <a href="/notes/rem6mv7lebiuf5ajcmm2acd">Splunk</a></p>
<ol start="9">
<li><strong>Horizontal Scaling VS Vertical Scaling</strong></li>
</ol>
<p><strong>Horizontal scaling</strong> means scaling by adding more machines to your pool of resources (also described as “scaling out”); something like <a href="/notes/520xdfstn930e413i3byvxs">AWS Auto Scaling Group</a>, creating replicas(think <a href="/notes/gbeh61d6hvbmxxy63chp81b">Kubernetes</a>). If you are hosting an application on a server and find that it no longer has the capacity or capabilities to handle traffic, adding a server may be your solution.</p>
<p>Whereas <strong>vertical scaling</strong> refers to scaling by adding more power (e.g. CPU, RAM) to an existing machine (also described as “scaling up”). For instance, if your server requires more processing power, vertical scaling would mean upgrading the CPUs. You can also vertically scale the memory, storage, or network speed.</p>
<p>See: <a href="https://www.cloudzero.com/blog/horizontal-vs-vertical-scaling">Horizontal Vs. Vertical Scaling: How Do They Compare?</a></p>
<ol start="10">
<li><strong>ReplicationController in Kubernetes</strong></li>
</ol>
<p>A ReplicationController is responsible for running the specified number of pod copies(replicas) across the cluster.</p>
<p>ReplicationController is not auto scale.</p>
<pre class="language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> ReplicationController
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx
          <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx
          <span class="token key atrule">ports</span><span class="token punctuation">:</span>
            <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span>
</code></pre>
<ol start="11">
<li><strong>What is helm?</strong></li>
</ol>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">Helm</span></div>
<a href="/notes/5uu4351w46y16xtyufz0fy4" class="portal-arrow">Go to text <span class="right-arrow">→</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><p>Helm is a Kubernetes deployment tool for automating creation, packaging, configuration, and deployment of applications and services to Kubernetes clusters.</p>
<p><a href="/notes/gbeh61d6hvbmxxy63chp81b">Kubernetes</a> is a powerful container-orchestration system for application deployment. There are multiple independent resources to deal with, and each requires a dedicated YAML manifest file.</p>
<p>Helm deploys packaged applications to Kubernetes and structures them into charts. The charts contain all pre-configured application resources along with all the versions into one easily manageable package.</p>
<p>Helm streamlines installing, upgrading, fetching dependencies, and configuring deployments on Kubernetes with simple CLI commands. Software packages are found in repositories or are created.</p>
<p>Unlike Homebrew or Aptitude desktop package managers, or Azure Resource Manager templates (ARMs) / Amazon Machine Images (AMIs) that are run on a single server, Helm charts are built atop Kubernetes and benefit from its cluster architecture. The main benefit of this approach is the ability to consider scalability from the start. The charts of all the images used by Helm are stored in a registry called Helm Workspace, so the DevOps teams can search them and add to their projects with ease.</p>
<p>Kubernetes objects are challenging to manage. With helpful tools, the Kubernetes learning curve becomes smooth and manageable. Helm automates maintenance of YAML manifests for Kubernetes objects by packaging information into charts and advertises them to a Kubernetes cluster.</p>
<p>Helm keeps track of the versioned history of every chart installation and change. Rolling back to a previous version or upgrading to a newer version is completed with comprehensible commands.</p>
<h2 id="helm-chart"><a aria-hidden="true" class="anchor-heading" href="#helm-chart"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Helm Chart</h2>
<p>Helm charts are Helm packages consisting of YAML files and templates which convert into Kubernetes manifest files. Charts are reusable by anyone for any environment, which reduces complexity and duplicates. Folders have the following structure:</p>
<p><img src="https://res.cloudinary.com/zubayr/image/upload/v1655905668/wiki/f0iewhj44stf8t90qnfr.png"></p>













































<table><thead><tr><th><strong>Name</strong></th><th><strong>Type</strong></th><th><strong>Function</strong></th></tr></thead><tbody><tr><td><strong>charts/</strong></td><td>Directory</td><td>Directory for manually managed chart dependencies.</td></tr><tr><td><strong>templates/</strong></td><td>Directory</td><td>Template files are written in Golang and combined with configuration values from the values.yaml file to generate Kubernetes manifests.</td></tr><tr><td><strong>Chart.yaml</strong></td><td>File</td><td>Metadata about the chart, such as the version, name, search keywords, etc.</td></tr><tr><td><strong>LICENSE (optional)</strong></td><td>File</td><td>License for the chart in plaintext format.</td></tr><tr><td><strong>README.md (optional)</strong></td><td>File</td><td>Human readable information for the users of the chart.</td></tr><tr><td><strong>requirements.yaml (optional)</strong></td><td>File</td><td>List of chart’s dependencies.</td></tr><tr><td><strong>values.yaml</strong></td><td>File</td><td>Default configuration <a href="https://phoenixnap.com/kb/helm-get-values">values</a> for the chart.</td></tr></tbody></table>
<p>Via - <a href="https://phoenixnap.com/kb/what-is-helm">What is Helm? Helm and Helm Charts Explained</a></p>
<h2 id="resources"><a aria-hidden="true" class="anchor-heading" href="#resources"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Resources</h2>
<ul>
<li><a href="https://phoenixnap.com/kb/what-is-helm">What is Helm? Helm and Helm Charts Explained</a></li>
</ul>
</div></div><p></p><p></p>
<ol start="12">
<li><strong>Which Python module would you use to write a simple program to test API code?</strong></li>
</ol>
<p>The code should just check if the API endpoint is working or not.</p>
<p><strong>Answer:</strong></p>
<p>I would use the <strong>request</strong> module in <a href="/notes/wa4wthnhw54hd5wfyocyvqa">Python</a>. It has the <code>get()</code> function wherein you'd pass pass the API endpoint and fetch status/http code <code>.status_code</code>.</p>
<p>If it returns <code>200</code> - API endpoint is working fine.</p>
<ol start="13">
<li><strong>Which HTTP responses would you monitor and for which would you trigger alerts?</strong></li>
</ol>
<p>Example of API endpoints:</p>
<pre><code>/this-is-an-endpoint
/another/endpoint
/some/other/endpoint
/login
/accounts
/cart/items
</code></pre>
<p>HTTP response status codes indicate whether a specific HTTP request has been successfully completed. Responses are grouped in five classes:</p>
<ul>
<li>
<p>Informational responses (100–199)</p>
</li>
<li>
<p>Successful responses (200–299)</p>
</li>
<li>
<p>Redirection messages (300–399)</p>
</li>
<li>
<p>Client error responses (400–499)</p>
</li>
<li>
<p>Server error responses (500–599)</p>
</li>
<li>
<p>If HTTP response is in the 400 or 500 range we'll trigger an alert by writing a script of setting up a monitoring service.</p>
</li>
</ul>
<ol start="14">
<li><strong>Can we run a Jenkins agent inside a docker container along with our test?</strong></li>
</ol>
<ul>
<li>Running Jenkins agent inside a <a href="/notes/wf37vjntme0oklsx52ycrn4">Docker</a> container is (one of) the standard way of implementing pipelines.</li>
<li>This question is focusing on isolation of pipeline steps.</li>
<li>Here the code is expected to run inside a docker container which can use a custom test image if required.</li>
<li>This is the best way to perform testing without having every <a href="/notes/jqsu891tokqfup82wkq6j02">Jenkins</a> agent needing to have packages installed.</li>
<li>To use any kind of Docker containers, I'd add it as a part of the agent. We can call any kind of images(from DockerHub or Custom), whatever stages and test steps we mention as part of our pipeline will run inside that docker container. Multi node Jenkins setup.</li>
</ul>
<p><img src="https://res.cloudinary.com/zubayr/image/upload/v1655971387/wiki/m9mcgppau59fyaewl0uw.png"></p>
<ol start="15">
<li><strong>What are some of the ways of setting up alerts?</strong></li>
</ol>
<p>It is not feasible to have multiple alerting methods in one org. Alerts notifications can be Emails, Phone calls, Slack messages, etc. This ties into the <a href="/notes/01rcytslgl5ysbgp9kzekoj">On Call</a> management.</p>
<p>Some of the open source methods:</p>
<ul>
<li><a href="/notes/9fa6uf6xbnv00fv944o1y1o">Prometheus</a> -> Alertmanager</li>
<li><a href="/notes/42qbhsb3tdd4z7w7ynfmjj1">AWS CloudWatch</a> -> SNS</li>
<li>Nagios -> Alerting</li>
</ul>
<ol start="16">
<li><strong>Help repository to store/access helm charts</strong></li>
</ol>
<p>Helm repositories are a common practice to store helm charts, which can be access by our <a href="/notes/gbeh61d6hvbmxxy63chp81b">Kubernetes</a> cluster as part of a deployment. This helps with versioning our helm charts, rollbacks, upgrade etc.</p>
<ul>
<li>Cloudsmith</li>
<li>Jfrog Artifactory</li>
<li><a href="/notes/faxbzx44mmk9u2ix1fdfuu7">AWS S3</a></li>
<li>Google Cloud Storage</li>
<li>Artifact Hub(Open Source)</li>
</ul>
<ol start="17">
<li><strong>Jenkins multi-node setup, how to add new slave/follower to master?</strong></li>
</ol>
<p>Having only one node is usually not enough(availability) for any organization so a multi-node setup is required for scaling. You can add a slave/follower for a master on "Manage <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">devlog.Jekins (Private)</a>" page and the option "Manage Nodes and Clouds". You will provide node info such as the IP Address, username, password etc and get it registered. We can also add slaves dynamically, you'd need an auto scaling group by your Cloud Provider.</p>
<ol start="18">
<li><strong>How do you block an IAM user from accessing a specific S3 bucket?</strong></li>
</ol>
<p>It is difficult to manage bucket level policies using IAM policy. We can achieve this using <a href="/notes/faxbzx44mmk9u2ix1fdfuu7">AWS S3</a> bucket policy, you'd use the IAM user's ARN and “deny” the user.</p>
<ol start="19">
<li><strong>Is a large docker image a cause of concern? How would you tackle it?</strong></li>
</ol>
<p>Applications can be large if they're complex and do a lot of things but if it is a simple application...large size is not warranted and can be mitigated.</p>
<ul>
<li>Bigger docker image would result in longer build time.</li>
<li>Docker image downloaded(from DockerHub) may throw errors or cause API rate limit issues.</li>
<li>Application will be bulkier and harder to debug and scale.</li>
</ul>
<p>To resolve this:</p>
<ul>
<li>Smaller Base Image(Alpine images).</li>
<li>Introduce Multi-stage build - from the Base image you build up your image and discard your previous image builds.</li>
<li>Remove package binaries after installing and don't install packages that are not necessary.</li>
<li>Lock your package/dependencies versions.</li>
</ul>
<ol start="20">
<li><strong>Are you aware of AWS IAM policies/can you read them?</strong></li>
</ol>
<p>Policy evaluation logic</p>
<p>Example</p>
<pre class="language-json"><code class="language-json"><span class="token punctuation">{</span>
  <span class="token property">"Version"</span><span class="token operator">:</span> <span class="token string">"2012-10-17"</span><span class="token punctuation">,</span> <span class="token comment">// version is fixed</span>
  <span class="token property">"Statement"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
      <span class="token property">"Sid"</span><span class="token operator">:</span> <span class="token string">"AllowS3ListRead"</span><span class="token punctuation">,</span> <span class="token comment">// high level ALLOW action</span>
      <span class="token property">"Effect"</span><span class="token operator">:</span> <span class="token string">"Allow"</span><span class="token punctuation">,</span>
      <span class="token property">"Action"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
        <span class="token string">"s3:GetBucketLocation"</span><span class="token punctuation">,</span> <span class="token comment">// explicitly defining what actions are allowed</span>
        <span class="token string">"s3:GetAccountPublicAccessBlock"</span><span class="token punctuation">,</span>
        <span class="token string">"s3:ListAccessPoints"</span><span class="token punctuation">,</span>
        <span class="token string">"s3:ListAllMyBuckets"</span>
      <span class="token punctuation">]</span><span class="token punctuation">,</span>
      <span class="token property">"Resource"</span><span class="token operator">:</span> <span class="token string">"arn:aws:s3:::*"</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span>
      <span class="token property">"Sid"</span><span class="token operator">:</span> <span class="token string">"AllowS3Self"</span><span class="token punctuation">,</span> <span class="token comment">// high level ALLOW action</span>
      <span class="token property">"Effect"</span><span class="token operator">:</span> <span class="token string">"Allow"</span><span class="token punctuation">,</span>
      <span class="token property">"Action"</span><span class="token operator">:</span> <span class="token string">"s3:*"</span><span class="token punctuation">,</span> <span class="token comment">// everything is allowed but only limited to the below two buckets</span>
      <span class="token property">"Resource"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"arn:aws:s3:::carlossalazar/*"</span><span class="token punctuation">,</span> <span class="token string">"arn:aws:s3:::carlossalazar"</span><span class="token punctuation">]</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span>
      <span class="token property">"Sid"</span><span class="token operator">:</span> <span class="token string">"DenyS3Logs"</span><span class="token punctuation">,</span> <span class="token comment">// high level DENY action</span>
      <span class="token property">"Effect"</span><span class="token operator">:</span> <span class="token string">"Deny"</span><span class="token punctuation">,</span>
      <span class="token property">"Action"</span><span class="token operator">:</span> <span class="token string">"s3:*"</span><span class="token punctuation">,</span> <span class="token comment">// everything is denied for any bucket with the suffix "logs"</span>
      <span class="token property">"Resource"</span><span class="token operator">:</span> <span class="token string">"arn:aws:s3:::*log*"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre>
<p>Example via - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html">Policy evaluation logic - AWS Identity and Access Management</a></p>
<ol start="21">
<li><strong>What role does <code>pv</code> and <code>pvc</code> play in Kubernetes?</strong></li>
</ol>
<p>PV stands for PersistentVolume
PVC stands for PersistentVolumeClaim</p>
<p>Pod gets it's storage using PVC which would in turn get hold of PV which will utilize an NFS or <a href="/notes/fsmsti0qns6uixhjwtqk7p6">AWS EBS</a> volume.</p>
<p>PVC will define what kind of volume and the storage amount it needs and it will search of PVs(which you'd have provisioned) and choose from those.</p>
<p>PV can exist independently from a Pod, you don't need to have a Pod for a PV to be created.</p>
<ol start="22">
<li><strong>When creating RDS using Terraform, how do you save DB username and password securely?</strong></li>
</ol>
<p>Since DB username and password are considered secrets, they cannot be saved in plaintext on a repository along with the Terraform code.</p>
<p>You can use Hashicorp's Vault, store your secrets on your Vault.</p>
<p>Integrate it with Terraform. In your <code>main.tf</code> you can reference it as a Data Block. Mention Vault provider in the Provider section.</p>
<p>You can also use other secret stores/managers like <a href="/notes/3rpt3gh5gtw0qr6qs8f4qkq">AWS Secrets Manager</a> etc. Or you can also use environment variables.</p>
<ol start="23">
<li><strong>Have you support any DBs?</strong></li>
</ol>
<p>You don't need to be a DB expert but you have to have clear understanding of different kinds of DBs used, their use cases and pick one based on your experience and profile.</p>
<ul>
<li>Key-value DB
<ul>
<li><a href="/notes/1kiomlco81afa6al0dp14lh">Redis</a>, etcd</li>
</ul>
</li>
<li>Column DB
<ul>
<li>Cassandra</li>
</ul>
</li>
<li>NoSQL/Document/Schemaless DB
<ul>
<li><a href="/notes/lh0h2j2j55l3xcy3v5np9m2">mongoDB</a></li>
<li><a href="/notes/tczgjqm691xi6olmcy78ckc">AWS DynamoDB</a></li>
</ul>
</li>
<li>Relational DB
<ul>
<li><a href="/notes/ypszfixe0p3s5k0inqs5g08">MySql</a></li>
<li><a href="/notes/dxfqif7isp6w1p8xoohln8b">AWS Aurora</a></li>
<li><a href="/notes/l5a3o2n7ud6bypkz8jh688u">PostgreSQL</a></li>
</ul>
</li>
</ul>
<ol start="24">
<li><strong>How do you ensure certain packages are installed on all your EC2 instances and are persisted?</strong></li>
</ol>
<p>HashiCorp's <a href="/notes/kxbhnz5oxpmort5y94uvqc9">Packer</a> is the solution. Packer is an open source tool that enables you to create identical machine images for multiple platforms from a single source template which can be written in JSON. You can use it in your multi-cloud setup or On-prem infra too.</p>
<p>A common use case is creating "Golden Images" that teams across an organization can use in cloud infrastructure.</p>
<p>We could also use custom <a href="/notes/38n6krgljsgboencwqjmb5j">AWS AMI</a> images too but is limited to Cloud.</p>
<ol start="25">
<li><strong>Differentiate Managed, Customer Managed and Inline IAM policies.</strong></li>
</ol>
<ul>
<li>Managed: Created and maintained by AWS.</li>
<li>Customer Managed: Created and maintained by customer. Custom policies created by an organization using <a href="/notes/mkwoo3ek4y6bnddpy1iteb9">Terraform</a>. It can be attached to multiple users or groups.</li>
<li>Inline: Created and attached directly to IAM User, Group or Role. It is created for that specific Role, User, Group ...if the Role, User or Group is destroyed, the policy is also deleted.</li>
</ul>
<ol start="26">
<li><strong>What build tools are you familiar with?</strong></li>
</ol>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">Build Automation</span></div>
<a href="/notes/qcaw5ht4vcnucydq104gh7v" class="portal-arrow">Go to text <span class="right-arrow">→</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><h2 id="building-java-applications"><a aria-hidden="true" class="anchor-heading" href="#building-java-applications"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Building Java Applications</h2>
<ul>
<li>
<p>Install IntelliJ IDEA.</p>
</li>
<li>
<p>Install Java(or use IDEA to Download SDK).</p>
</li>
<li>
<p>Setup JDK, SDK (make sure Java executable is added to <a href="/notes/iicnw4qgra6rr5f6k4dlw3z">$PATH</a> or <code>%PATH%</code>, respectively.)</p>
</li>
<li>
<p>Set <code>JAVA_HOME</code> as an <a href="/notes/o729gsazu87le197nswwxtp">environment variable</a> (Maven prerequisite).</p>
</li>
<li>
<p>Use SCM to clone(<code>git clone</code>) your Java application's repository.</p>
</li>
<li>
<p>Build a <a href="/notes/482tfc3v73o3d4p8yi3b4z7">Maven</a> project.</p>
<ul>
<li>Open source code of your Java application in IntelliJ IDEA.</li>
<li>Wait for IDEA to index the source code.</li>
<li>IDEA will automatically detect the <code>pom.xml</code> and resolve all the dependencies.</li>
<li>Run the application(preview in the "Run" tab).</li>
<li>Download <a href="https://maven.apache.org">Maven</a> and add it's <code>/bin</code>'s path to <code>$PATH</code> or <code>%PATH%</code>.</li>
<li>Use <code>mvn</code> commands and profit!</li>
<li>After building, <code>.jar</code> or <code>.war</code> files can be found in the <code>./target</code> folder.</li>
</ul>
</li>
<li>
<p>Build a <a href="/notes/saz485w50o682scob9knhp1">Gradle</a> project.</p>
<ul>
<li>Open source code of your Java application in IntelliJ IDEA</li>
<li>If it has a gradle wrapper(folder) inside the repo, you don't need to install gradle</li>
<li>Make sure the JVM configured is compatible with gradle wrapper's version.(<code>JAVA_HOME</code>)</li>
<li>Run the application to check if everything is working fine</li>
<li>Build with <code>./gradlew build</code></li>
<li>After building, <code>.jar</code> or <code>.war</code> files can be found in the <code>./build</code> folder.</li>
</ul>
</li>
</ul>
<p>To run a <code>.jar</code>: <code>java -jar name-of-the-app-SNAPSHOT.jar</code></p>
<h2 id="managing-java-dependencies"><a aria-hidden="true" class="anchor-heading" href="#managing-java-dependencies"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Managing Java dependencies</h2>
<ul>
<li>Build tools(Maven, Gradle, NPM) are required even for local development of the application.</li>
<li>Dependencies file is what keeps track of all the dependencies used in the application. Refer to <code>pom.xml</code> for a Maven project and <code>build.gradle</code> for a Gradle project.</li>
<li>All the dependencies for both kind of projects are fetched from <a href="https://mvnrepository.com">mvnrepository.com</a>, they're downloaded locally from the remote repository.</li>
</ul>
<h2 id="building-javascript--devlognodejs-private-project"><a aria-hidden="true" class="anchor-heading" href="#building-javascript--devlognodejs-private-project"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Building <a href="/notes/cxjsfosx0onyz5nt07qna7w">Javascript</a> / <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">devlog.nodejs (Private)</a> project.</h2>
<ul>
<li>
<p>Install Node.js, run <code>npm</code> to check if everything is working correctly.</p>
</li>
<li>
<p>Alternatively to <code>npm</code> you can also use <code>yarn</code> to manage dependencies and to run build commands.</p>
</li>
<li>
<p>Use the build commands from <code>package.json</code> to build it.</p>
</li>
<li>
<p>Unlike Java applications, after building a JavaScript will result in <code>.zip</code> or <code>.tgz</code> file.</p>
</li>
<li>
<p>Both <code>npm</code> and <code>yarn</code> are <strong>package managers not build tools</strong>, they're used for managing dependencies not for transpiling JS code.</p>
</li>
<li>
<p>Use <code>npm pack</code> to pack.</p>
</li>
<li>
<p>The resulting zip or tar doesn't contain dependencies; only the application code.</p>
</li>
<li>
<p>To run the application, you first need to install the dependencies.</p>
</li>
<li>
<p>Unpack the zip/tar.</p>
</li>
<li>
<p>Run the app.</p>
</li>
<li>
<p>Package frontend JS code.</p>
<ul>
<li>Separate <code>package.json</code> for frontend and backend.</li>
<li>or have a common <code>package.json</code> for both frontend and backend.</li>
<li>React code needs to be transpiled since it uses <code>jsx</code> syntax.</li>
<li>Compress and minify.</li>
</ul>
</li>
<li>
<p>Build tools in the JavaScript world as also known as Bundlers.</p>
</li>
<li>
<p>Most popular bundler is <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">devlog.webpack (Private)</a></p>
</li>
<li>
<p>Webpack will transpile, minify, bundles, compresses(removes whitespaces etc).</p>
</li>
</ul>
</div></div><p></p><p></p>
<ol start="26">
<li><strong>Ingress and Egress</strong></li>
</ol>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">archive.ingress (Private)</a> and <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">archive.egress (Private)</a></p>
<p>In IT they refer to:</p>
<p>Ingress: Incoming/Inbound traffic
Egress: Outgoing/Outbound traffic</p>
<p>They're mostly associated with security groups(<a href="/notes/xqc8lgkkzwz4qcbb6htkgfv">Firewall</a>). They're also associated with VPCs and subnets where we control the incoming traffic from the internet and routing the traffic between subnets.</p>
<p>Eg: Allowing or denying ports for certain traffic(<a href="/notes/o922t6d9q6wur3yve290hir">SSH</a>, <a href="/notes/9j4ykuha11rmbi2q6e3fmm3">HTTP</a>). In prod, you'd usually lock it in the VPC IP Range.</p>
<ol start="27">
<li><strong>Differentiate Docker Image and Docker Layer.</strong></li>
</ol>
<ul>
<li>Docker Layers are not separate components, you cannot have a single Docker Layer and use it. They're a subcomponent of a Docker Image.</li>
<li>But an Image can consist of a single Layer(that's often the case when running <code>squash</code> command)</li>
<li>Each Layer is an Image by itself.</li>
<li>They're generated when you run <code>docker container</code> commands.</li>
<li>Each Layer stores the changes compared to the image it's based on <code>docker history</code> can fetch you info on it's Layers.</li>
<li>Each instruction in a Dockerfile results in a layer.</li>
</ul>
<ol start="28">
<li><strong>What is bastion host or gateway server and what roles do they play?</strong></li>
</ol>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">Bastion Host</span></div>
<a href="/notes/9zyk7wwcwwwx1bi9pxy7e82" class="portal-arrow">Go to text <span class="right-arrow">→</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><p>A Bastion Host/Server is used to manage access to an internal or private network from an external network. It is also known as a Gateway Server or a Jump Box or a Jump Server.</p>
<p>It basically helps with security - monitoring incoming and outgoing traffic. Bastion Host has clear user rules defined; who can access what.</p>
<p>User will first sign into Bastion Host and get validated and proceed with SSHing into other machines. If we want to cut off all the external access to our internal servers, all we'd have to do is destroy the Bastion Host.</p>
</div></div><p></p><p></p>
<ol start="29">
<li><strong>How do you troubleshoot an Auto Scaling Group that is facing issues provisioning new nodes(it is using spot instances)?</strong></li>
</ol>
<p>Having an <a href="/notes/520xdfstn930e413i3byvxs">AWS Auto Scaling Group</a> full of only spot instances is not a good idea unless the application can bear a little bit of downtime or have a back up ASG.</p>
<p>There could be two of many causes for this:</p>
<ul>
<li>Increase in spot price</li>
<li>EC2 quota limit</li>
</ul>
<p>Spot instances an be taken away from you if there is a change in bid price. Bidding high doesn't guarantee yous spot instances.</p>
<p>There is a soft limit set of every account on how many EC2 instances you can spin up. It can be mitigated by going to AWS Support and raise a ticket for increasing quota limit.</p>
<ol start="30">
<li><strong>Troubleshoot a Pod that is unable to access a volume due to access error.</strong></li>
</ol>
<p>Volumes are handled/provisioned in Kubernetes as a part of PVs.</p>
<p>AccessMode of volume - see that your PVC or your volume supports <code>ReadWriteMany</code> permission for multi-pod access.</p>
<p>The access modes are:</p>
<ul>
<li>
<p><code>ReadWriteOnce</code> the volume can be mounted as read-write by a single node. ReadWriteOnce access mode can still allow multiple pods to access the volume when the pods are running on the same node.</p>
</li>
<li>
<p><code>ReadOnlyMany</code> the volume can be mounted as read-only by many nodes.</p>
</li>
<li>
<p><code>ReadWriteMany</code> the volume can be mounted as read-write by many nodes.</p>
</li>
<li>
<p><code>ReadWriteOncePod</code> the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI volumes and Kubernetes version 1.22+.</p>
</li>
<li>
<p><a href="/notes/i6etf4qol4i62as5pcjfphu">NFS</a> allows <code>ReadWriteMany</code>.</p>
</li>
<li>
<p><a href="/notes/fsmsti0qns6uixhjwtqk7p6">AWS EBS</a> only allows <code>ReadWriteOnce</code>.</p>
</li>
</ul>
<ol start="31">
<li><strong>How to setup K8s in AWS?</strong></li>
</ol>
<p>You can set them up directly on <a href="/notes/1h5mmmv2b68di6siwa31jvh">AWS EC2</a>. Using either <code>kops</code> commands or <code>kubeadmin</code>. Or you can go with <a href="/notes/ervusa6qabwyx7sbb1a4vcg">AWS EKS</a>.</p>
<p>If you want to set it up locally you'd go for <code>minicube</code>.</p>
<p>In prod we look for stability of our environment, hence the safest bet would be to go with a managed service such as EKS. For development you can go with <a href="https://github.com/kubernetes/kops">kOps</a> clusters.</p>
<p>Code deployment should be taken care by <a href="/notes/5uu4351w46y16xtyufz0fy4">Helm</a>.</p>
<ol start="32">
<li><strong>Explain Load Balancers in AWS.</strong></li>
</ol>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">Load Balancer</span></div>
<a href="/notes/hjm4wdsz5kubm8h8pm006dd" class="portal-arrow">Go to text <span class="right-arrow">→</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><p>What is a Load Balancer? A load balancer acts as the “traffic cop” sitting in front of your servers and routing client requests across all servers capable of fulfilling those requests in a manner that maximizes speed and capacity utilization and ensures that no one server is overworked, which could degrade performance.</p>
<p>If a single server goes down, the load balancer redirects traffic to the remaining online servers. When a new server is added to the server group, the load balancer automatically starts to send requests to it.</p>
<h2 id="types-of-load-balancers--based-on-functions"><a aria-hidden="true" class="anchor-heading" href="#types-of-load-balancers--based-on-functions"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Types of Load Balancers – Based on Functions</h2>
<p>Several load balancing techniques are there for addressing the specific network issues:</p>
<p><strong>Network Load Balancer / Layer 4 (L4) Load Balancer:</strong></p>
<p>Based on the network variables like <a href="/notes/mj7tlpbhhzfpg8pbegkkasb">IP Address</a> and destination ports, Network Load balancing is the distribution of traffic at the transport level through the routing decisions. Such load balancing is TCP i.e. level 4, and does not consider any parameter at the application level like the type of content, cookie data, headers, locations, application behavior etc. Performing network addressing translations without inspecting the content of discrete packets, Network Load Balancing cares only about the network layer information and directs the traffic on this basis only.</p>
<p><strong>Application Load Balancer / Layer 7 (L7) Load Balancer:</strong></p>
<p>Ranking highest in the <a href="/notes/mmu3q72ghud78jtb8fv44ex">OSI Model</a>, Layer 7 load balancer distributes the requests based on multiple parameters at the application level. A much wider range of data is evaluated by the L7 load balancer including the HTTP headers and SSL sessions and distributes the server load based on the decision arising from a combination of several variables. This way application load balancers control the server traffic based on the individual usage and behavior.</p>
<p><strong>Global Server Load Balancer/Multi-site Load Balancer:</strong></p>
<p>With the increasing number of applications being hosted in cloud data centers, located at varied geographies, the GSLB extends the capabilities of general L4 and L7 across various data centers facilitating the efficient global load distribution, without degrading the experience for end users. In addition to the efficient traffic balancing, multi-site load balancers also help in quick recovery and seamless business operations, in case of server disaster or disaster at any data center, as other data centers at any part of the world can be used for business continuity.</p>
<h2 id="types-of-load-balancers--based-on-configurations"><a aria-hidden="true" class="anchor-heading" href="#types-of-load-balancers--based-on-configurations"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Types of Load Balancers – Based on Configurations</h2>
<p>Load Balancers are also classified as:</p>
<p><strong>Hardware Load Balancers:</strong></p>
<p>As the name suggests, this is a physical, on-premise, hardware equipment to distribute the traffic on various servers. Though they are capable of handling a huge volume of traffic but are limited in terms of flexibility, and are also fairly high in prices.</p>
<p><strong>Software Load Balancers:</strong></p>
<p>They are the computer applications that need to be installed in the system and function similarly to the hardware load balancers. They are of two kinds- Commercial and Open Source and are a cost-effective alternative to the hardware counterparts.</p>
<p><strong>Virtual Load Balancers:</strong></p>
<p>This load balancer is different from both the software and hardware load balancers as it is the combination of the program of a hardware load balancer working on a virtual machine.</p>
<p>Through virtualization, this kind of load balancer imitates the software driven infrastructure. The program application of hardware equipment is executed on a virtual machine to get the traffic redirected accordingly. But such load balancers have similar challenges as of the physical on-premise balancers viz. lack of central management, lesser scalability and much limited automation.</p>
<h2 id="load-balancing-methods"><a aria-hidden="true" class="anchor-heading" href="#load-balancing-methods"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Load Balancing Methods</h2>
<p>All kinds of Load Balancers receive the balancing requests, which are processed in accordance with a pre-configured algorithm.</p>
<p><strong>Industry Standard Algorithms</strong></p>
<p>The most common load balancing methodologies include:</p>
<ul>
<li>
<p>Round Robin Algorithm:
It relies on a rotation system to sort the traffic when working with servers of equal value. The request is transferred to the first available server and then that server is placed at the bottom of the line.</p>
</li>
<li>
<p>Weighted Round Robin Algorithm:
This algorithm is deployed to balance loads of different servers with different characteristics.</p>
</li>
<li>
<p>Least Connections Algorithm:
In this algorithm, traffic is directed to the server having the least traffic. This helps maintain the optimized performance, especially at peak hours by maintaining a uniform load at all the servers.</p>
</li>
<li>
<p>Least Response Time Algorithm:
This algorithm, like the least connection one, directs traffic to the server with a lower number of active connections and also considers the server having the least response time as its top priority.</p>
</li>
<li>
<p>IP Hash Algorithm:
A fairly simple balancing technique assigns the client’s IP address to a fixed server for optimal performance.</p>
</li>
</ul>
<p>Via - <a href="https://www.appviewx.com/education-center/load-balancer-and-types/">Load Balancer | Types of Load Balancers | Benefits of Load balancer</a></p>
</div></div><p></p><p></p>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">AWS ELB</span></div>
<a href="/notes/uedjwc3ldf8k1p2l1l7fcxj" class="portal-arrow">Go to text <span class="right-arrow">→</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><p>Elastic Load Balancing supports the following types of load balancers: Application Load Balancers, Network Load Balancers, and Classic Load Balancers. Amazon ECS services can use these types of load balancer. Application Load Balancers are used to route HTTP/HTTPS (or Layer 7) traffic. Network Load Balancers and Classic Load Balancers are used to route <a href="/notes/ahxqeweymljwoy5u6g6hxb3">TCP</a> (or Layer 4) traffic.</p>
<p><strong>Classic Load Balancer</strong></p>
<ul>
<li>Uses Round Robin traffic routing</li>
<li>Will be deprecated from Aug 2022</li>
</ul>
<p><strong>Application Load Balancer</strong></p>
<ul>
<li>Path based routing - if you have path <code>/home</code> or <code>/mobile</code> serving different pages from different ASGs.</li>
<li>Multiple ASG balancing.</li>
</ul>
<p><strong>Network Load Balancer</strong></p>
<ul>
<li>Streaming service(packet transmission)</li>
<li>Uses network layer (TCP protocol) (makes this LB faster)</li>
</ul>
<p><strong>Gateway Load Balancer</strong></p>
<ul>
<li>GWLB Target groups support the Generic Networking Virtualization Encapsulation(<strong>GENEVE</strong>) on port: 6081.</li>
<li>Runs within one Availability Zone.</li>
</ul>
</div></div><p></p><p></p>
<ol start="33">
<li><strong>Have you used sonarqube?</strong></li>
</ol>
<p></p><p></p><div class="portal-container">
<div class="portal-head">
<div class="portal-backlink">
<div class="portal-title">From <span class="portal-text-title">SonarQube</span></div>
<a href="/notes/llkndm159erk2w16us6q24k" class="portal-arrow">Go to text <span class="right-arrow">→</span></a>
</div>
</div>
<div id="portal-parent-anchor" class="portal-parent" markdown="1">
<div class="portal-parent-fader-top"></div>
<div class="portal-parent-fader-bottom"></div><p>SonarQube® is an automatic code review tool to detect bugs, vulnerabilities, and code smells in your code. It can integrate with your existing workflow to enable continuous code inspection across your project branches and pull requests.</p>
<ul>
<li><a href="https://www.sonarlint.org/">SonarLint</a> – SonarLint is a companion product that works in your editor giving immediate feedback so you can catch and fix issues before they get to the repository.</li>
<li><a href="https://docs.sonarqube.org/latest/user-guide/quality-gates/">Quality Gate</a> – The Quality Gate lets you know if your project is ready for production.</li>
<li><a href="https://docs.sonarqube.org/latest/user-guide/clean-as-you-code/">Clean as You Code</a> – Clean as You Code is an approach to code quality that eliminates a lot of the challenges that come with traditional approaches. As a developer, you focus on maintaining high standards and taking responsibility specifically in the New Code you're working on.</li>
<li><a href="https://docs.sonarqube.org/latest/user-guide/issues/">Issues</a> – SonarQube raises issues whenever a piece of your code breaks a coding rule, whether it's an error that will break your code (bug), a point in your code open to attack (vulnerability), or a maintainability issue (code smell).</li>
<li><a href="https://docs.sonarqube.org/latest/user-guide/security-hotspots/">Security Hotspots</a> – SonarQube highlights security-sensitive pieces of code that need to be reviewed. Upon review, you'll either find there is no threat or you need to apply a fix to secure the code.</li>
</ul>
<p>Via - <a href="https://docs.sonarqube.org/latest/">SonarQube Docs</a></p>
</div></div><p></p><p></p>
<ol start="34">
<li><strong>Best practices for Incident Management.</strong></li>
</ol>
<p>Applications expose metrics, metrics are collected using monitoring system, we have alert rules to trigger a phone call, Slack notification or email to the <a href="/notes/01rcytslgl5ysbgp9kzekoj">On Call</a> engineer.</p>
<p>The organization should have a proper:</p>
<ul>
<li>Monitoring system
<ul>
<li><a href="/notes/42qbhsb3tdd4z7w7ynfmjj1">AWS CloudWatch</a></li>
<li><a href="/notes/9fa6uf6xbnv00fv944o1y1o">Prometheus</a></li>
</ul>
</li>
<li>Alerting system
<ul>
<li>SNS</li>
<li>AlertManager</li>
</ul>
</li>
</ul>
<p>Postmortem: Understand what went wrong and how to mitigate in the future.</p>
<ol start="35">
<li><strong>How to validate variables during terraform plan time, for example format of the variable?</strong></li>
</ol>
<p>In <a href="/notes/mkwoo3ek4y6bnddpy1iteb9">Terraform</a> you can define a <code>validation</code> block and specify a condition for the variable.</p>
<p><img src="https://res.cloudinary.com/zubayr/image/upload/v1656067537/wiki/heqtzp8ccvxdlfsaqsbx.png"></p>
<p>Scenario: String may not contain a /.</p>
<pre class="language-json"><code class="language-json">variable <span class="token string">"string_may_not_contain"</span> <span class="token punctuation">{</span>
  type = string
  default = <span class="token string">"test"</span>

  validation <span class="token punctuation">{</span>
    error_message = <span class="token string">"Value cannot contain a \"/\"."</span>
    condition = !can(regex(<span class="token string">"/"</span><span class="token punctuation">,</span> var.string_may_not_contain))
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre>
<p>Example via - <a href="https://dev.to/drewmullen/terraform-variable-validation-with-samples-1ank">Terraform: Variable validation with samples</a></p>
<ol start="36">
<li><strong>Explain/Differentiate <code>CMD</code> and <code>ENTRYPOINT</code> in Docker.</strong></li>
</ol>
<p>The CMD command​ specifies the instruction that is to be executed when a Docker container starts. This CMD command is not really necessary for the container to work, as the echo command can be called in a RUN statement as well. The main purpose of the CMD command is to launch the software required in a container.</p>
<p>CMD commands are ignored by Daemon when there are parameters stated within the docker run command.</p>
<p>CMD. Sets default parameters that can be overridden from the Docker Command Line Interface (CLI) when a container is running.</p>
<p>You can pass input from CMD to ENTRYPOINT.</p>
<pre class="language-bash"><code class="language-bash"><span class="token comment"># CMD</span>
FROM ubuntu:latest
CMD<span class="token punctuation">[</span><span class="token string">"echo"</span>, <span class="token string">"Hello World!"</span><span class="token punctuation">]</span>
</code></pre>
<p>ENTRYPOINT</p>
<p>It is a directive or instruction that is used to specify the executable which should run when a container is started from a Docker image. It has two forms, the first one is the ‘exec’ form and the second one is the ‘shell’ form. If there is no entrypoint or CMD specified in the Docker image, it starts and exits at the same time that means container stops automatically so, we must have to specify entrypoint or CMD so that when we will start the container it should execute or it'll stop.</p>
<p>We can override the ENTRYPOINT instruction while starting the container using the ‘–entrypoint’ flag. Also if we have multiple ENTRYPOINT instructions mentioned in Dockerfile then the last ENTRYPOINT will have an effect.</p>
<p>You can run shell scripts using ENTRYPOINT and pass it's output to CMD</p>
<pre class="language-bash"><code class="language-bash">FROM ubuntu
RUN <span class="token function">apt-get</span> update <span class="token operator">&#x26;&#x26;</span> <span class="token function">apt-get</span> <span class="token function">install</span> -y nginx
ENTRYPOINT <span class="token punctuation">[</span><span class="token string">"nginx"</span>, <span class="token string">"-g"</span>, <span class="token string">"daemon off;"</span><span class="token punctuation">]</span>
</code></pre>
<p>Both run during docker container runtime. Using either one of them is best practice but they can be combined too.</p>
<ol start="37">
<li><strong>Troubleshoot EC2 instance in an ASG that are getting terminated.</strong></li>
</ol>
<p>If EC2 quota and pricing is not the issue:</p>
<p>EC2 instances get terminated if they're unhealthy.EC2 instances can become unhealthy if:</p>
<ul>
<li>Disk space being full.</li>
<li>High CPU usage.</li>
<li>No memory left.</li>
</ul>
<p>To debug</p>
<ul>
<li>Run <a href="/notes/laeri4eob3q0wi2i0d71atn">top</a> command to see CPU utilization, check what process/application is using up the resources. Take this up with your developer.</li>
<li>Disk space EBS volume could be full and and OS might be running out of all disk space.</li>
<li>Run <a href="/notes/hatou0atny4e6tj2yapy1zc">Free</a> command to check if any swap memory is left or not, you might want to increase it.</li>
</ul>
<p>See: <a href="/notes/zbxml2z2zzzsjj8eiagfzex">Create and Use Swap File on Linux</a></p>
<ol start="38">
<li><strong>How to control of deployment of pods on nodes that are going to be used explicitly for those pods?</strong></li>
</ol>
<ul>
<li>We can achieve this by using <strong>Taints and Tolerance</strong> in K8's.</li>
<li>A taint when attached to a node, will ripple pods from getting provisioned or getting accepted.</li>
<li>We can add a taint to a pod <code>kubectl taint nodes nodex key=value:Effect</code></li>
<li>Taints are properties of nodes that push pods away if they don't tolerate this taint.</li>
<li>Like labels, one or more Taints can be applied to a node. The node must not accept any pod that doesn't tolerate all of these taints.</li>
</ul>
<ol start="40">
<li><strong>You've 2 different servers with different ports and usernames, how do you use ansible runbook/playbook on both of the servers?</strong></li>
</ol>
<p>In our <code>ansible.conf</code> we can define different <code>ansible_user</code> and <code>ansible_port</code> in the hostfile.</p>
<pre class="language-conf"><code class="language-conf">[webservers]
10.4.20.90 ansible_port=4000 ansible_user=roger
39.12.3.23 ansible_port=8001 ansible_user=liam
</code></pre>
<ol start="41">
<li><strong>What is Terraform state lock?</strong></li>
</ol>
<p>If supported by your backend, <a href="/notes/mkwoo3ek4y6bnddpy1iteb9">Terraform</a> will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state. State locking happens automatically on all operations that could write state.</p>
<p>You won't see any logs of when/as this happens. If state locking fails, Terraform will continue.</p>
<p>You can disable state locking for most commands with the <code>-lock</code> flag but it is not recommended. If acquiring the lock is taking longer than expected, Terraform will output a status message. If Terraform doesn't output a message, state locking is still occurring if your backend supports it.</p>
<ol start="42">
<li><strong>How do you setup slack notifications on Jekins?</strong></li>
</ol>
<p>You can leverage Jenkins plugins, you can use the Jenkins - Slack plugin and use Slack webhook URL.
Whenever a pipeline job fails:
`slackSend color: "failure", message: "Pipeline failed, critical."</p>
<ol start="43">
<li><strong>How do you see your trajectory as a DevOps Engineer?</strong></li>
</ol>
<p>There is always something to learn. You could mention some of the tools/technologies that you want to learn - some you want to better understand. Show them that you are someone who is looking to constantly improve himself/herself(themselves).</p>
<p>You'd want to collaborate with your senior engineers to learn from them directly. Maybe you want to take up more responsibility in the organization etc.</p>
<ol start="44">
<li><strong>How to ignore a certain part of our ansible playbook that might fail and cause our playbook to exit?</strong></li>
</ol>
<p>We can achieve this using <code>ignore_errors</code> sub arguments.</p>
<pre class="language-yaml"><code class="language-yaml"><span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> Do not count this as a failure
  <span class="token key atrule">ansible.builtin.command</span><span class="token punctuation">:</span> /bin/false
    <span class="token key atrule">ignore_errors</span><span class="token punctuation">:</span> yes
</code></pre>
<ol start="45">
<li><strong>When making a change to existing code what is a <code>git</code> best practice?</strong></li>
</ol>
<p>Git clone the repo, checkout the respective branch. Make your change and commit your changes. To understand the motivation behind the change and the history, you can use <code>git blame</code>.</p>
<p>The git blame command is used to examine the contents of a file line by line and see when each line was last modified and who the author of the modifications was. Basically gives you the history and the author of the file.</p>
<ol start="46">
<li><strong>At a high level, create an shell script to automatically push certain logs to S3 at a particular time.</strong></li>
</ol>
<p>This is a Whiteboard design question.</p>
<ul>
<li>Firstly, we'd make sure what we're using, IAM role or access keys.</li>
</ul>
<pre class="language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token comment"># assign path-to-logs to a variable</span>

<span class="token comment"># use aws CLI commands to copy files to S3</span>

<span class="token comment"># aws s3 cp &#x3C;path-to-logs> &#x3C;s3-bucket></span>

<span class="token comment"># use cron job to execute the script at a particular time</span>
</code></pre>
<ol start="47">
<li><strong>What is a package in Python?</strong></li>
</ol>
<p>The module is a simple Python file that contains collections of functions and global variables and with having a .py extension file. It is an executable file.To organize modules we have <strong>Packages</strong> in Python.</p>
<p>Modules can have other modules inside them.</p>
<p>Package(1) -> Modules(x) -> Fns(x)</p>
<p>Example module:</p>
<pre class="language-py"><code class="language-py"><span class="token comment">#the os module provides an operating system interface from Python</span>
<span class="token keyword">import</span> os
<span class="token comment">#prints the name of the operating system</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>name<span class="token punctuation">)</span>
<span class="token comment">#prints the absolute path for the module</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<ol start="48">
<li><strong>What are your day to day responsibilities as a DevOps engineer?</strong></li>
</ol>
<p>Paint a brief picture of what your day to day work looks like.</p>
<ul>
<li>A DevOps engineer works in collaboration other engineers(devs, testers, SRE, sales).</li>
<li>9:00 AM -> 10:00 AM check Slack, update <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">devlog.Jira (Private)</a> with your tasks.</li>
<li>10:00 AM -> 10:30 AM daily standup, share progress with your team on the work you've done previous day and what you'll do today.</li>
<li>10:30 AM -> 1:00 PM work on your Jira ticket.</li>
<li>1:00 PM -> 2:00 PM lunch.</li>
<li>2:00 PM -> 4:00 PM pair program with other engineers or meetings, design discussions.</li>
<li>4:00 PM -> 5:00 PM learning new stuff that can benefit the org, share knowledge with other team members.</li>
</ul>
<h2 id="credits"><a aria-hidden="true" class="anchor-heading" href="#credits"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Credits</h2>
<ul>
<li><a href="https://www.udemy.com/course/50-devops-interview-questions-answers/">50 DevOps Interview Questions &#x26; Answers - 2022 | Udemy</a></li>
</ul>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/522ww47pqvt6y0yj8zparmy">DevOps</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#credits" title="Credits">Credits</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"5eybx5vd339tjnkglx8igde","title":"Devops Interview Questions","desc":"","updated":1656161299068,"created":1655882305714,"custom":{},"fname":"devlog.devops interview questions","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"c115db526fea49e4d94df64eac8ebe97","links":[{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EC2","alias":"devlog.AWS EC2","position":{"start":{"line":4,"column":3,"offset":104},"end":{"line":4,"column":21,"offset":122},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EC2"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EBS","alias":"devlog.AWS EBS","position":{"start":{"line":4,"column":53,"offset":154},"end":{"line":4,"column":71,"offset":172},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EBS"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.prometheus","alias":"devlog.prometheus","position":{"start":{"line":11,"column":40,"offset":632},"end":{"line":11,"column":61,"offset":653},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.prometheus"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.kubernetes","alias":"devlog.kubernetes","position":{"start":{"line":17,"column":1,"offset":750},"end":{"line":17,"column":22,"offset":771},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.kubernetes"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.kOps","alias":"devlog.kOps","position":{"start":{"line":17,"column":23,"offset":772},"end":{"line":17,"column":38,"offset":787},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.kOps"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EKS","alias":"devlog.AWS EKS","position":{"start":{"line":19,"column":143,"offset":1024},"end":{"line":19,"column":161,"offset":1042},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EKS"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS","alias":"devlog.AWS","position":{"start":{"line":25,"column":19,"offset":1331},"end":{"line":25,"column":33,"offset":1345},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EC2","alias":"devlog.AWS EC2","position":{"start":{"line":25,"column":106,"offset":1418},"end":{"line":25,"column":124,"offset":1436},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EC2"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EMR","alias":"devlog.AWS EMR","position":{"start":{"line":33,"column":38,"offset":1991},"end":{"line":33,"column":56,"offset":2009},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EMR"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.git","alias":"devlog.git","position":{"start":{"line":37,"column":38,"offset":2281},"end":{"line":37,"column":52,"offset":2295},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.git"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.web server \u0026 application server","alias":"devlog.web server \u0026 application server","position":{"start":{"line":43,"column":1,"offset":2448},"end":{"line":43,"column":43,"offset":2490},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.web server \u0026 application server"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.tomcat","alias":"devlog.tomcat","position":{"start":{"line":45,"column":32,"offset":2523},"end":{"line":45,"column":49,"offset":2540},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.tomcat"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS CloudTrail","alias":"devlog.AWS CloudTrail","position":{"start":{"line":54,"column":266,"offset":3139},"end":{"line":54,"column":291,"offset":3164},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS CloudTrail"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.splunk","alias":"devlog.splunk","position":{"start":{"line":54,"column":295,"offset":3168},"end":{"line":54,"column":312,"offset":3185},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.splunk"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS Auto Scaling Group","alias":"devlog.AWS Auto Scaling Group","position":{"start":{"line":58,"column":138,"offset":3371},"end":{"line":58,"column":171,"offset":3404},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS Auto Scaling Group"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.kubernetes","alias":"devlog.kubernetes","position":{"start":{"line":58,"column":197,"offset":3430},"end":{"line":58,"column":218,"offset":3451},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.kubernetes"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.python","alias":"devlog.python","position":{"start":{"line":102,"column":39,"offset":4839},"end":{"line":102,"column":56,"offset":4856},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.python"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.Docker","alias":"devlog.Docker","position":{"start":{"line":131,"column":34,"offset":5793},"end":{"line":131,"column":51,"offset":5810},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Docker"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.Jenkins","alias":"devlog.Jenkins","position":{"start":{"line":134,"column":64,"offset":6108},"end":{"line":134,"column":82,"offset":6126},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Jenkins"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.on-call","alias":"devlog.on-call","position":{"start":{"line":141,"column":155,"offset":6740},"end":{"line":141,"column":173,"offset":6758},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.on-call"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.prometheus","alias":"devlog.prometheus","position":{"start":{"line":145,"column":3,"offset":6808},"end":{"line":145,"column":24,"offset":6829},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.prometheus"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS CloudWatch","alias":"devlog.AWS CloudWatch","position":{"start":{"line":146,"column":3,"offset":6848},"end":{"line":146,"column":28,"offset":6873},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS CloudWatch"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.kubernetes","alias":"devlog.kubernetes","position":{"start":{"line":151,"column":90,"offset":7045},"end":{"line":151,"column":111,"offset":7066},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.kubernetes"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS S3","alias":"devlog.AWS S3","position":{"start":{"line":155,"column":3,"offset":7204},"end":{"line":155,"column":20,"offset":7221},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS S3"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.Jekins","alias":"devlog.Jekins","position":{"start":{"line":161,"column":179,"offset":7528},"end":{"line":161,"column":196,"offset":7545},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Jekins"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS S3","alias":"devlog.AWS S3","position":{"start":{"line":165,"column":93,"offset":7952},"end":{"line":165,"column":110,"offset":7969},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS S3"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EBS","alias":"devlog.AWS EBS","position":{"start":{"line":226,"column":97,"offset":10183},"end":{"line":226,"column":115,"offset":10201},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EBS"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS Secrets Manager","alias":"devlog.AWS Secrets Manager","position":{"start":{"line":240,"column":52,"offset":10935},"end":{"line":240,"column":82,"offset":10965},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS Secrets Manager"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.Redis","alias":"devlog.Redis","position":{"start":{"line":247,"column":5,"offset":11243},"end":{"line":247,"column":21,"offset":11259},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Redis"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.mongoDB","alias":"devlog.mongoDB","position":{"start":{"line":251,"column":5,"offset":11327},"end":{"line":251,"column":23,"offset":11345},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mongoDB"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS DynamoDB","alias":"devlog.AWS DynamoDB","position":{"start":{"line":252,"column":5,"offset":11350},"end":{"line":252,"column":28,"offset":11373},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS DynamoDB"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.mysql","alias":"devlog.mysql","position":{"start":{"line":254,"column":5,"offset":11394},"end":{"line":254,"column":21,"offset":11410},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mysql"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS Aurora","alias":"devlog.AWS Aurora","position":{"start":{"line":255,"column":5,"offset":11415},"end":{"line":255,"column":26,"offset":11436},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS Aurora"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.PostgreSQL","alias":"devlog.PostgreSQL","position":{"start":{"line":256,"column":5,"offset":11441},"end":{"line":256,"column":26,"offset":11462},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.PostgreSQL"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.Packer","alias":"devlog.Packer","position":{"start":{"line":260,"column":13,"offset":11579},"end":{"line":260,"column":30,"offset":11596},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Packer"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS AMI","alias":"devlog.AWS AMI","position":{"start":{"line":264,"column":26,"offset":11982},"end":{"line":264,"column":44,"offset":12000},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS AMI"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.terraform","alias":"devlog.terraform","position":{"start":{"line":269,"column":106,"offset":12259},"end":{"line":269,"column":126,"offset":12279},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.terraform"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"archive.ingress","alias":"archive.ingress","position":{"start":{"line":278,"column":1,"offset":12684},"end":{"line":278,"column":20,"offset":12703},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.ingress"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"archive.egress","alias":"archive.egress","position":{"start":{"line":278,"column":25,"offset":12708},"end":{"line":278,"column":43,"offset":12726},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.egress"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.firewall","alias":"devlog.firewall","position":{"start":{"line":285,"column":48,"offset":12866},"end":{"line":285,"column":67,"offset":12885},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.firewall"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.ssh","alias":"devlog.ssh","position":{"start":{"line":287,"column":51,"offset":13082},"end":{"line":287,"column":65,"offset":13096},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.ssh"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.http","alias":"devlog.http","position":{"start":{"line":287,"column":67,"offset":13098},"end":{"line":287,"column":82,"offset":13113},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.http"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS Auto Scaling Group","alias":"devlog.AWS Auto Scaling Group","position":{"start":{"line":304,"column":11,"offset":13974},"end":{"line":304,"column":44,"offset":14007},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS Auto Scaling Group"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.NFS","alias":"devlog.NFS","position":{"start":{"line":328,"column":3,"offset":15437},"end":{"line":328,"column":17,"offset":15451},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.NFS"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EBS","alias":"devlog.AWS EBS","position":{"start":{"line":329,"column":3,"offset":15478},"end":{"line":329,"column":21,"offset":15496},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EBS"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EC2","alias":"devlog.AWS EC2","position":{"start":{"line":333,"column":33,"offset":15593},"end":{"line":333,"column":51,"offset":15611},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EC2"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS EKS","alias":"devlog.AWS EKS","position":{"start":{"line":333,"column":117,"offset":15677},"end":{"line":333,"column":135,"offset":15695},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS EKS"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.helm","alias":"devlog.helm","position":{"start":{"line":339,"column":41,"offset":16005},"end":{"line":339,"column":56,"offset":16020},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.helm"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.on-call","alias":"devlog.on-call","position":{"start":{"line":353,"column":157,"offset":16373},"end":{"line":353,"column":175,"offset":16391},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.on-call"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS CloudWatch","alias":"devlog.AWS CloudWatch","position":{"start":{"line":358,"column":5,"offset":16467},"end":{"line":358,"column":30,"offset":16492},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.AWS CloudWatch"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.prometheus","alias":"devlog.prometheus","position":{"start":{"line":359,"column":5,"offset":16497},"end":{"line":359,"column":26,"offset":16518},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.prometheus"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.terraform","alias":"devlog.terraform","position":{"start":{"line":368,"column":4,"offset":16740},"end":{"line":368,"column":24,"offset":16760},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.terraform"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.top","alias":"devlog.top","position":{"start":{"line":432,"column":7,"offset":19360},"end":{"line":432,"column":21,"offset":19374},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.top"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.free","alias":"devlog.free","position":{"start":{"line":434,"column":7,"offset":19594},"end":{"line":434,"column":22,"offset":19609},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.free"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.create and use swap file on linux","alias":"devlog.create and use swap file on linux","position":{"start":{"line":436,"column":6,"offset":19699},"end":{"line":436,"column":50,"offset":19743},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.create and use swap file on linux"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.Terraform","alias":"devlog.Terraform","position":{"start":{"line":458,"column":31,"offset":20740},"end":{"line":458,"column":51,"offset":20760},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Terraform"}},{"type":"wiki","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.Jira","alias":"devlog.Jira","position":{"start":{"line":534,"column":43,"offset":24104},"end":{"line":534,"column":58,"offset":24119},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Jira"}},{"type":"ref","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.prometheus","position":{"start":{"line":13,"column":1,"offset":676},"end":{"line":13,"column":40,"offset":715},"indent":[]},"xvault":false,"to":{"fname":"devlog.prometheus","anchorHeader":" Getting-metrics"}},{"type":"ref","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.helm","position":{"start":{"line":94,"column":1,"offset":4616},"end":{"line":94,"column":17,"offset":4632},"indent":[]},"xvault":false,"to":{"fname":"devlog.helm"}},{"type":"ref","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.build automation","position":{"start":{"line":274,"column":1,"offset":12569},"end":{"line":274,"column":86,"offset":12654},"indent":[]},"xvault":false,"to":{"fname":"devlog.build automation","anchorHeader":" Building-Java-Applications:# Publishing-build-artifacts"}},{"type":"ref","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.bastion host","position":{"start":{"line":300,"column":1,"offset":13809},"end":{"line":300,"column":25,"offset":13833},"indent":[]},"xvault":false,"to":{"fname":"devlog.bastion host"}},{"type":"ref","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.load balancer","position":{"start":{"line":343,"column":1,"offset":16063},"end":{"line":343,"column":26,"offset":16088},"indent":[]},"xvault":false,"to":{"fname":"devlog.load balancer"}},{"type":"ref","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.AWS ELB","position":{"start":{"line":345,"column":1,"offset":16090},"end":{"line":345,"column":20,"offset":16109},"indent":[]},"xvault":false,"to":{"fname":"devlog.AWS ELB"}},{"type":"ref","from":{"fname":"devlog.devops interview questions","id":"5eybx5vd339tjnkglx8igde","vaultName":"Dendron"},"value":"devlog.sonarqube","position":{"start":{"line":349,"column":1,"offset":16145},"end":{"line":349,"column":22,"offset":16166},"indent":[]},"xvault":false,"to":{"fname":"devlog.sonarqube"}},{"from":{"fname":"areas.devops","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":175,"column":3,"offset":5113},"end":{"line":175,"column":40,"offset":5150},"indent":[]},"value":"devlog.devops interview questions","alias":"devlog.devops interview questions"}],"anchors":{"credits":{"type":"header","text":"Credits","value":"credits","line":547,"column":0,"depth":2}},"children":[],"parent":"9gtn7g40cvqui0sifl1s7t5","data":{}},"body":"\u003ch1 id=\"devops-interview-questions\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#devops-interview-questions\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eDevops Interview Questions\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eEC2 instance is running out of disk space. What actions will you take to mitigate the issue?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/1h5mmmv2b68di6siwa31jvh\"\u003eAWS EC2\u003c/a\u003e disk space typically refers to \u003ca href=\"/notes/fsmsti0qns6uixhjwtqk7p6\"\u003eAWS EBS\u003c/a\u003e volume.\u003c/li\u003e\n\u003cli\u003eWe'll first check if its a root volume or any other volume\n\u003cul\u003e\n\u003cli\u003e/root - OS\u003c/li\u003e\n\u003cli\u003e/application - for all our applications\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eIf its root volume then we'll first try to check logs(\u003ccode\u003e/var/logs\u003c/code\u003e) and clear some space if not the instance might shut down.\u003c/li\u003e\n\u003cli\u003eIf its the application volume(learn the reason of high disk usage), then we'll use EBS feature to take the snapshot and increase disk space for the EC2 instance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eExplain different ways in which\u003c/strong\u003e \u003ca href=\"/notes/9fa6uf6xbnv00fv944o1y1o\"\u003ePrometheus\u003c/a\u003e \u003cstrong\u003ecan get metrics?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003ePrometheus\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/9fa6uf6xbnv00fv944o1y1o\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003e→\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003ch2 id=\"getting-metrics\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#getting-metrics\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eGetting metrics\u003c/h2\u003e\n\u003cp\u003eHow does Prometheus collect metrics from the targets?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePulling\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYour application(regardless of technology) will have to expose a metrics HTTP endpoint and Prometheus will scrape from the endpoint. By default is is: \u003ccode\u003ehostaddress/metrics\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eData available in the \u003ccode\u003e/metrics\u003c/code\u003e endpoint should be in the correct format that Prometheus understands.\u003c/p\u003e\n\u003cp\u003eSome servers expose Prometheus endpoints by default so you don't really have to do extra work for it. But many services don't have native Prometheus endpoints in which case you'd need an \u003cstrong\u003eExporter\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExporter\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIt basically a script/service that fetches metrics from your target and converts them in format Prometheus understands and exposes it's converted data at it's own \u003ccode\u003e/metrics\u003c/code\u003e endpoint where Prometheus can scrape them.\u003c/p\u003e\n\u003cp\u003ePrometheus has a list of exporters for different services like \u003ca href=\"/notes/ypszfixe0p3s5k0inqs5g08\"\u003eMySql\u003c/a\u003e, \u003ca href=\"/notes/boyz5i8gtwc9aoj4zfz556y\"\u003eElasticsearch\u003c/a\u003e, \u003ca href=\"/notes/owoutsv5dicylguol2odc3e\"\u003eLinux\u003c/a\u003e servers, \u003ca href=\"/notes/qcaw5ht4vcnucydq104gh7v\"\u003eBuild Tools\u003c/a\u003e, Cloud Platforms and so on.\u003c/p\u003e\n\u003cp\u003eIf you want to monitor a Linux server, see: \u003ca href=\"https://prometheus.io/docs/guides/node-exporter/\"\u003eMonitoring Linux host metrics with the Node Exporter | Prometheus\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://res.cloudinary.com/zubayr/image/upload/v1656150261/wiki/xfe8c37gmzogdoin3wtx.png\"\u003e\u003c/p\u003e\n\u003cp\u003eExporters are also available as Docker images. SO\u003c/p\u003e\n\u003cp\u003eIf you want to monitor \u003ca href=\"/notes/ypszfixe0p3s5k0inqs5g08\"\u003eMySql\u003c/a\u003e container in a \u003ca href=\"/notes/gbeh61d6hvbmxxy63chp81b\"\u003eKubernetes\u003c/a\u003e cluster, you can deploy  a sidecar container of MySQL exporter that will run inside the pod with MySQL container, connect to it and start sending MySQL metrics for Prometheus and making them available at it’s own \u003ccode\u003e/metrics\u003c/code\u003e endpoint.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMonitoring  your own applications?\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow many requests your applications are receiving.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow many exceptions are occurring.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow many server resources your application is using.\u003c/p\u003e\n\u003cp\u003e For this you can use Client Libraries for different languages using which you can expose \u003ccode\u003e/metrics\u003c/code\u003e endpoint for metrics that are relevant to you.\n\u003ca href=\"https://prometheus.io/docs/instrumenting/clientlibs/\"\u003eClient libraries | Prometheus\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"push-based-vs-pull-based\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#push-based-vs-pull-based\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePush based VS Pull based\u003c/h2\u003e\n\u003cp\u003eMost monitoring systems like \u003ca href=\"/notes/42qbhsb3tdd4z7w7ynfmjj1\"\u003eAWS CloudWatch\u003c/a\u003e or \u003ca href=\"/notes/g8txkrlv8lggex7u6372sd1\"\u003eNew Relic\u003c/a\u003e etc use a Push system. Applications and servers are responsible for pushing their metric data to a centralized collection platform of that monitoring tool.\u003c/p\u003e\n\u003cp\u003eIn large microservices based system this approach can create a bottleneck for your infrastructure as all of these microservices constantly make push request to your monitoring tool thus flooding your system.\u003c/p\u003e\n\u003cp\u003ePlus, you’ll also need to install additional software(daemons) on each of your targets to push the metrics to the monitoring server. In contrast with Prometheus which only requires a scraping endpoint.\u003c/p\u003e\n\u003cp\u003eMultiple Prometheus instances can collect/pull metrics. Using pull, Prometheus can easily detect whether a service is up and running or not.\u003c/p\u003e\n\u003cp\u003ePushing can be ambiguous when checking if the service is up or not when compared to pull mechanism. Because there can be many reasons for a push request to fail. \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePushing\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePushgateway can be utilized when a target only runs for a short time.\nEg: A batch job, scheduled job etc. For such jobs, Prometheus offers Pushgateway component. So these services can push metrics directly to Prometheus DB.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://res.cloudinary.com/zubayr/image/upload/v1655885235/wiki/yguspabejdgcr4qbzegm.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"configuring-prometheus\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#configuring-prometheus\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eConfiguring Prometheus\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eprometheus.yaml\u003c/code\u003e file contains all the info needed for Prometheus to know what(targets) to scrape and when(intervals). \u003c/p\u003e\n\u003cp\u003ePrometheus then uses \u003cstrong\u003eService Discovery\u003c/strong\u003e  mechanism to find those target endpoints.\u003c/p\u003e\n\u003cp\u003eYou can find the sample config files with default values which comes with your first Prometheus installation.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://res.cloudinary.com/zubayr/image/upload/v1656153062/wiki/p2l9ubydx8h1viv4pafn.png\"\u003e\u003c/p\u003e\n\u003cp\u003eUnder \u003ccode\u003eglobal:\u003c/code\u003e you define how often Prometheus will scrape it’s targets.\u003c/p\u003e\n\u003cp\u003eRules are for aggregating metric values or creating alerts when conditions\nare met.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003escrape_configs:\u003c/code\u003e define what resources Prometheus monitors; essentially targets. You can define your own jobs and default values for each job(overwrite global interval values).\u003c/p\u003e\n\u003cp\u003eSince Prometheus has it’s own \u003ccode\u003e/metrics\u003c/code\u003e endpoint, it can monitor it’s own health.\u003c/p\u003e\n\u003ch2 id=\"alertmanager\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#alertmanager\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eAlertManager\u003c/h2\u003e\n\u003cp\u003eHow does Prometheus trigger alerts that are defined by rules in \u003ccode\u003eprometheus.yaml\u003c/code\u003e and who receives these alerts?\u003c/p\u003e\n\u003cp\u003ePrometheus has a component called AlertManger that is responsible for firing alerts via different channels (Emails, Slack channel or other notification clients).\u003c/p\u003e\n\u003cp\u003ePrometheus server will read alert rules and if the conditions under rules is met an alert is fired. \u003c/p\u003e\n\u003ch2 id=\"prometheus-data-storage\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#prometheus-data-storage\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePrometheus Data Storage\u003c/h2\u003e\n\u003cp\u003eWhere does Prometheus store all the data that it collects/aggregates? How can other systems use this data?\u003c/p\u003e\n\u003cp\u003ePrometheus stores metric data on disks, includes Local on disk \u003cstrong\u003eTime Series DB\u003c/strong\u003e but also optionally integrates with remote storage system. It is stored in custom Time Series format. Because of this you cannot directly write this data on a relational DB or something else.\u003c/p\u003e\n\u003cp\u003eOnce collected, Prometheus lets you query the data through it’s server API using it’s query language called \u003cstrong\u003ePromQL\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"promql\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#promql\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePromQL\u003c/h2\u003e\n\u003cp\u003eYou can use Prometheus dashboard UI to ask Prometheus server via PromQL to for example show the status of a target right now.\u003c/p\u003e\n\u003cp\u003eOr use more powerful data visualization tools like \u003cstrong\u003eGrafana\u003c/strong\u003e to display the data which uses PromQL under the hood to get data out of Prometheus .\u003c/p\u003e\n\u003cp\u003eExample PromQL query to:\u003c/p\u003e\n\u003cp\u003eQuery all HTTP status codes except \u003ccode\u003e4xx\u003c/code\u003e ones\u003c/p\u003e\n\u003cpre class=\"language-sql\"\u003e\u003ccode class=\"language-sql\"\u003ehttp_requests_total{\u003cspan class=\"token keyword\"\u003estatus\u003c/span\u003e\u003cspan class=\"token operator\"\u003e!\u003c/span\u003e\u003cspan class=\"token operator\"\u003e~\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"4..\"\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis query does some subquery:\u003c/p\u003e\n\u003cp\u003eReturns the 5 minute rate of the \u003cem\u003ehttp_requests_total\u003c/em\u003e metric for the past 30 minutes.\u003c/p\u003e\n\u003cpre class=\"language-sql\"\u003e\u003ccode class=\"language-sql\"\u003erate\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ehttp_requests_total\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e5\u003c/span\u003em\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e30\u003c/span\u003em:\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://res.cloudinary.com/zubayr/image/upload/v1656153983/wiki/qu5c4x2hrwuozqeqebxy.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"sailent-characteristics-of-prometheus\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#sailent-characteristics-of-prometheus\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSailent Characteristics of Prometheus\u003c/h2\u003e\n\u003cp\u003eIt is designed to be reliable even when other systems have an outage so you can diagnose the problems and fix them.\u003c/p\u003e\n\u003cp\u003eEach Prometheus server is standalone and self-contained. It doesn’t depend on network storage or other remote services. It is meant to be still working when other parts of the infrastructure are broken.\u003c/p\u003e\n\u003cp\u003eIt doesn’t require extensive setup needed.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDrawbacks:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIt can be difficult to scale, when you have hundreds of servers that you want to use multiple Prometheus instances for aggregation of metrics setting them up can get complicated.\u003c/p\u003e\n\u003cp\u003eA workaround this would be to increase the capacity of your Prometheus server, limit the number of metrics Prometheus collects from applications.\u003c/p\u003e\n\u003ch2 id=\"prometheus-federation\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#prometheus-federation\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePrometheus Federation\u003c/h2\u003e\n\u003cp\u003eTo scale monitoring with scalable cloud apps.\u003c/p\u003e\n\u003cp\u003ePrometheus Federation allows one Prometheus server to scrape data from another Prometheus server. This will allow you to scale your Prometheus setup with your multi-node applications.\u003c/p\u003e\n\u003ch2 id=\"prometheus-with-docker--kubernetes\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#prometheus-with-docker--kubernetes\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePrometheus with Docker \u0026#x26; Kubernetes\u003c/h2\u003e\n\u003cp\u003eIt is fully compatible with both.\u003c/p\u003e\n\u003cp\u003ePrometheus components are available as Docker images and therefore can be deployed on Kubernetes or other container environments.\u003c/p\u003e\n\u003cp\u003eIt provides monitoring of K8s Cluster Node Resource out of the box! Once deployed on K8s, it starts gather metrics data on each Kubernetes node server without any extra configuration.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat is Kubernetes kOps?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"/notes/gbeh61d6hvbmxxy63chp81b\"\u003eKubernetes\u003c/a\u003e \u003ca href=\"/notes/fhi59s6yqplah2w1hfx9i5x\"\u003ekOps\u003c/a\u003e is an automation tool used to setup Kubernetes cluster. It is an alternative to \u003ccode\u003ekubeadmin\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003ekOps can help you spin a test cluster or a small dev cluster quickly. It is not something that will help you setup managed Kubernetes cluster(\u003ca href=\"/notes/ervusa6qabwyx7sbb1a4vcg\"\u003eAWS EKS\u003c/a\u003e). It can create, destroy, upgrade, maintain production-grade, high availability clusters and also provision necessary cloud infrastructure(only recommended if you cannot afford managed service).\u003c/p\u003e\n\u003cp\u003eIt supports many cloud providers.\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat is instance fleet in AWS?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eInstance fleet in \u003ca href=\"/notes/gvpkbgglehtr9ej5e0uj44j\"\u003eAWS\u003c/a\u003e refers to a configuration - information to launch a fleet or a group of \u003ca href=\"/notes/1h5mmmv2b68di6siwa31jvh\"\u003eAWS EC2\u003c/a\u003e instances, in a single API call. A fleet can launch multiple instance(mixed set) types across multiple Availability Zones using On-Demand Instance, Reserved Instance and Spot Instance purchasing options together.\u003c/p\u003e\n\u003cp\u003eIn the configuration you can define:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSeparate capacity targets and maximum amount you're willing to pay per hour for On-Demand, Spot instances.\u003c/li\u003e\n\u003cli\u003eSpecify the instance types that work best for your applications.\u003c/li\u003e\n\u003cli\u003eSpecify how EC2 should distribute your fleet capacity within each purchasing options.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe instance fleet configuration for \u003ca href=\"/notes/w0ukvte4c5p8inzz5kf7zf6\"\u003eAWS EMR\u003c/a\u003e lets you select wide variety of provisioning options for Amazon EC2 instances and helps you develop a flexible and elastic resourcing strategy for each node type in your cluster.\u003c/p\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow do you pass “message” for your git commit\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUse the flag \u003ccode\u003e-m [message]\u003c/code\u003e for your \u003ca href=\"/notes/qkp5jc6pta8a9f35evsbvod\"\u003eGit\u003c/a\u003e commit. Although you can commit without passing a message.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egit commit -m \"🔥 commit message\"\u003c/code\u003e\u003c/p\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat application server are you familiar with?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"/notes/jew452yotysgzsmcmgmzj6j\"\u003eWeb Server \u0026#x26; Application Server\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor Java applications, we have \u003ca href=\"/notes/w2frd082bpmiik2nqaxf8wc\"\u003eTomcat\u003c/a\u003e but there are different application servers for applications based on different technologies.\u003c/p\u003e\n\u003col start=\"7\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow to check logs of a Docker container/filter last 200 lines from the logs.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ccode\u003edocker container logs \u0026#x3C;container_name\u003e\u003c/code\u003e\n\u003ccode\u003edocker container logs --tail 200 \u0026#x3C;container_name\u003e\u003c/code\u003e\u003c/p\u003e\n\u003col start=\"8\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat happens to container logs if it is restarted?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou won’t lose any logs for restarting a container but since containers are stateless, you will lose the logs if a container is \u003cstrong\u003edeleted\u003c/strong\u003e. If you want to persist logs you can use external persistent storage. You can also push the container logs to something like \u003ca href=\"/notes/002tn8rmi02kqtdd6xzllg2\"\u003eAWS CloudTrail\u003c/a\u003e or \u003ca href=\"/notes/rem6mv7lebiuf5ajcmm2acd\"\u003eSplunk\u003c/a\u003e\u003c/p\u003e\n\u003col start=\"9\"\u003e\n\u003cli\u003e\u003cstrong\u003eHorizontal Scaling VS Vertical Scaling\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eHorizontal scaling\u003c/strong\u003e means scaling by adding more machines to your pool of resources (also described as “scaling out”); something like \u003ca href=\"/notes/520xdfstn930e413i3byvxs\"\u003eAWS Auto Scaling Group\u003c/a\u003e, creating replicas(think \u003ca href=\"/notes/gbeh61d6hvbmxxy63chp81b\"\u003eKubernetes\u003c/a\u003e). If you are hosting an application on a server and find that it no longer has the capacity or capabilities to handle traffic, adding a server may be your solution.\u003c/p\u003e\n\u003cp\u003eWhereas \u003cstrong\u003evertical scaling\u003c/strong\u003e refers to scaling by adding more power (e.g. CPU, RAM) to an existing machine (also described as “scaling up”). For instance, if your server requires more processing power, vertical scaling would mean upgrading the CPUs. You can also vertically scale the memory, storage, or network speed.\u003c/p\u003e\n\u003cp\u003eSee: \u003ca href=\"https://www.cloudzero.com/blog/horizontal-vs-vertical-scaling\"\u003eHorizontal Vs. Vertical Scaling: How Do They Compare?\u003c/a\u003e\u003c/p\u003e\n\u003col start=\"10\"\u003e\n\u003cli\u003e\u003cstrong\u003eReplicationController in Kubernetes\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA ReplicationController is responsible for running the specified number of pod copies(replicas) across the cluster.\u003c/p\u003e\n\u003cp\u003eReplicationController is not auto scale.\u003c/p\u003e\n\u003cpre class=\"language-yaml\"\u003e\u003ccode class=\"language-yaml\"\u003e\u003cspan class=\"token key atrule\"\u003eapiVersion\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e v1\n\u003cspan class=\"token key atrule\"\u003ekind\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e ReplicationController\n\u003cspan class=\"token key atrule\"\u003emetadata\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"token key atrule\"\u003ename\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e nginx\n\u003cspan class=\"token key atrule\"\u003espec\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"token key atrule\"\u003ereplicas\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e \u003cspan class=\"token number\"\u003e3\u003c/span\u003e\n  \u003cspan class=\"token key atrule\"\u003eselector\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"token key atrule\"\u003eapp\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e nginx\n  \u003cspan class=\"token key atrule\"\u003etemplate\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"token key atrule\"\u003emetadata\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n      \u003cspan class=\"token key atrule\"\u003ename\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e nginx\n      \u003cspan class=\"token key atrule\"\u003elabels\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"token key atrule\"\u003eapp\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e nginx\n    \u003cspan class=\"token key atrule\"\u003espec\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n      \u003cspan class=\"token key atrule\"\u003econtainers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"token punctuation\"\u003e-\u003c/span\u003e \u003cspan class=\"token key atrule\"\u003ename\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e nginx\n          \u003cspan class=\"token key atrule\"\u003eimage\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e nginx\n          \u003cspan class=\"token key atrule\"\u003eports\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n            \u003cspan class=\"token punctuation\"\u003e-\u003c/span\u003e \u003cspan class=\"token key atrule\"\u003econtainerPort\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e \u003cspan class=\"token number\"\u003e80\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"11\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat is helm?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003eHelm\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/5uu4351w46y16xtyufz0fy4\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003e→\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003cp\u003eHelm is a Kubernetes deployment tool for automating creation, packaging, configuration, and deployment of applications and services to Kubernetes clusters.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/notes/gbeh61d6hvbmxxy63chp81b\"\u003eKubernetes\u003c/a\u003e is a powerful container-orchestration system for application deployment. There are multiple independent resources to deal with, and each requires a dedicated YAML manifest file.\u003c/p\u003e\n\u003cp\u003eHelm deploys packaged applications to Kubernetes and structures them into charts. The charts contain all pre-configured application resources along with all the versions into one easily manageable package.\u003c/p\u003e\n\u003cp\u003eHelm streamlines installing, upgrading, fetching dependencies, and configuring deployments on Kubernetes with simple CLI commands. Software packages are found in repositories or are created.\u003c/p\u003e\n\u003cp\u003eUnlike Homebrew or Aptitude desktop package managers, or Azure Resource Manager templates (ARMs) / Amazon Machine Images (AMIs) that are run on a single server, Helm charts are built atop Kubernetes and benefit from its cluster architecture. The main benefit of this approach is the ability to consider scalability from the start. The charts of all the images used by Helm are stored in a registry called Helm Workspace, so the DevOps teams can search them and add to their projects with ease.\u003c/p\u003e\n\u003cp\u003eKubernetes objects are challenging to manage. With helpful tools, the Kubernetes learning curve becomes smooth and manageable. Helm automates maintenance of YAML manifests for Kubernetes objects by packaging information into charts and advertises them to a Kubernetes cluster.\u003c/p\u003e\n\u003cp\u003eHelm keeps track of the versioned history of every chart installation and change. Rolling back to a previous version or upgrading to a newer version is completed with comprehensible commands.\u003c/p\u003e\n\u003ch2 id=\"helm-chart\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#helm-chart\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eHelm Chart\u003c/h2\u003e\n\u003cp\u003eHelm charts are Helm packages consisting of YAML files and templates which convert into Kubernetes manifest files. Charts are reusable by anyone for any environment, which reduces complexity and duplicates. Folders have the following structure:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://res.cloudinary.com/zubayr/image/upload/v1655905668/wiki/f0iewhj44stf8t90qnfr.png\"\u003e\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e\u003cstrong\u003eName\u003c/strong\u003e\u003c/th\u003e\u003cth\u003e\u003cstrong\u003eType\u003c/strong\u003e\u003c/th\u003e\u003cth\u003e\u003cstrong\u003eFunction\u003c/strong\u003e\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003echarts/\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eDirectory\u003c/td\u003e\u003ctd\u003eDirectory for manually managed chart dependencies.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003etemplates/\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eDirectory\u003c/td\u003e\u003ctd\u003eTemplate files are written in Golang and combined with configuration values from the values.yaml file to generate Kubernetes manifests.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eChart.yaml\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eFile\u003c/td\u003e\u003ctd\u003eMetadata about the chart, such as the version, name, search keywords, etc.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eLICENSE (optional)\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eFile\u003c/td\u003e\u003ctd\u003eLicense for the chart in plaintext format.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eREADME.md (optional)\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eFile\u003c/td\u003e\u003ctd\u003eHuman readable information for the users of the chart.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003erequirements.yaml (optional)\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eFile\u003c/td\u003e\u003ctd\u003eList of chart’s dependencies.\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003evalues.yaml\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003eFile\u003c/td\u003e\u003ctd\u003eDefault configuration \u003ca href=\"https://phoenixnap.com/kb/helm-get-values\"\u003evalues\u003c/a\u003e for the chart.\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eVia - \u003ca href=\"https://phoenixnap.com/kb/what-is-helm\"\u003eWhat is Helm? Helm and Helm Charts Explained\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"resources\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#resources\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eResources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://phoenixnap.com/kb/what-is-helm\"\u003eWhat is Helm? Helm and Helm Charts Explained\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003col start=\"12\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhich Python module would you use to write a simple program to test API code?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe code should just check if the API endpoint is working or not.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnswer:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eI would use the \u003cstrong\u003erequest\u003c/strong\u003e module in \u003ca href=\"/notes/wa4wthnhw54hd5wfyocyvqa\"\u003ePython\u003c/a\u003e. It has the \u003ccode\u003eget()\u003c/code\u003e function wherein you'd pass pass the API endpoint and fetch status/http code \u003ccode\u003e.status_code\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf it returns \u003ccode\u003e200\u003c/code\u003e - API endpoint is working fine.\u003c/p\u003e\n\u003col start=\"13\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhich HTTP responses would you monitor and for which would you trigger alerts?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eExample of API endpoints:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/this-is-an-endpoint\n/another/endpoint\n/some/other/endpoint\n/login\n/accounts\n/cart/items\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHTTP response status codes indicate whether a specific HTTP request has been successfully completed. Responses are grouped in five classes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInformational responses (100–199)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSuccessful responses (200–299)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRedirection messages (300–399)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eClient error responses (400–499)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eServer error responses (500–599)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf HTTP response is in the 400 or 500 range we'll trigger an alert by writing a script of setting up a monitoring service.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"14\"\u003e\n\u003cli\u003e\u003cstrong\u003eCan we run a Jenkins agent inside a docker container along with our test?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eRunning Jenkins agent inside a \u003ca href=\"/notes/wf37vjntme0oklsx52ycrn4\"\u003eDocker\u003c/a\u003e container is (one of) the standard way of implementing pipelines.\u003c/li\u003e\n\u003cli\u003eThis question is focusing on isolation of pipeline steps.\u003c/li\u003e\n\u003cli\u003eHere the code is expected to run inside a docker container which can use a custom test image if required.\u003c/li\u003e\n\u003cli\u003eThis is the best way to perform testing without having every \u003ca href=\"/notes/jqsu891tokqfup82wkq6j02\"\u003eJenkins\u003c/a\u003e agent needing to have packages installed.\u003c/li\u003e\n\u003cli\u003eTo use any kind of Docker containers, I'd add it as a part of the agent. We can call any kind of images(from DockerHub or Custom), whatever stages and test steps we mention as part of our pipeline will run inside that docker container. Multi node Jenkins setup.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://res.cloudinary.com/zubayr/image/upload/v1655971387/wiki/m9mcgppau59fyaewl0uw.png\"\u003e\u003c/p\u003e\n\u003col start=\"15\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat are some of the ways of setting up alerts?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIt is not feasible to have multiple alerting methods in one org. Alerts notifications can be Emails, Phone calls, Slack messages, etc. This ties into the \u003ca href=\"/notes/01rcytslgl5ysbgp9kzekoj\"\u003eOn Call\u003c/a\u003e management.\u003c/p\u003e\n\u003cp\u003eSome of the open source methods:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/9fa6uf6xbnv00fv944o1y1o\"\u003ePrometheus\u003c/a\u003e -\u003e Alertmanager\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/42qbhsb3tdd4z7w7ynfmjj1\"\u003eAWS CloudWatch\u003c/a\u003e -\u003e SNS\u003c/li\u003e\n\u003cli\u003eNagios -\u003e Alerting\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"16\"\u003e\n\u003cli\u003e\u003cstrong\u003eHelp repository to store/access helm charts\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHelm repositories are a common practice to store helm charts, which can be access by our \u003ca href=\"/notes/gbeh61d6hvbmxxy63chp81b\"\u003eKubernetes\u003c/a\u003e cluster as part of a deployment. This helps with versioning our helm charts, rollbacks, upgrade etc.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCloudsmith\u003c/li\u003e\n\u003cli\u003eJfrog Artifactory\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/faxbzx44mmk9u2ix1fdfuu7\"\u003eAWS S3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGoogle Cloud Storage\u003c/li\u003e\n\u003cli\u003eArtifact Hub(Open Source)\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"17\"\u003e\n\u003cli\u003e\u003cstrong\u003eJenkins multi-node setup, how to add new slave/follower to master?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHaving only one node is usually not enough(availability) for any organization so a multi-node setup is required for scaling. You can add a slave/follower for a master on \"Manage \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003edevlog.Jekins (Private)\u003c/a\u003e\" page and the option \"Manage Nodes and Clouds\". You will provide node info such as the IP Address, username, password etc and get it registered. We can also add slaves dynamically, you'd need an auto scaling group by your Cloud Provider.\u003c/p\u003e\n\u003col start=\"18\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow do you block an IAM user from accessing a specific S3 bucket?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIt is difficult to manage bucket level policies using IAM policy. We can achieve this using \u003ca href=\"/notes/faxbzx44mmk9u2ix1fdfuu7\"\u003eAWS S3\u003c/a\u003e bucket policy, you'd use the IAM user's ARN and “deny” the user.\u003c/p\u003e\n\u003col start=\"19\"\u003e\n\u003cli\u003e\u003cstrong\u003eIs a large docker image a cause of concern? How would you tackle it?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eApplications can be large if they're complex and do a lot of things but if it is a simple application...large size is not warranted and can be mitigated.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBigger docker image would result in longer build time.\u003c/li\u003e\n\u003cli\u003eDocker image downloaded(from DockerHub) may throw errors or cause API rate limit issues.\u003c/li\u003e\n\u003cli\u003eApplication will be bulkier and harder to debug and scale.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo resolve this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSmaller Base Image(Alpine images).\u003c/li\u003e\n\u003cli\u003eIntroduce Multi-stage build - from the Base image you build up your image and discard your previous image builds.\u003c/li\u003e\n\u003cli\u003eRemove package binaries after installing and don't install packages that are not necessary.\u003c/li\u003e\n\u003cli\u003eLock your package/dependencies versions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"20\"\u003e\n\u003cli\u003e\u003cstrong\u003eAre you aware of AWS IAM policies/can you read them?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePolicy evaluation logic\u003c/p\u003e\n\u003cp\u003eExample\u003c/p\u003e\n\u003cpre class=\"language-json\"\u003e\u003ccode class=\"language-json\"\u003e\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003e\n  \u003cspan class=\"token property\"\u003e\"Version\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"2012-10-17\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token comment\"\u003e// version is fixed\u003c/span\u003e\n  \u003cspan class=\"token property\"\u003e\"Statement\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Sid\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"AllowS3ListRead\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token comment\"\u003e// high level ALLOW action\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Effect\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"Allow\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Action\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\n        \u003cspan class=\"token string\"\u003e\"s3:GetBucketLocation\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token comment\"\u003e// explicitly defining what actions are allowed\u003c/span\u003e\n        \u003cspan class=\"token string\"\u003e\"s3:GetAccountPublicAccessBlock\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"token string\"\u003e\"s3:ListAccessPoints\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"token string\"\u003e\"s3:ListAllMyBuckets\"\u003c/span\u003e\n      \u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Resource\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"arn:aws:s3:::*\"\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Sid\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"AllowS3Self\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token comment\"\u003e// high level ALLOW action\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Effect\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"Allow\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Action\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"s3:*\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token comment\"\u003e// everything is allowed but only limited to the below two buckets\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Resource\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"arn:aws:s3:::carlossalazar/*\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"arn:aws:s3:::carlossalazar\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Sid\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"DenyS3Logs\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token comment\"\u003e// high level DENY action\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Effect\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"Deny\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Action\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"s3:*\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token comment\"\u003e// everything is denied for any bucket with the suffix \"logs\"\u003c/span\u003e\n      \u003cspan class=\"token property\"\u003e\"Resource\"\u003c/span\u003e\u003cspan class=\"token operator\"\u003e:\u003c/span\u003e \u003cspan class=\"token string\"\u003e\"arn:aws:s3:::*log*\"\u003c/span\u003e\n    \u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\n  \u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExample via - \u003ca href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\"\u003ePolicy evaluation logic - AWS Identity and Access Management\u003c/a\u003e\u003c/p\u003e\n\u003col start=\"21\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat role does \u003ccode\u003epv\u003c/code\u003e and \u003ccode\u003epvc\u003c/code\u003e play in Kubernetes?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePV stands for PersistentVolume\nPVC stands for PersistentVolumeClaim\u003c/p\u003e\n\u003cp\u003ePod gets it's storage using PVC which would in turn get hold of PV which will utilize an NFS or \u003ca href=\"/notes/fsmsti0qns6uixhjwtqk7p6\"\u003eAWS EBS\u003c/a\u003e volume.\u003c/p\u003e\n\u003cp\u003ePVC will define what kind of volume and the storage amount it needs and it will search of PVs(which you'd have provisioned) and choose from those.\u003c/p\u003e\n\u003cp\u003ePV can exist independently from a Pod, you don't need to have a Pod for a PV to be created.\u003c/p\u003e\n\u003col start=\"22\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhen creating RDS using Terraform, how do you save DB username and password securely?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSince DB username and password are considered secrets, they cannot be saved in plaintext on a repository along with the Terraform code.\u003c/p\u003e\n\u003cp\u003eYou can use Hashicorp's Vault, store your secrets on your Vault.\u003c/p\u003e\n\u003cp\u003eIntegrate it with Terraform. In your \u003ccode\u003emain.tf\u003c/code\u003e you can reference it as a Data Block. Mention Vault provider in the Provider section.\u003c/p\u003e\n\u003cp\u003eYou can also use other secret stores/managers like \u003ca href=\"/notes/3rpt3gh5gtw0qr6qs8f4qkq\"\u003eAWS Secrets Manager\u003c/a\u003e etc. Or you can also use environment variables.\u003c/p\u003e\n\u003col start=\"23\"\u003e\n\u003cli\u003e\u003cstrong\u003eHave you support any DBs?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou don't need to be a DB expert but you have to have clear understanding of different kinds of DBs used, their use cases and pick one based on your experience and profile.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKey-value DB\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/1kiomlco81afa6al0dp14lh\"\u003eRedis\u003c/a\u003e, etcd\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eColumn DB\n\u003cul\u003e\n\u003cli\u003eCassandra\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNoSQL/Document/Schemaless DB\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/lh0h2j2j55l3xcy3v5np9m2\"\u003emongoDB\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/tczgjqm691xi6olmcy78ckc\"\u003eAWS DynamoDB\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRelational DB\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/ypszfixe0p3s5k0inqs5g08\"\u003eMySql\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/dxfqif7isp6w1p8xoohln8b\"\u003eAWS Aurora\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/l5a3o2n7ud6bypkz8jh688u\"\u003ePostgreSQL\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"24\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow do you ensure certain packages are installed on all your EC2 instances and are persisted?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHashiCorp's \u003ca href=\"/notes/kxbhnz5oxpmort5y94uvqc9\"\u003ePacker\u003c/a\u003e is the solution. Packer is an open source tool that enables you to create identical machine images for multiple platforms from a single source template which can be written in JSON. You can use it in your multi-cloud setup or On-prem infra too.\u003c/p\u003e\n\u003cp\u003eA common use case is creating \"Golden Images\" that teams across an organization can use in cloud infrastructure.\u003c/p\u003e\n\u003cp\u003eWe could also use custom \u003ca href=\"/notes/38n6krgljsgboencwqjmb5j\"\u003eAWS AMI\u003c/a\u003e images too but is limited to Cloud.\u003c/p\u003e\n\u003col start=\"25\"\u003e\n\u003cli\u003e\u003cstrong\u003eDifferentiate Managed, Customer Managed and Inline IAM policies.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eManaged: Created and maintained by AWS.\u003c/li\u003e\n\u003cli\u003eCustomer Managed: Created and maintained by customer. Custom policies created by an organization using \u003ca href=\"/notes/mkwoo3ek4y6bnddpy1iteb9\"\u003eTerraform\u003c/a\u003e. It can be attached to multiple users or groups.\u003c/li\u003e\n\u003cli\u003eInline: Created and attached directly to IAM User, Group or Role. It is created for that specific Role, User, Group ...if the Role, User or Group is destroyed, the policy is also deleted.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"26\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat build tools are you familiar with?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003eBuild Automation\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/qcaw5ht4vcnucydq104gh7v\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003e→\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003ch2 id=\"building-java-applications\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#building-java-applications\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eBuilding Java Applications\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstall IntelliJ IDEA.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall Java(or use IDEA to Download SDK).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSetup JDK, SDK (make sure Java executable is added to \u003ca href=\"/notes/iicnw4qgra6rr5f6k4dlw3z\"\u003e$PATH\u003c/a\u003e or \u003ccode\u003e%PATH%\u003c/code\u003e, respectively.)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSet \u003ccode\u003eJAVA_HOME\u003c/code\u003e as an \u003ca href=\"/notes/o729gsazu87le197nswwxtp\"\u003eenvironment variable\u003c/a\u003e (Maven prerequisite).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse SCM to clone(\u003ccode\u003egit clone\u003c/code\u003e) your Java application's repository.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild a \u003ca href=\"/notes/482tfc3v73o3d4p8yi3b4z7\"\u003eMaven\u003c/a\u003e project.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen source code of your Java application in IntelliJ IDEA.\u003c/li\u003e\n\u003cli\u003eWait for IDEA to index the source code.\u003c/li\u003e\n\u003cli\u003eIDEA will automatically detect the \u003ccode\u003epom.xml\u003c/code\u003e and resolve all the dependencies.\u003c/li\u003e\n\u003cli\u003eRun the application(preview in the \"Run\" tab).\u003c/li\u003e\n\u003cli\u003eDownload \u003ca href=\"https://maven.apache.org\"\u003eMaven\u003c/a\u003e and add it's \u003ccode\u003e/bin\u003c/code\u003e's path to \u003ccode\u003e$PATH\u003c/code\u003e or \u003ccode\u003e%PATH%\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eUse \u003ccode\u003emvn\u003c/code\u003e commands and profit!\u003c/li\u003e\n\u003cli\u003eAfter building, \u003ccode\u003e.jar\u003c/code\u003e or \u003ccode\u003e.war\u003c/code\u003e files can be found in the \u003ccode\u003e./target\u003c/code\u003e folder.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild a \u003ca href=\"/notes/saz485w50o682scob9knhp1\"\u003eGradle\u003c/a\u003e project.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen source code of your Java application in IntelliJ IDEA\u003c/li\u003e\n\u003cli\u003eIf it has a gradle wrapper(folder) inside the repo, you don't need to install gradle\u003c/li\u003e\n\u003cli\u003eMake sure the JVM configured is compatible with gradle wrapper's version.(\u003ccode\u003eJAVA_HOME\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eRun the application to check if everything is working fine\u003c/li\u003e\n\u003cli\u003eBuild with \u003ccode\u003e./gradlew build\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eAfter building, \u003ccode\u003e.jar\u003c/code\u003e or \u003ccode\u003e.war\u003c/code\u003e files can be found in the \u003ccode\u003e./build\u003c/code\u003e folder.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo run a \u003ccode\u003e.jar\u003c/code\u003e: \u003ccode\u003ejava -jar name-of-the-app-SNAPSHOT.jar\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"managing-java-dependencies\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#managing-java-dependencies\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eManaging Java dependencies\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eBuild tools(Maven, Gradle, NPM) are required even for local development of the application.\u003c/li\u003e\n\u003cli\u003eDependencies file is what keeps track of all the dependencies used in the application. Refer to \u003ccode\u003epom.xml\u003c/code\u003e for a Maven project and \u003ccode\u003ebuild.gradle\u003c/code\u003e for a Gradle project.\u003c/li\u003e\n\u003cli\u003eAll the dependencies for both kind of projects are fetched from \u003ca href=\"https://mvnrepository.com\"\u003emvnrepository.com\u003c/a\u003e, they're downloaded locally from the remote repository.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"building-javascript--devlognodejs-private-project\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#building-javascript--devlognodejs-private-project\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eBuilding \u003ca href=\"/notes/cxjsfosx0onyz5nt07qna7w\"\u003eJavascript\u003c/a\u003e / \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003edevlog.nodejs (Private)\u003c/a\u003e project.\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstall Node.js, run \u003ccode\u003enpm\u003c/code\u003e to check if everything is working correctly.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAlternatively to \u003ccode\u003enpm\u003c/code\u003e you can also use \u003ccode\u003eyarn\u003c/code\u003e to manage dependencies and to run build commands.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse the build commands from \u003ccode\u003epackage.json\u003c/code\u003e to build it.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUnlike Java applications, after building a JavaScript will result in \u003ccode\u003e.zip\u003c/code\u003e or \u003ccode\u003e.tgz\u003c/code\u003e file.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBoth \u003ccode\u003enpm\u003c/code\u003e and \u003ccode\u003eyarn\u003c/code\u003e are \u003cstrong\u003epackage managers not build tools\u003c/strong\u003e, they're used for managing dependencies not for transpiling JS code.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse \u003ccode\u003enpm pack\u003c/code\u003e to pack.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe resulting zip or tar doesn't contain dependencies; only the application code.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo run the application, you first need to install the dependencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUnpack the zip/tar.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun the app.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePackage frontend JS code.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSeparate \u003ccode\u003epackage.json\u003c/code\u003e for frontend and backend.\u003c/li\u003e\n\u003cli\u003eor have a common \u003ccode\u003epackage.json\u003c/code\u003e for both frontend and backend.\u003c/li\u003e\n\u003cli\u003eReact code needs to be transpiled since it uses \u003ccode\u003ejsx\u003c/code\u003e syntax.\u003c/li\u003e\n\u003cli\u003eCompress and minify.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild tools in the JavaScript world as also known as Bundlers.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMost popular bundler is \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003edevlog.webpack (Private)\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWebpack will transpile, minify, bundles, compresses(removes whitespaces etc).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003col start=\"26\"\u003e\n\u003cli\u003e\u003cstrong\u003eIngress and Egress\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003earchive.ingress (Private)\u003c/a\u003e and \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003earchive.egress (Private)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIn IT they refer to:\u003c/p\u003e\n\u003cp\u003eIngress: Incoming/Inbound traffic\nEgress: Outgoing/Outbound traffic\u003c/p\u003e\n\u003cp\u003eThey're mostly associated with security groups(\u003ca href=\"/notes/xqc8lgkkzwz4qcbb6htkgfv\"\u003eFirewall\u003c/a\u003e). They're also associated with VPCs and subnets where we control the incoming traffic from the internet and routing the traffic between subnets.\u003c/p\u003e\n\u003cp\u003eEg: Allowing or denying ports for certain traffic(\u003ca href=\"/notes/o922t6d9q6wur3yve290hir\"\u003eSSH\u003c/a\u003e, \u003ca href=\"/notes/9j4ykuha11rmbi2q6e3fmm3\"\u003eHTTP\u003c/a\u003e). In prod, you'd usually lock it in the VPC IP Range.\u003c/p\u003e\n\u003col start=\"27\"\u003e\n\u003cli\u003e\u003cstrong\u003eDifferentiate Docker Image and Docker Layer.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eDocker Layers are not separate components, you cannot have a single Docker Layer and use it. They're a subcomponent of a Docker Image.\u003c/li\u003e\n\u003cli\u003eBut an Image can consist of a single Layer(that's often the case when running \u003ccode\u003esquash\u003c/code\u003e command)\u003c/li\u003e\n\u003cli\u003eEach Layer is an Image by itself.\u003c/li\u003e\n\u003cli\u003eThey're generated when you run \u003ccode\u003edocker container\u003c/code\u003e commands.\u003c/li\u003e\n\u003cli\u003eEach Layer stores the changes compared to the image it's based on \u003ccode\u003edocker history\u003c/code\u003e can fetch you info on it's Layers.\u003c/li\u003e\n\u003cli\u003eEach instruction in a Dockerfile results in a layer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"28\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat is bastion host or gateway server and what roles do they play?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003eBastion Host\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/9zyk7wwcwwwx1bi9pxy7e82\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003e→\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003cp\u003eA Bastion Host/Server is used to manage access to an internal or private network from an external network. It is also known as a Gateway Server or a Jump Box or a Jump Server.\u003c/p\u003e\n\u003cp\u003eIt basically helps with security - monitoring incoming and outgoing traffic. Bastion Host has clear user rules defined; who can access what.\u003c/p\u003e\n\u003cp\u003eUser will first sign into Bastion Host and get validated and proceed with SSHing into other machines. If we want to cut off all the external access to our internal servers, all we'd have to do is destroy the Bastion Host.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003col start=\"29\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow do you troubleshoot an Auto Scaling Group that is facing issues provisioning new nodes(it is using spot instances)?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHaving an \u003ca href=\"/notes/520xdfstn930e413i3byvxs\"\u003eAWS Auto Scaling Group\u003c/a\u003e full of only spot instances is not a good idea unless the application can bear a little bit of downtime or have a back up ASG.\u003c/p\u003e\n\u003cp\u003eThere could be two of many causes for this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIncrease in spot price\u003c/li\u003e\n\u003cli\u003eEC2 quota limit\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSpot instances an be taken away from you if there is a change in bid price. Bidding high doesn't guarantee yous spot instances.\u003c/p\u003e\n\u003cp\u003eThere is a soft limit set of every account on how many EC2 instances you can spin up. It can be mitigated by going to AWS Support and raise a ticket for increasing quota limit.\u003c/p\u003e\n\u003col start=\"30\"\u003e\n\u003cli\u003e\u003cstrong\u003eTroubleshoot a Pod that is unable to access a volume due to access error.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eVolumes are handled/provisioned in Kubernetes as a part of PVs.\u003c/p\u003e\n\u003cp\u003eAccessMode of volume - see that your PVC or your volume supports \u003ccode\u003eReadWriteMany\u003c/code\u003e permission for multi-pod access.\u003c/p\u003e\n\u003cp\u003eThe access modes are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eReadWriteOnce\u003c/code\u003e the volume can be mounted as read-write by a single node. ReadWriteOnce access mode can still allow multiple pods to access the volume when the pods are running on the same node.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eReadOnlyMany\u003c/code\u003e the volume can be mounted as read-only by many nodes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eReadWriteMany\u003c/code\u003e the volume can be mounted as read-write by many nodes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eReadWriteOncePod\u003c/code\u003e the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI volumes and Kubernetes version 1.22+.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"/notes/i6etf4qol4i62as5pcjfphu\"\u003eNFS\u003c/a\u003e allows \u003ccode\u003eReadWriteMany\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"/notes/fsmsti0qns6uixhjwtqk7p6\"\u003eAWS EBS\u003c/a\u003e only allows \u003ccode\u003eReadWriteOnce\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"31\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow to setup K8s in AWS?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou can set them up directly on \u003ca href=\"/notes/1h5mmmv2b68di6siwa31jvh\"\u003eAWS EC2\u003c/a\u003e. Using either \u003ccode\u003ekops\u003c/code\u003e commands or \u003ccode\u003ekubeadmin\u003c/code\u003e. Or you can go with \u003ca href=\"/notes/ervusa6qabwyx7sbb1a4vcg\"\u003eAWS EKS\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIf you want to set it up locally you'd go for \u003ccode\u003eminicube\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIn prod we look for stability of our environment, hence the safest bet would be to go with a managed service such as EKS. For development you can go with \u003ca href=\"https://github.com/kubernetes/kops\"\u003ekOps\u003c/a\u003e clusters.\u003c/p\u003e\n\u003cp\u003eCode deployment should be taken care by \u003ca href=\"/notes/5uu4351w46y16xtyufz0fy4\"\u003eHelm\u003c/a\u003e.\u003c/p\u003e\n\u003col start=\"32\"\u003e\n\u003cli\u003e\u003cstrong\u003eExplain Load Balancers in AWS.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003eLoad Balancer\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/hjm4wdsz5kubm8h8pm006dd\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003e→\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003cp\u003eWhat is a Load Balancer? A load balancer acts as the “traffic cop” sitting in front of your servers and routing client requests across all servers capable of fulfilling those requests in a manner that maximizes speed and capacity utilization and ensures that no one server is overworked, which could degrade performance.\u003c/p\u003e\n\u003cp\u003eIf a single server goes down, the load balancer redirects traffic to the remaining online servers. When a new server is added to the server group, the load balancer automatically starts to send requests to it.\u003c/p\u003e\n\u003ch2 id=\"types-of-load-balancers--based-on-functions\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#types-of-load-balancers--based-on-functions\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eTypes of Load Balancers – Based on Functions\u003c/h2\u003e\n\u003cp\u003eSeveral load balancing techniques are there for addressing the specific network issues:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNetwork Load Balancer / Layer 4 (L4) Load Balancer:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBased on the network variables like \u003ca href=\"/notes/mj7tlpbhhzfpg8pbegkkasb\"\u003eIP Address\u003c/a\u003e and destination ports, Network Load balancing is the distribution of traffic at the transport level through the routing decisions. Such load balancing is TCP i.e. level 4, and does not consider any parameter at the application level like the type of content, cookie data, headers, locations, application behavior etc. Performing network addressing translations without inspecting the content of discrete packets, Network Load Balancing cares only about the network layer information and directs the traffic on this basis only.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eApplication Load Balancer / Layer 7 (L7) Load Balancer:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRanking highest in the \u003ca href=\"/notes/mmu3q72ghud78jtb8fv44ex\"\u003eOSI Model\u003c/a\u003e, Layer 7 load balancer distributes the requests based on multiple parameters at the application level. A much wider range of data is evaluated by the L7 load balancer including the HTTP headers and SSL sessions and distributes the server load based on the decision arising from a combination of several variables. This way application load balancers control the server traffic based on the individual usage and behavior.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGlobal Server Load Balancer/Multi-site Load Balancer:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWith the increasing number of applications being hosted in cloud data centers, located at varied geographies, the GSLB extends the capabilities of general L4 and L7 across various data centers facilitating the efficient global load distribution, without degrading the experience for end users. In addition to the efficient traffic balancing, multi-site load balancers also help in quick recovery and seamless business operations, in case of server disaster or disaster at any data center, as other data centers at any part of the world can be used for business continuity.\u003c/p\u003e\n\u003ch2 id=\"types-of-load-balancers--based-on-configurations\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#types-of-load-balancers--based-on-configurations\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eTypes of Load Balancers – Based on Configurations\u003c/h2\u003e\n\u003cp\u003eLoad Balancers are also classified as:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHardware Load Balancers:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAs the name suggests, this is a physical, on-premise, hardware equipment to distribute the traffic on various servers. Though they are capable of handling a huge volume of traffic but are limited in terms of flexibility, and are also fairly high in prices.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSoftware Load Balancers:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThey are the computer applications that need to be installed in the system and function similarly to the hardware load balancers. They are of two kinds- Commercial and Open Source and are a cost-effective alternative to the hardware counterparts.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVirtual Load Balancers:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis load balancer is different from both the software and hardware load balancers as it is the combination of the program of a hardware load balancer working on a virtual machine.\u003c/p\u003e\n\u003cp\u003eThrough virtualization, this kind of load balancer imitates the software driven infrastructure. The program application of hardware equipment is executed on a virtual machine to get the traffic redirected accordingly. But such load balancers have similar challenges as of the physical on-premise balancers viz. lack of central management, lesser scalability and much limited automation.\u003c/p\u003e\n\u003ch2 id=\"load-balancing-methods\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#load-balancing-methods\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eLoad Balancing Methods\u003c/h2\u003e\n\u003cp\u003eAll kinds of Load Balancers receive the balancing requests, which are processed in accordance with a pre-configured algorithm.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIndustry Standard Algorithms\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe most common load balancing methodologies include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRound Robin Algorithm:\nIt relies on a rotation system to sort the traffic when working with servers of equal value. The request is transferred to the first available server and then that server is placed at the bottom of the line.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWeighted Round Robin Algorithm:\nThis algorithm is deployed to balance loads of different servers with different characteristics.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLeast Connections Algorithm:\nIn this algorithm, traffic is directed to the server having the least traffic. This helps maintain the optimized performance, especially at peak hours by maintaining a uniform load at all the servers.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLeast Response Time Algorithm:\nThis algorithm, like the least connection one, directs traffic to the server with a lower number of active connections and also considers the server having the least response time as its top priority.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIP Hash Algorithm:\nA fairly simple balancing technique assigns the client’s IP address to a fixed server for optimal performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eVia - \u003ca href=\"https://www.appviewx.com/education-center/load-balancer-and-types/\"\u003eLoad Balancer | Types of Load Balancers | Benefits of Load balancer\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003eAWS ELB\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/uedjwc3ldf8k1p2l1l7fcxj\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003e→\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003cp\u003eElastic Load Balancing supports the following types of load balancers: Application Load Balancers, Network Load Balancers, and Classic Load Balancers. Amazon ECS services can use these types of load balancer. Application Load Balancers are used to route HTTP/HTTPS (or Layer 7) traffic. Network Load Balancers and Classic Load Balancers are used to route \u003ca href=\"/notes/ahxqeweymljwoy5u6g6hxb3\"\u003eTCP\u003c/a\u003e (or Layer 4) traffic.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eClassic Load Balancer\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUses Round Robin traffic routing\u003c/li\u003e\n\u003cli\u003eWill be deprecated from Aug 2022\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eApplication Load Balancer\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePath based routing - if you have path \u003ccode\u003e/home\u003c/code\u003e or \u003ccode\u003e/mobile\u003c/code\u003e serving different pages from different ASGs.\u003c/li\u003e\n\u003cli\u003eMultiple ASG balancing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNetwork Load Balancer\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStreaming service(packet transmission)\u003c/li\u003e\n\u003cli\u003eUses network layer (TCP protocol) (makes this LB faster)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eGateway Load Balancer\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGWLB Target groups support the Generic Networking Virtualization Encapsulation(\u003cstrong\u003eGENEVE\u003c/strong\u003e) on port: 6081.\u003c/li\u003e\n\u003cli\u003eRuns within one Availability Zone.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003col start=\"33\"\u003e\n\u003cli\u003e\u003cstrong\u003eHave you used sonarqube?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cdiv class=\"portal-container\"\u003e\n\u003cdiv class=\"portal-head\"\u003e\n\u003cdiv class=\"portal-backlink\"\u003e\n\u003cdiv class=\"portal-title\"\u003eFrom \u003cspan class=\"portal-text-title\"\u003eSonarQube\u003c/span\u003e\u003c/div\u003e\n\u003ca href=\"/notes/llkndm159erk2w16us6q24k\" class=\"portal-arrow\"\u003eGo to text \u003cspan class=\"right-arrow\"\u003e→\u003c/span\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\"\u003e\n\u003cdiv class=\"portal-parent-fader-top\"\u003e\u003c/div\u003e\n\u003cdiv class=\"portal-parent-fader-bottom\"\u003e\u003c/div\u003e\u003cp\u003eSonarQube® is an automatic code review tool to detect bugs, vulnerabilities, and code smells in your code. It can integrate with your existing workflow to enable continuous code inspection across your project branches and pull requests.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.sonarlint.org/\"\u003eSonarLint\u003c/a\u003e – SonarLint is a companion product that works in your editor giving immediate feedback so you can catch and fix issues before they get to the repository.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.sonarqube.org/latest/user-guide/quality-gates/\"\u003eQuality Gate\u003c/a\u003e – The Quality Gate lets you know if your project is ready for production.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.sonarqube.org/latest/user-guide/clean-as-you-code/\"\u003eClean as You Code\u003c/a\u003e – Clean as You Code is an approach to code quality that eliminates a lot of the challenges that come with traditional approaches. As a developer, you focus on maintaining high standards and taking responsibility specifically in the New Code you're working on.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.sonarqube.org/latest/user-guide/issues/\"\u003eIssues\u003c/a\u003e – SonarQube raises issues whenever a piece of your code breaks a coding rule, whether it's an error that will break your code (bug), a point in your code open to attack (vulnerability), or a maintainability issue (code smell).\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.sonarqube.org/latest/user-guide/security-hotspots/\"\u003eSecurity Hotspots\u003c/a\u003e – SonarQube highlights security-sensitive pieces of code that need to be reviewed. Upon review, you'll either find there is no threat or you need to apply a fix to secure the code.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eVia - \u003ca href=\"https://docs.sonarqube.org/latest/\"\u003eSonarQube Docs\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\u003col start=\"34\"\u003e\n\u003cli\u003e\u003cstrong\u003eBest practices for Incident Management.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eApplications expose metrics, metrics are collected using monitoring system, we have alert rules to trigger a phone call, Slack notification or email to the \u003ca href=\"/notes/01rcytslgl5ysbgp9kzekoj\"\u003eOn Call\u003c/a\u003e engineer.\u003c/p\u003e\n\u003cp\u003eThe organization should have a proper:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMonitoring system\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/42qbhsb3tdd4z7w7ynfmjj1\"\u003eAWS CloudWatch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/9fa6uf6xbnv00fv944o1y1o\"\u003ePrometheus\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eAlerting system\n\u003cul\u003e\n\u003cli\u003eSNS\u003c/li\u003e\n\u003cli\u003eAlertManager\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePostmortem: Understand what went wrong and how to mitigate in the future.\u003c/p\u003e\n\u003col start=\"35\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow to validate variables during terraform plan time, for example format of the variable?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn \u003ca href=\"/notes/mkwoo3ek4y6bnddpy1iteb9\"\u003eTerraform\u003c/a\u003e you can define a \u003ccode\u003evalidation\u003c/code\u003e block and specify a condition for the variable.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://res.cloudinary.com/zubayr/image/upload/v1656067537/wiki/heqtzp8ccvxdlfsaqsbx.png\"\u003e\u003c/p\u003e\n\u003cp\u003eScenario: String may not contain a /.\u003c/p\u003e\n\u003cpre class=\"language-json\"\u003e\u003ccode class=\"language-json\"\u003evariable \u003cspan class=\"token string\"\u003e\"string_may_not_contain\"\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003e\n  type = string\n  default = \u003cspan class=\"token string\"\u003e\"test\"\u003c/span\u003e\n\n  validation \u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003e\n    error_message = \u003cspan class=\"token string\"\u003e\"Value cannot contain a \\\"/\\\".\"\u003c/span\u003e\n    condition = !can(regex(\u003cspan class=\"token string\"\u003e\"/\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e var.string_may_not_contain))\n  \u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\n\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExample via - \u003ca href=\"https://dev.to/drewmullen/terraform-variable-validation-with-samples-1ank\"\u003eTerraform: Variable validation with samples\u003c/a\u003e\u003c/p\u003e\n\u003col start=\"36\"\u003e\n\u003cli\u003e\u003cstrong\u003eExplain/Differentiate \u003ccode\u003eCMD\u003c/code\u003e and \u003ccode\u003eENTRYPOINT\u003c/code\u003e in Docker.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe CMD command​ specifies the instruction that is to be executed when a Docker container starts. This CMD command is not really necessary for the container to work, as the echo command can be called in a RUN statement as well. The main purpose of the CMD command is to launch the software required in a container.\u003c/p\u003e\n\u003cp\u003eCMD commands are ignored by Daemon when there are parameters stated within the docker run command.\u003c/p\u003e\n\u003cp\u003eCMD. Sets default parameters that can be overridden from the Docker Command Line Interface (CLI) when a container is running.\u003c/p\u003e\n\u003cp\u003eYou can pass input from CMD to ENTRYPOINT.\u003c/p\u003e\n\u003cpre class=\"language-bash\"\u003e\u003ccode class=\"language-bash\"\u003e\u003cspan class=\"token comment\"\u003e# CMD\u003c/span\u003e\nFROM ubuntu:latest\nCMD\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"token string\"\u003e\"Hello World!\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eENTRYPOINT\u003c/p\u003e\n\u003cp\u003eIt is a directive or instruction that is used to specify the executable which should run when a container is started from a Docker image. It has two forms, the first one is the ‘exec’ form and the second one is the ‘shell’ form. If there is no entrypoint or CMD specified in the Docker image, it starts and exits at the same time that means container stops automatically so, we must have to specify entrypoint or CMD so that when we will start the container it should execute or it'll stop.\u003c/p\u003e\n\u003cp\u003eWe can override the ENTRYPOINT instruction while starting the container using the ‘–entrypoint’ flag. Also if we have multiple ENTRYPOINT instructions mentioned in Dockerfile then the last ENTRYPOINT will have an effect.\u003c/p\u003e\n\u003cp\u003eYou can run shell scripts using ENTRYPOINT and pass it's output to CMD\u003c/p\u003e\n\u003cpre class=\"language-bash\"\u003e\u003ccode class=\"language-bash\"\u003eFROM ubuntu\nRUN \u003cspan class=\"token function\"\u003eapt-get\u003c/span\u003e update \u003cspan class=\"token operator\"\u003e\u0026#x26;\u0026#x26;\u003c/span\u003e \u003cspan class=\"token function\"\u003eapt-get\u003c/span\u003e \u003cspan class=\"token function\"\u003einstall\u003c/span\u003e -y nginx\nENTRYPOINT \u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"nginx\"\u003c/span\u003e, \u003cspan class=\"token string\"\u003e\"-g\"\u003c/span\u003e, \u003cspan class=\"token string\"\u003e\"daemon off;\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBoth run during docker container runtime. Using either one of them is best practice but they can be combined too.\u003c/p\u003e\n\u003col start=\"37\"\u003e\n\u003cli\u003e\u003cstrong\u003eTroubleshoot EC2 instance in an ASG that are getting terminated.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf EC2 quota and pricing is not the issue:\u003c/p\u003e\n\u003cp\u003eEC2 instances get terminated if they're unhealthy.EC2 instances can become unhealthy if:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDisk space being full.\u003c/li\u003e\n\u003cli\u003eHigh CPU usage.\u003c/li\u003e\n\u003cli\u003eNo memory left.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo debug\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRun \u003ca href=\"/notes/laeri4eob3q0wi2i0d71atn\"\u003etop\u003c/a\u003e command to see CPU utilization, check what process/application is using up the resources. Take this up with your developer.\u003c/li\u003e\n\u003cli\u003eDisk space EBS volume could be full and and OS might be running out of all disk space.\u003c/li\u003e\n\u003cli\u003eRun \u003ca href=\"/notes/hatou0atny4e6tj2yapy1zc\"\u003eFree\u003c/a\u003e command to check if any swap memory is left or not, you might want to increase it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee: \u003ca href=\"/notes/zbxml2z2zzzsjj8eiagfzex\"\u003eCreate and Use Swap File on Linux\u003c/a\u003e\u003c/p\u003e\n\u003col start=\"38\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow to control of deployment of pods on nodes that are going to be used explicitly for those pods?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eWe can achieve this by using \u003cstrong\u003eTaints and Tolerance\u003c/strong\u003e in K8's.\u003c/li\u003e\n\u003cli\u003eA taint when attached to a node, will ripple pods from getting provisioned or getting accepted.\u003c/li\u003e\n\u003cli\u003eWe can add a taint to a pod \u003ccode\u003ekubectl taint nodes nodex key=value:Effect\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTaints are properties of nodes that push pods away if they don't tolerate this taint.\u003c/li\u003e\n\u003cli\u003eLike labels, one or more Taints can be applied to a node. The node must not accept any pod that doesn't tolerate all of these taints.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"40\"\u003e\n\u003cli\u003e\u003cstrong\u003eYou've 2 different servers with different ports and usernames, how do you use ansible runbook/playbook on both of the servers?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn our \u003ccode\u003eansible.conf\u003c/code\u003e we can define different \u003ccode\u003eansible_user\u003c/code\u003e and \u003ccode\u003eansible_port\u003c/code\u003e in the hostfile.\u003c/p\u003e\n\u003cpre class=\"language-conf\"\u003e\u003ccode class=\"language-conf\"\u003e[webservers]\n10.4.20.90 ansible_port=4000 ansible_user=roger\n39.12.3.23 ansible_port=8001 ansible_user=liam\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"41\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat is Terraform state lock?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf supported by your backend, \u003ca href=\"/notes/mkwoo3ek4y6bnddpy1iteb9\"\u003eTerraform\u003c/a\u003e will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state. State locking happens automatically on all operations that could write state.\u003c/p\u003e\n\u003cp\u003eYou won't see any logs of when/as this happens. If state locking fails, Terraform will continue.\u003c/p\u003e\n\u003cp\u003eYou can disable state locking for most commands with the \u003ccode\u003e-lock\u003c/code\u003e flag but it is not recommended. If acquiring the lock is taking longer than expected, Terraform will output a status message. If Terraform doesn't output a message, state locking is still occurring if your backend supports it.\u003c/p\u003e\n\u003col start=\"42\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow do you setup slack notifications on Jekins?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou can leverage Jenkins plugins, you can use the Jenkins - Slack plugin and use Slack webhook URL.\nWhenever a pipeline job fails:\n`slackSend color: \"failure\", message: \"Pipeline failed, critical.\"\u003c/p\u003e\n\u003col start=\"43\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow do you see your trajectory as a DevOps Engineer?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThere is always something to learn. You could mention some of the tools/technologies that you want to learn - some you want to better understand. Show them that you are someone who is looking to constantly improve himself/herself(themselves).\u003c/p\u003e\n\u003cp\u003eYou'd want to collaborate with your senior engineers to learn from them directly. Maybe you want to take up more responsibility in the organization etc.\u003c/p\u003e\n\u003col start=\"44\"\u003e\n\u003cli\u003e\u003cstrong\u003eHow to ignore a certain part of our ansible playbook that might fail and cause our playbook to exit?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWe can achieve this using \u003ccode\u003eignore_errors\u003c/code\u003e sub arguments.\u003c/p\u003e\n\u003cpre class=\"language-yaml\"\u003e\u003ccode class=\"language-yaml\"\u003e\u003cspan class=\"token punctuation\"\u003e-\u003c/span\u003e \u003cspan class=\"token key atrule\"\u003ename\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e Do not count this as a failure\n  \u003cspan class=\"token key atrule\"\u003eansible.builtin.command\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e /bin/false\n    \u003cspan class=\"token key atrule\"\u003eignore_errors\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e yes\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"45\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhen making a change to existing code what is a \u003ccode\u003egit\u003c/code\u003e best practice?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eGit clone the repo, checkout the respective branch. Make your change and commit your changes. To understand the motivation behind the change and the history, you can use \u003ccode\u003egit blame\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe git blame command is used to examine the contents of a file line by line and see when each line was last modified and who the author of the modifications was. Basically gives you the history and the author of the file.\u003c/p\u003e\n\u003col start=\"46\"\u003e\n\u003cli\u003e\u003cstrong\u003eAt a high level, create an shell script to automatically push certain logs to S3 at a particular time.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis is a Whiteboard design question.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirstly, we'd make sure what we're using, IAM role or access keys.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre class=\"language-bash\"\u003e\u003ccode class=\"language-bash\"\u003e\u003cspan class=\"token shebang important\"\u003e#!/bin/bash\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# assign path-to-logs to a variable\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# use aws CLI commands to copy files to S3\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# aws s3 cp \u0026#x3C;path-to-logs\u003e \u0026#x3C;s3-bucket\u003e\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# use cron job to execute the script at a particular time\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"47\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat is a package in Python?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe module is a simple Python file that contains collections of functions and global variables and with having a .py extension file. It is an executable file.To organize modules we have \u003cstrong\u003ePackages\u003c/strong\u003e in Python.\u003c/p\u003e\n\u003cp\u003eModules can have other modules inside them.\u003c/p\u003e\n\u003cp\u003ePackage(1) -\u003e Modules(x) -\u003e Fns(x)\u003c/p\u003e\n\u003cp\u003eExample module:\u003c/p\u003e\n\u003cpre class=\"language-py\"\u003e\u003ccode class=\"language-py\"\u003e\u003cspan class=\"token comment\"\u003e#the os module provides an operating system interface from Python\u003c/span\u003e\n\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"token comment\"\u003e#prints the name of the operating system\u003c/span\u003e\n\u003cspan class=\"token keyword\"\u003eprint\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eos\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ename\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003cspan class=\"token comment\"\u003e#prints the absolute path for the module\u003c/span\u003e\n\u003cspan class=\"token keyword\"\u003eprint\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eos\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003egetcwd\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"48\"\u003e\n\u003cli\u003e\u003cstrong\u003eWhat are your day to day responsibilities as a DevOps engineer?\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePaint a brief picture of what your day to day work looks like.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA DevOps engineer works in collaboration other engineers(devs, testers, SRE, sales).\u003c/li\u003e\n\u003cli\u003e9:00 AM -\u003e 10:00 AM check Slack, update \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003edevlog.Jira (Private)\u003c/a\u003e with your tasks.\u003c/li\u003e\n\u003cli\u003e10:00 AM -\u003e 10:30 AM daily standup, share progress with your team on the work you've done previous day and what you'll do today.\u003c/li\u003e\n\u003cli\u003e10:30 AM -\u003e 1:00 PM work on your Jira ticket.\u003c/li\u003e\n\u003cli\u003e1:00 PM -\u003e 2:00 PM lunch.\u003c/li\u003e\n\u003cli\u003e2:00 PM -\u003e 4:00 PM pair program with other engineers or meetings, design discussions.\u003c/li\u003e\n\u003cli\u003e4:00 PM -\u003e 5:00 PM learning new stuff that can benefit the org, share knowledge with other team members.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"credits\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#credits\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.udemy.com/course/50-devops-interview-questions-answers/\"\u003e50 DevOps Interview Questions \u0026#x26; Answers - 2022 | Udemy\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/522ww47pqvt6y0yj8zparmy\"\u003eDevOps\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"3nfl4nvv516muyzozhcwrw8","title":"/root","desc":"","updated":1655559901157,"created":1637610830605,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"581715455a6f0f7a699209e8521b4acf","links":[{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"my","position":{"start":{"line":4,"column":9,"offset":37},"end":{"line":4,"column":29,"offset":57},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":20,"column":111,"offset":1051},"end":{"line":20,"column":117,"offset":1057},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":21,"column":3,"offset":1198},"end":{"line":21,"column":9,"offset":1204},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes","alias":"swipes","position":{"start":{"line":27,"column":3,"offset":1724},"end":{"line":27,"column":13,"offset":1734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.quotes","alias":"quotes","position":{"start":{"line":27,"column":48,"offset":1769},"end":{"line":27,"column":72,"offset":1793},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.quotes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.excerpts","alias":"excerpts","position":{"start":{"line":27,"column":74,"offset":1795},"end":{"line":27,"column":102,"offset":1823},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.excerpts"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.sayings","alias":"sayings","position":{"start":{"line":27,"column":104,"offset":1825},"end":{"line":27,"column":130,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.sayings"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.phrases","alias":"phrases","position":{"start":{"line":27,"column":132,"offset":1853},"end":{"line":27,"column":158,"offset":1879},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.phrases"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"resources.people","alias":"others","position":{"start":{"line":27,"column":214,"offset":1935},"end":{"line":27,"column":241,"offset":1962},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"resources.people"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"inbox.webmark","alias":"webmark","position":{"start":{"line":31,"column":235,"offset":2463},"end":{"line":31,"column":260,"offset":2488},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"inbox.webmark"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"slipbox.Ontology","alias":"slipbox.Ontology","position":{"start":{"line":55,"column":3,"offset":3735},"end":{"line":55,"column":23,"offset":3755},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"slipbox.Ontology"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"About me","position":{"start":{"line":60,"column":3,"offset":3963},"end":{"line":60,"column":29,"offset":3989},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}}],"anchors":{"welcome-to-noetic-noggin":{"type":"header","text":"Welcome to Noetic Noggin","value":"welcome-to-noetic-noggin","line":8,"column":0,"depth":1},"principles":{"type":"header","text":"Principles","value":"principles","line":18,"column":0,"depth":2},"all-notes-should-be-relative-to-me":{"type":"header","text":"All notes should be relative to me.","value":"all-notes-should-be-relative-to-me","line":20,"column":0,"depth":3},"gotta-capture-em-all":{"type":"header","text":"Gotta capture 'em all","value":"gotta-capture-em-all","line":30,"column":0,"depth":3},"dont-force-evolution":{"type":"header","text":"Don't force evolution","value":"dont-force-evolution","line":40,"column":0,"depth":3},"noise--signal":{"type":"header","text":"Noise \u0026 Signal","value":"noise--signal","line":45,"column":0,"depth":3},"why-do-any-of-this":{"type":"header","text":"Why do any of this?","value":"why-do-any-of-this","line":50,"column":0,"depth":3},"structure-of-this-wiki":{"type":"header","text":"Structure of this wiki","value":"structure-of-this-wiki","line":59,"column":0,"depth":2},"quicklinks":{"type":"header","text":"Quicklinks","value":"quicklinks","line":64,"column":0,"depth":2}},"children":["0yay2om15bsg2li2p6qgux7","05c4nnjqa92zx11ld6o0ytn","9gtn7g40cvqui0sifl1s7t5","ftbd1hknsd3ocd7jao26tn3","a1kmkdbpclaz5p6sykaw6kc","z121gkmqfo09m8r7jgnpfgn","gkqrr7xbt18xhi93dmjrwzj","ja2x4lrgejr9o9wvit0bd0d","luv39odkfibx3wdosvigwvy","vtvk3bi6o72w58oima9xzf3","yy652kvqrkfn9ipk07m40h4"],"parent":null,"data":{},"body":"\n# Welcome to Noetic Noggin\n\nThis is [[my|archive.about]] personal wiki and a commonplace book; notes by me, for me.\n\n🚧 Permanently under construction 🚧\n\n![](https://res.cloudinary.com/zubayr/image/upload/v1658499909/wiki/ajevkuyebljlxiblyst2.png)\n\nThis wiki was made possible with [dendron.so](https://dendron.so) and [obisidian.md](https://obsidian.md). Stored on [Github Repository](https://github.com/zubayrrr/dendron) and hosted on [Netlify](https://netlify.com) for free.\n\n## Principles\n\n### All notes should be relative to me.\n\n- All notes in principle are written for me; what I know about a subject, how I feel about a particular thing.\n- Opinions are fine as long as I feel strong epistemic confidence in the given opinion.\n- Don't over explain a note if it's not necessary, remember, these notes are for you and are relative to whatever knowledge you posses about the subject.\n- Read books and make an dedicated notes for them.\n- Listen podcasts but capture them inside a \"subject specific\" note or \"Map of Concept\" note or a note tagged #areas. Because making notes from podcasts can be tedious as they're not as well structured as books for consumption.(Whose merit is debatable.)\n- #areas are basically \"Map of Concept\" notes but I have recently come to the realization that its better to maintain them [Nikita Voloboev style](https://wiki.nikiv.dev/) but with heavy usage of transclusion and backlinking.\n  - \"Resources\" should be first processed and then mentioned inside the note, otherwise they should be left in inbox.\n\n### Gotta capture 'em all\n\n- Hog whatever information tickles your pickle([anything that gratifies one's intellectual curiosity](https://news.ycombinator.com/newsguidelines.html)).\n- [[swipes]] are interesting/useful bits of... [[quotes|swipes.quotes]], [[excerpts|swipes.excerpts]], [[sayings|swipes.sayings]], [[phrases|swipes.phrases]]. Essentially, ideas, opinions that are swiped off from [[others|resources.people]].\n- Make no distinction between \"your\" ideas and ideas of \"others\", because if you vibe with an idea; it's already yours.\n- But also remember \"If you've time to consume, you've time to produce.\".\n- Use [raindrop.io](https://raindrop.io) to manage your URL bookmarks.\n- If you need to bookmark a webpage or an article all together, use [MarkDownload](https://chrome.google.com/webstore/detail/markdownload-markdown-web/pcmpcfapbekmbjjkdalcgopdkipoggdi?hl=en-GB) to rip the entire page. Let's call it a [[webmark|inbox.webmark]]; it belong in the `/inbox`.\n- Similar process is employed for capturing tweets using [tweet-to-markdown](https://github.com/kbravh/tweet-to-markdown) and it also belongs in the `/inbox`.\n\n### Don't force evolution\n\n- Let your second brain evolve at it's own pace.\n- The structure should never be _too_ rigid because its meant to take form by itself.\n\n### Noise \u0026 Signal\n\n- While capturing ideas left and right is recommended, make sure you're not harming your periods of focus.\n- Have impenetrable focus periods (use Pomodoro method) where you only care about the work on hand and nothing else.\n\n### Why do any of this?\n\n- Because I can't remember everything - there's a lot of information around that interests me and there isn't enough working memory installed in me.\n- So, I make notes - to remember, to create, to meditate, to think.\n- A bodybuilder's portfolio is their body - my portfolio is my wiki.\n- Not only am I making - whatever I know - tangible by writing it down. I know exactly where to look if I ever forget something.\n- I am at the beginning of my learning adventures. When I look back at it, I will know where I came from and how my thoughts evolved over time.\n- Plus, its really fun to nerd out.\n\n## Structure of this wiki\n\n- [[slipbox.Ontology]] explains the structure of this wiki and the tags, backlinks used in it.\n- Dendron takes care of the structure and hierarchy(mostly), but I insist on using tags for backwards compatibility.\n\n## Quicklinks\n\n- [[About me|archive.about]]\n- [Github](https://github.com/zubayrrr)\n- [Twitter](https://twitter.com/zoobhalu)\n- [Blog](https://zubayrali.in)\n- [Guestbook](https://www.yourworldoftext.com/~zubayrali/)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template","insertNote":{"initialValue":"templates"}},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Noetic Noggin","description":"Personal Wiki / Digital Garden","author":"Zubayr Ali","twitter":"zoobhalu"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteUrl":"localhost:3000","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"5eybx5vd339tjnkglx8igde"},"buildId":"4oDSInv8WeDX6cVSprjyl","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>