<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Big Data</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Wiki / Digital Garden"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="zoobhalu"/><meta name="twitter:creator" content="zoobhalu"/><meta property="og:title" content="Big Data"/><meta property="og:description" content="Personal Wiki / Digital Garden"/><meta property="og:url" content="localhost:3000/notes/s5t9uscswvpo3p6gebui1ea/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="Invalid DateTime"/><meta property="article:modified_time" content="5/23/2022"/><link rel="canonical" href="localhost:3000/notes/s5t9uscswvpo3p6gebui1ea/"/><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/cc2307f6fd3a2fec.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cc2307f6fd3a2fec.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-709abf7ab5f510de.js" defer=""></script><script src="/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/_next/static/chunks/main-c4b0e551a2150d17.js" defer=""></script><script src="/_next/static/chunks/pages/_app-62c5b93605efada7.js" defer=""></script><script src="/_next/static/chunks/78-13ae6acd5ce7ca5b.js" defer=""></script><script src="/_next/static/chunks/373-2f3879190a46a3d9.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-69449972c2a725d8.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_buildManifest.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_ssgManifest.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout site-layout" style="margin-top:64px"><section class="ant-layout site-layout"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="big-data"><a aria-hidden="true" class="anchor-heading" href="#big-data"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Big Data</h1>
<ul>
<li>Areas: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">devlog.Data Engineering (Private)</a></li>
</ul>
<hr>
<p>Big Data is a term for collections of data sets so large and complex that it becomes difficult to process them using in-hand database management tools or traditional data processing applications.</p>
<h3 id="5-vs-of-big-data"><a aria-hidden="true" class="anchor-heading" href="#5-vs-of-big-data"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>5 Vs of Big Data</h3>
<ul>
<li>Volume - Storing and processing huge data sets</li>
<li>Variety - Storing and processing different types of data</li>
<li>Velocity - The speed at which data is being generated</li>
<li>Value - Finding meaning out of data, making sense of data</li>
<li>Veracity - Uncertainty and inconsistencies in the data</li>
</ul>
<h3 id="use-cases"><a aria-hidden="true" class="anchor-heading" href="#use-cases"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Use cases:</h3>
<ul>
<li>Web and E-tailing
<ul>
<li>Recommendation Engines</li>
<li>Ad Targeting</li>
<li>Search Quality</li>
</ul>
</li>
<li>Telecommunications
<ul>
<li>Abuse and Click Fraud Detection</li>
<li>Customer Churn Prevention</li>
<li>Network Performance Optimization</li>
<li>Calling Data Record(CDR) Analysis</li>
<li>Analyzing Network to Predict Failure</li>
</ul>
</li>
<li>Banks and Financial Services
<ul>
<li>Modeling True Risk</li>
<li>Threat Analysis</li>
<li>Fraud Detection</li>
<li>Trade Surveillance</li>
<li>Credit Scoring and Analysis</li>
</ul>
</li>
<li>Retail
<ul>
<li>Point of Sales Transaction Analysis</li>
<li>Customer Churn Analysis</li>
<li>Sentiment Analysis</li>
</ul>
</li>
<li>Government
<ul>
<li>Fraud Detection and Cybersecurity</li>
<li>Welfare Schemes</li>
<li>Justice</li>
</ul>
</li>
<li>Healthcare and Life Sciences
<ul>
<li>Health Information Exchange</li>
<li>Gene Sequencing</li>
<li>Serialization</li>
<li>Healthcare Service Quality Improvements</li>
<li>Drug Safety</li>
<li>See <a href="https://cwiki.apache.org/confluence/display/HADOOP2/poweredby">PoweredBy</a> for companies that are these technologies for Big Data</li>
</ul>
</li>
<li>Semi-structured data</li>
<li>Unstructured data</li>
<li>Structured data</li>
<li>Large amount of data cannot fit on a single machine that needs processing is processed using distributed machines.</li>
<li>A distributed process has access to the computational resources across a number of machines connected through a network.</li>
<li>After a certain point, it is easier to scale out to many lower <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">CPU (Private)</a> machines, than to try to scale up to a single machine with a high CPU.</li>
<li>Distributed machines also have the advanteage of easily scaling, you can just add more machines.</li>
<li>They also include fault tolerance, if one machines fails, the whole network can still go on.</li>
</ul>
<h3 id="hadoop"><a aria-hidden="true" class="anchor-heading" href="#hadoop"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a></h3>
<ul>
<li>Hadoop is a way to distribute very large files across multiple machines.</li>
<li>It uses the Hadoop Distributed File System <a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a></li>
<li>HDFS allows a user to work with large data sets</li>
<li>HDFS also duplicates blocks of data for fault tolerance</li>
<li>It also then uses <a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a></li>
<li>MapReduce allows computations on that data</li>
<li>You've a NameNode(Master) and that connects to multiple DataNode(s)(Slave)</li>
</ul>
<p>Hadoop really only comes with HDFS and MapReduce and all the other technologies are addons, developed separately by different individuals.</p>
<p>Java is highly preferable for MR</p>
<ul>
<li>Pig was invented by Yahoo, PigLatin is the language, it a scripting language that was brought about to replace SQL/Java in MR. Internally it runs on MR and converts the code to Java. Most stuff that can be done on Hive can be done on Pig.</li>
<li>Sqoop was invented by a group, to bring data from RDBMS to Hadoop you write code in MapReduce, to avoid this, they brought about sqoop, sqoop doesn't use a programming language but commands to bring in data to Hadoop. sqoop also allows you to send data back to RDBMS from Hadoop. Import and Export. Used for Datapipeline, sqoop internally runs on Java.</li>
<li>Oozie was invented by Yahoo, its like an XML file, its a scheduler, used for automating, scheduling jobs, like running queries on a specific time. Big Data tech stack may include non Big Data specific schedulers. It runs on Java internally.</li>
</ul>
<p>All of above technologies discussed are abstractions of MapReduce.</p>
<p>Big Data tech stack don't only consist of Big Data tools/technologies, MySQL for example is not a Big Data specific tool/technologies.</p>
<h3 id="hbase"><a aria-hidden="true" class="anchor-heading" href="#hbase"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a href="#HBase">HBase</a></h3>
<p>Storage part, Database for Hadoop but NoSQL, invented by Facebook, HBase shell commands, works on top of Hadoop, all the modern databases are NoSQL database.
It is different from Hive.</p>
<h3 id="mahoot"><a aria-hidden="true" class="anchor-heading" href="#mahoot"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Mahoot</h3>
<p>Framework for Data Science</p>
<h3 id="flume"><a aria-hidden="true" class="anchor-heading" href="#flume"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Flume</h3>
<p>Similar to sqoop, used in datapipelines, flume can retrieve data in realtime. Messaging queue. sqoop is only for RDBMS unlike Flume which can be used for a fetching data from many other sources.(like twitter). Flume only retrieves data to Hadoop but cannot send it back to any other targets, only incoming.</p>
<p>There are other MQs such as Kafka, Scribe.</p>
<p>Hadoop is a losely couple framework, you can swap out/in components like oozie and it would still work, unlike tightly coupled frameworks(like .NET frameworks)</p>
<p>Integrations - Hadoop can be integrated well with Spark or any other Big Data frameworks.</p>
<p>Hadoop can also be connected with non Big Data components like <a href="/notes/ypszfixe0p3s5k0inqs5g08">MySql</a> or other RDBMSes.</p>
<h3 id="hdfs"><a aria-hidden="true" class="anchor-heading" href="#hdfs"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a></h3>
<ul>
<li>HDFS will use blocks of data, with a size of 128 MB by default.</li>
<li>Each of these blocks is replicated 3 times.</li>
<li>The blocks are distributed in a way to support fault tolerance.</li>
<li>Smaller blocks provide more parallelization during processing.</li>
<li>Multiple copies of a block prevents loss of data due to a failure of a node.</li>
</ul>
<h3 id="mapreduce"><a aria-hidden="true" class="anchor-heading" href="#mapreduce"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>MapReduce</h3>
<ul>
<li>MapReduce is a way of splitting a computation task to a distributed set of files (such as HDFS).</li>
<li>It consists of a Job Tracker and multiple Task Trackers.</li>
<li>The Job Tracker sends code to run on the Task Trackers.</li>
<li>The Task Trackers allocate CPU and memory for the tasks and monitor the tasks on the worker nodes.</li>
</ul>
<h3 id="spark"><a aria-hidden="true" class="anchor-heading" href="#spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark</h3>
<ul>
<li>One of the latest and open source technologies built to quickly and easily handle Big Data.</li>
<li>It is a Framework for dealing with large data, distributing it and doing calculations across distributed network. It is written in Scala. Scala gets the Spark's latest features, Scala is written in Java so Java can also be used for Spark's lastest features.</li>
<li>Created at AMPLab at UC Berkeley, first released in Feb of 2013.</li>
<li>It is a flexible alternative to MapReduce. (doesn't necessarily replaces Hadoop but MapReduce)</li>
<li>Spark can use data stored in a variety of formats
<ul>
<li>Cassandra</li>
<li>AWS S3</li>
<li><a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a> and more</li>
</ul>
</li>
</ul>
<h3 id="apache-spark-vs-mapreduce"><a aria-hidden="true" class="anchor-heading" href="#apache-spark-vs-mapreduce"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a href="/notes/f2kecna72pmc7re3wh1ugk4">Apache Spark</a> VS <a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a></h3>
<ul>
<li>MapReduce requires files to be stored in HDFS, Spark does not!</li>
<li>Spark also can perform operations up to 100x faster than MapReduce.</li>
<li>It is not really Hadoop VS MapReduce, people sometime make it as such because MapReduce uses HDFS.</li>
<li>MapReduce writes most data to disk after each Map and Reduce operation.</li>
<li>Spark keeps most of the data in memory something like RAM after each Transformation. Spark can spill over to <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">HDD (Private)</a> if you don't have enough <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">RAM (Private)</a>.</li>
</ul>
<h3 id="spark-rdds"><a aria-hidden="true" class="anchor-heading" href="#spark-rdds"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark RDDs</h3>
<ul>
<li>At the core of Spark is the idea of a Resilient Distributed Dataset(RDD).</li>
<li>Resilient Distributed Dataset (RDD) has 4 main features:
<ul>
<li>Distrbuted Collections of Data</li>
<li>Fault-tolerant</li>
<li>Parallel operation - ability to be partitioned</li>
<li>Ability to use many data sources</li>
</ul>
</li>
</ul>
<p>At the abstract level it looks like:</p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.ewfeqm3kqzs.png"></p>
<p>At physical level:</p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.3f0c49iwkep.png"></p>
<ul>
<li><a href="/notes/2vn2g9mhmi624b83rl5se0k">RDD</a>s are immutable, lazily evaluated and cacheable</li>
<li>There are two types of Spark operations:
<ul>
<li>Transformations</li>
<li>Actions</li>
</ul>
</li>
<li>Transformations are basically a recipe to follow.</li>
<li>Actions actually perform what the recipe says to do and returns something back.</li>
<li>This behavior carries over to the syntax when coding.</li>
<li>A lot of times you will write a method call off of a <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DataFrame (Private)</a> you won't see anything as a result until you call the action.</li>
<li>This makes sense because with a large dataset, you don't want to calculate all the transformations until you are sure you want to perform them!</li>
<li>When discussing Spark syntax you'll often see RDD versus DataFrame syntax show up.</li>
<li>With the release of Spark 2.0, Spark is moving towards a DataFrame based syntax, but keep in mind that the way files are being distributed can still be thought of as RDDs(it is still happening in RDD at the pyhsical level), it is just the typed out syntax that is changing.</li>
</ul>
<h3 id="spark-dataframe-privates"><a aria-hidden="true" class="anchor-heading" href="#spark-dataframe-privates"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Spark <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">DataFrame (Private)</a>s</h3>
<ul>
<li>Spark DataFrames are also now the standard way of using Spark's Machine Learning capabilities.</li>
<li>Spark DataFrame documentation is still pretty new and can be sparse, it can be found on <a href="https://spark.apache.org/docs/latest/">spark.apache.org</a></li>
</ul>
<h3 id="python-and-spark"><a aria-hidden="true" class="anchor-heading" href="#python-and-spark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Python and Spark</h3>
<ul>
<li>Realistically Spark won't be running on a single machine, it will run on a cluster on a service, like <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">AWS (Private)</a>.</li>
<li>These cluster services will pretty much always be a <a href="/notes/owoutsv5dicylguol2odc3e">Linux</a>) based system.</li>
</ul>
<hr>
<h2 id="miscellaneous-notes"><a aria-hidden="true" class="anchor-heading" href="#miscellaneous-notes"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Miscellaneous Notes</h2>
<h3 id="scalable-cluster-computing"><a aria-hidden="true" class="anchor-heading" href="#scalable-cluster-computing"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Scalable Cluster Computing</h3>
<p>Scalable Cluster Computing is a method of combining discrete computing resources(e.g servers) to increase the performance of a single application.</p>
<p>Does not work for applications! (i.e., not all applications "scale")</p>
<h3 id="of-servers-nodes-and-daemons"><a aria-hidden="true" class="anchor-heading" href="#of-servers-nodes-and-daemons"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Of Servers, Nodes and Daemons</h3>
<h4 id="servernode"><a aria-hidden="true" class="anchor-heading" href="#servernode"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Server/Node</h4>
<p>A Cluster is a collection of servers working together on one problem.</p>
<ul>
<li>Servers come with cores, memory, storage and network</li>
<li>Servers can be called a "<strong>NODE</strong>" in a cluster of servers</li>
</ul>
<h4 id="daemon"><a aria-hidden="true" class="anchor-heading" href="#daemon"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Daemon</h4>
<p>A daemon is a program that is constantly running and responding to requests (e.g., web server)</p>
<ul>
<li>Servers can have one or multiple daemons running on them</li>
<li>Daemons can communicate with other daemons and form a network within and between servers</li>
<li>Each daemon can also be called a "<strong>NODE</strong>"</li>
</ul>
<h3 id="scalable-behaviour"><a aria-hidden="true" class="anchor-heading" href="#scalable-behaviour"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Scalable Behaviour</h3>
<p>of Scalable Systems:
The ability to apply resources (servers, VM instances) to a problem as a means to increase performance(green line) over a single server(red line).</p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.t6kfn1dy0om.png"></p>
<p>But, initially performance may lag due to overhead(the setup of our infrastructure).</p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/lz322a84xc913sor5r5c71u">Data Warehouse</a></li>
<li><a href="/notes/ffhm4kotv5wbd139ml9ch44">ETL</a></li>
<li><a href="/notes/hr5l5ukjr3mfsnsn0fkw6w5">ODS</a></li>
<li><a href="/notes/6k7g4x7ws565lditv1n8hxm">YARN</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#5-vs-of-big-data" title="5 Vs of Big Data">5 Vs of Big Data</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#use-cases" title="Use cases:">Use cases:</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#devloghadoop" title="devlog.hadoop">devlog.hadoop</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#hbase" title="HBase">HBase</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#mahoot" title="Mahoot">Mahoot</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#flume" title="Flume">Flume</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#devloghdfs" title="devlog.hdfs">devlog.hdfs</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#mapreduce" title="MapReduce">MapReduce</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark" title="Spark">Spark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#devlogapache-spark-vs-devlogmapreduce" title="devlog.apache spark VS devlog.mapreduce">devlog.apache spark VS devlog.mapreduce</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark-rdds" title="Spark RDDs">Spark RDDs</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark-dataframes" title="Spark DataFrames">Spark DataFrames</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#python-and-spark" title="Python and Spark">Python and Spark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#miscellaneous-notes" title="Miscellaneous Notes">Miscellaneous Notes</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#scalable-cluster-computing" title="Scalable Cluster Computing">Scalable Cluster Computing</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#of-servers-nodes-and-daemons" title="Of Servers, Nodes and Daemons">Of Servers, Nodes and Daemons</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#servernode" title="Server/Node">Server/Node</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#daemon" title="Daemon">Daemon</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#scalable-behaviour" title="Scalable Behaviour">Scalable Behaviour</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"s5t9uscswvpo3p6gebui1ea","title":"Big Data","desc":"","updated":1653318643709,"created":20211109142548908,"custom":{},"fname":"devlog.big data","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"cda24adf9ff453b6e0f548a083425589","links":[{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.Data Engineering","alias":"devlog.Data Engineering","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":37,"offset":37},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Data Engineering"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"CPU","alias":"CPU","position":{"start":{"line":54,"column":66,"offset":1844},"end":{"line":54,"column":73,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"CPU"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":58,"column":5,"offset":2122},"end":{"line":58,"column":22,"offset":2139},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":61,"column":46,"offset":2261},"end":{"line":61,"column":61,"offset":2276},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":64,"column":21,"offset":2405},"end":{"line":64,"column":41,"offset":2425},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.MySQL","alias":"devlog.MySQL","position":{"start":{"line":99,"column":64,"offset":4692},"end":{"line":99,"column":80,"offset":4708},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.MySQL"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":101,"column":5,"offset":4732},"end":{"line":101,"column":20,"offset":4747},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":125,"column":5,"offset":6015},"end":{"line":125,"column":20,"offset":6030},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.apache spark","alias":"devlog.apache spark","position":{"start":{"line":127,"column":5,"offset":6045},"end":{"line":127,"column":28,"offset":6068},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.apache spark"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":127,"column":32,"offset":6072},"end":{"line":127,"column":52,"offset":6092},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"HDD","alias":"HDD","position":{"start":{"line":133,"column":112,"offset":6516},"end":{"line":133,"column":119,"offset":6523},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"HDD"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"RAM","alias":"RAM","position":{"start":{"line":133,"column":145,"offset":6549},"end":{"line":133,"column":152,"offset":6556},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"RAM"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.rdd","alias":"devlog.rdd","position":{"start":{"line":152,"column":3,"offset":7085},"end":{"line":152,"column":17,"offset":7099},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.rdd"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"DataFrame","alias":"DataFrame","position":{"start":{"line":159,"column":56,"offset":7467},"end":{"line":159,"column":69,"offset":7480},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DataFrame"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"DataFrame","alias":"DataFrame","position":{"start":{"line":164,"column":11,"offset":8062},"end":{"line":164,"column":24,"offset":8075},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"DataFrame"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"AWS","alias":"AWS","position":{"start":{"line":171,"column":105,"offset":8450},"end":{"line":171,"column":112,"offset":8457},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"AWS"}},{"type":"wiki","from":{"fname":"devlog.big data","id":"s5t9uscswvpo3p6gebui1ea","vaultName":"Dendron"},"value":"devlog.linux","alias":"devlog.linux","position":{"start":{"line":172,"column":55,"offset":8513},"end":{"line":172,"column":71,"offset":8529},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.linux"}},{"from":{"fname":"devlog.data warehouse","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":39,"column":54,"offset":2039},"end":{"line":39,"column":73,"offset":2058},"indent":[]},"value":"devlog.Big Data","alias":"devlog.Big Data"},{"from":{"fname":"devlog.etl","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":38,"column":21,"offset":897},"end":{"line":38,"column":40,"offset":916},"indent":[]},"value":"devlog.Big Data","alias":"devlog.Big Data"},{"from":{"fname":"devlog.ods","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":8,"column":42,"offset":417},"end":{"line":8,"column":61,"offset":436},"indent":[]},"value":"devlog.Big Data","alias":"devlog.Big Data"},{"from":{"fname":"devlog.yarn","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":10,"offset":10},"end":{"line":2,"column":29,"offset":29},"indent":[]},"value":"devlog.Big Data","alias":"devlog.Big Data"}],"anchors":{"5-vs-of-big-data":{"type":"header","text":"5 Vs of Big Data","value":"5-vs-of-big-data","line":14,"column":0,"depth":3},"use-cases":{"type":"header","text":"Use cases:","value":"use-cases","line":22,"column":0,"depth":3},"devloghadoop":{"type":"header","text":"devlog.hadoop","value":"devloghadoop","line":64,"column":0,"depth":3},"hbase":{"type":"header","text":"HBase","value":"hbase","line":86,"column":0,"depth":3},"mahoot":{"type":"header","text":"Mahoot","value":"mahoot","line":91,"column":0,"depth":3},"flume":{"type":"header","text":"Flume","value":"flume","line":95,"column":0,"depth":3},"devloghdfs":{"type":"header","text":"devlog.hdfs","value":"devloghdfs","line":107,"column":0,"depth":3},"mapreduce":{"type":"header","text":"MapReduce","value":"mapreduce","line":115,"column":0,"depth":3},"spark":{"type":"header","text":"Spark","value":"spark","line":122,"column":0,"depth":3},"devlogapache-spark-vs-devlogmapreduce":{"type":"header","text":"devlog.apache spark VS devlog.mapreduce","value":"devlogapache-spark-vs-devlogmapreduce","line":133,"column":0,"depth":3},"spark-rdds":{"type":"header","text":"Spark RDDs","value":"spark-rdds","line":141,"column":0,"depth":3},"spark-dataframes":{"type":"header","text":"Spark DataFrames","value":"spark-dataframes","line":170,"column":0,"depth":3},"python-and-spark":{"type":"header","text":"Python and Spark","value":"python-and-spark","line":175,"column":0,"depth":3},"miscellaneous-notes":{"type":"header","text":"Miscellaneous Notes","value":"miscellaneous-notes","line":182,"column":0,"depth":2},"scalable-cluster-computing":{"type":"header","text":"Scalable Cluster Computing","value":"scalable-cluster-computing","line":184,"column":0,"depth":3},"of-servers-nodes-and-daemons":{"type":"header","text":"Of Servers, Nodes and Daemons","value":"of-servers-nodes-and-daemons","line":190,"column":0,"depth":3},"servernode":{"type":"header","text":"Server/Node","value":"servernode","line":192,"column":0,"depth":4},"daemon":{"type":"header","text":"Daemon","value":"daemon","line":199,"column":0,"depth":4},"scalable-behaviour":{"type":"header","text":"Scalable Behaviour","value":"scalable-behaviour","line":207,"column":0,"depth":3}},"children":[],"parent":"9gtn7g40cvqui0sifl1s7t5","data":{}},"body":"\u003ch1 id=\"big-data\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#big-data\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eBig Data\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eAreas: \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003edevlog.Data Engineering (Private)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eBig Data is a term for collections of data sets so large and complex that it becomes difficult to process them using in-hand database management tools or traditional data processing applications.\u003c/p\u003e\n\u003ch3 id=\"5-vs-of-big-data\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#5-vs-of-big-data\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e5 Vs of Big Data\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eVolume - Storing and processing huge data sets\u003c/li\u003e\n\u003cli\u003eVariety - Storing and processing different types of data\u003c/li\u003e\n\u003cli\u003eVelocity - The speed at which data is being generated\u003c/li\u003e\n\u003cli\u003eValue - Finding meaning out of data, making sense of data\u003c/li\u003e\n\u003cli\u003eVeracity - Uncertainty and inconsistencies in the data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"use-cases\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#use-cases\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eUse cases:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWeb and E-tailing\n\u003cul\u003e\n\u003cli\u003eRecommendation Engines\u003c/li\u003e\n\u003cli\u003eAd Targeting\u003c/li\u003e\n\u003cli\u003eSearch Quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTelecommunications\n\u003cul\u003e\n\u003cli\u003eAbuse and Click Fraud Detection\u003c/li\u003e\n\u003cli\u003eCustomer Churn Prevention\u003c/li\u003e\n\u003cli\u003eNetwork Performance Optimization\u003c/li\u003e\n\u003cli\u003eCalling Data Record(CDR) Analysis\u003c/li\u003e\n\u003cli\u003eAnalyzing Network to Predict Failure\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBanks and Financial Services\n\u003cul\u003e\n\u003cli\u003eModeling True Risk\u003c/li\u003e\n\u003cli\u003eThreat Analysis\u003c/li\u003e\n\u003cli\u003eFraud Detection\u003c/li\u003e\n\u003cli\u003eTrade Surveillance\u003c/li\u003e\n\u003cli\u003eCredit Scoring and Analysis\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRetail\n\u003cul\u003e\n\u003cli\u003ePoint of Sales Transaction Analysis\u003c/li\u003e\n\u003cli\u003eCustomer Churn Analysis\u003c/li\u003e\n\u003cli\u003eSentiment Analysis\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eGovernment\n\u003cul\u003e\n\u003cli\u003eFraud Detection and Cybersecurity\u003c/li\u003e\n\u003cli\u003eWelfare Schemes\u003c/li\u003e\n\u003cli\u003eJustice\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eHealthcare and Life Sciences\n\u003cul\u003e\n\u003cli\u003eHealth Information Exchange\u003c/li\u003e\n\u003cli\u003eGene Sequencing\u003c/li\u003e\n\u003cli\u003eSerialization\u003c/li\u003e\n\u003cli\u003eHealthcare Service Quality Improvements\u003c/li\u003e\n\u003cli\u003eDrug Safety\u003c/li\u003e\n\u003cli\u003eSee \u003ca href=\"https://cwiki.apache.org/confluence/display/HADOOP2/poweredby\"\u003ePoweredBy\u003c/a\u003e for companies that are these technologies for Big Data\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSemi-structured data\u003c/li\u003e\n\u003cli\u003eUnstructured data\u003c/li\u003e\n\u003cli\u003eStructured data\u003c/li\u003e\n\u003cli\u003eLarge amount of data cannot fit on a single machine that needs processing is processed using distributed machines.\u003c/li\u003e\n\u003cli\u003eA distributed process has access to the computational resources across a number of machines connected through a network.\u003c/li\u003e\n\u003cli\u003eAfter a certain point, it is easier to scale out to many lower \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eCPU (Private)\u003c/a\u003e machines, than to try to scale up to a single machine with a high CPU.\u003c/li\u003e\n\u003cli\u003eDistributed machines also have the advanteage of easily scaling, you can just add more machines.\u003c/li\u003e\n\u003cli\u003eThey also include fault tolerance, if one machines fails, the whole network can still go on.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hadoop\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#hadoop\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003ca href=\"/notes/85w31vcdf3bjnm0yxh72ygf\"\u003eHadoop\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHadoop is a way to distribute very large files across multiple machines.\u003c/li\u003e\n\u003cli\u003eIt uses the Hadoop Distributed File System \u003ca href=\"/notes/kdddo1f7ltfsuwhexj4s535\"\u003eHDFS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eHDFS allows a user to work with large data sets\u003c/li\u003e\n\u003cli\u003eHDFS also duplicates blocks of data for fault tolerance\u003c/li\u003e\n\u003cli\u003eIt also then uses \u003ca href=\"/notes/j6kfx6ziqg91r356nur9cwy\"\u003eMapReduce\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eMapReduce allows computations on that data\u003c/li\u003e\n\u003cli\u003eYou've a NameNode(Master) and that connects to multiple DataNode(s)(Slave)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHadoop really only comes with HDFS and MapReduce and all the other technologies are addons, developed separately by different individuals.\u003c/p\u003e\n\u003cp\u003eJava is highly preferable for MR\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePig was invented by Yahoo, PigLatin is the language, it a scripting language that was brought about to replace SQL/Java in MR. Internally it runs on MR and converts the code to Java. Most stuff that can be done on Hive can be done on Pig.\u003c/li\u003e\n\u003cli\u003eSqoop was invented by a group, to bring data from RDBMS to Hadoop you write code in MapReduce, to avoid this, they brought about sqoop, sqoop doesn't use a programming language but commands to bring in data to Hadoop. sqoop also allows you to send data back to RDBMS from Hadoop. Import and Export. Used for Datapipeline, sqoop internally runs on Java.\u003c/li\u003e\n\u003cli\u003eOozie was invented by Yahoo, its like an XML file, its a scheduler, used for automating, scheduling jobs, like running queries on a specific time. Big Data tech stack may include non Big Data specific schedulers. It runs on Java internally.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAll of above technologies discussed are abstractions of MapReduce.\u003c/p\u003e\n\u003cp\u003eBig Data tech stack don't only consist of Big Data tools/technologies, MySQL for example is not a Big Data specific tool/technologies.\u003c/p\u003e\n\u003ch3 id=\"hbase\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#hbase\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003ca href=\"#HBase\"\u003eHBase\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eStorage part, Database for Hadoop but NoSQL, invented by Facebook, HBase shell commands, works on top of Hadoop, all the modern databases are NoSQL database.\nIt is different from Hive.\u003c/p\u003e\n\u003ch3 id=\"mahoot\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mahoot\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMahoot\u003c/h3\u003e\n\u003cp\u003eFramework for Data Science\u003c/p\u003e\n\u003ch3 id=\"flume\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#flume\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eFlume\u003c/h3\u003e\n\u003cp\u003eSimilar to sqoop, used in datapipelines, flume can retrieve data in realtime. Messaging queue. sqoop is only for RDBMS unlike Flume which can be used for a fetching data from many other sources.(like twitter). Flume only retrieves data to Hadoop but cannot send it back to any other targets, only incoming.\u003c/p\u003e\n\u003cp\u003eThere are other MQs such as Kafka, Scribe.\u003c/p\u003e\n\u003cp\u003eHadoop is a losely couple framework, you can swap out/in components like oozie and it would still work, unlike tightly coupled frameworks(like .NET frameworks)\u003c/p\u003e\n\u003cp\u003eIntegrations - Hadoop can be integrated well with Spark or any other Big Data frameworks.\u003c/p\u003e\n\u003cp\u003eHadoop can also be connected with non Big Data components like \u003ca href=\"/notes/ypszfixe0p3s5k0inqs5g08\"\u003eMySql\u003c/a\u003e or other RDBMSes.\u003c/p\u003e\n\u003ch3 id=\"hdfs\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#hdfs\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003ca href=\"/notes/kdddo1f7ltfsuwhexj4s535\"\u003eHDFS\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHDFS will use blocks of data, with a size of 128 MB by default.\u003c/li\u003e\n\u003cli\u003eEach of these blocks is replicated 3 times.\u003c/li\u003e\n\u003cli\u003eThe blocks are distributed in a way to support fault tolerance.\u003c/li\u003e\n\u003cli\u003eSmaller blocks provide more parallelization during processing.\u003c/li\u003e\n\u003cli\u003eMultiple copies of a block prevents loss of data due to a failure of a node.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"mapreduce\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mapreduce\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMapReduce\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMapReduce is a way of splitting a computation task to a distributed set of files (such as HDFS).\u003c/li\u003e\n\u003cli\u003eIt consists of a Job Tracker and multiple Task Trackers.\u003c/li\u003e\n\u003cli\u003eThe Job Tracker sends code to run on the Task Trackers.\u003c/li\u003e\n\u003cli\u003eThe Task Trackers allocate CPU and memory for the tasks and monitor the tasks on the worker nodes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"spark\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSpark\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOne of the latest and open source technologies built to quickly and easily handle Big Data.\u003c/li\u003e\n\u003cli\u003eIt is a Framework for dealing with large data, distributing it and doing calculations across distributed network. It is written in Scala. Scala gets the Spark's latest features, Scala is written in Java so Java can also be used for Spark's lastest features.\u003c/li\u003e\n\u003cli\u003eCreated at AMPLab at UC Berkeley, first released in Feb of 2013.\u003c/li\u003e\n\u003cli\u003eIt is a flexible alternative to MapReduce. (doesn't necessarily replaces Hadoop but MapReduce)\u003c/li\u003e\n\u003cli\u003eSpark can use data stored in a variety of formats\n\u003cul\u003e\n\u003cli\u003eCassandra\u003c/li\u003e\n\u003cli\u003eAWS S3\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/kdddo1f7ltfsuwhexj4s535\"\u003eHDFS\u003c/a\u003e and more\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"apache-spark-vs-mapreduce\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#apache-spark-vs-mapreduce\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003ca href=\"/notes/f2kecna72pmc7re3wh1ugk4\"\u003eApache Spark\u003c/a\u003e VS \u003ca href=\"/notes/j6kfx6ziqg91r356nur9cwy\"\u003eMapReduce\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMapReduce requires files to be stored in HDFS, Spark does not!\u003c/li\u003e\n\u003cli\u003eSpark also can perform operations up to 100x faster than MapReduce.\u003c/li\u003e\n\u003cli\u003eIt is not really Hadoop VS MapReduce, people sometime make it as such because MapReduce uses HDFS.\u003c/li\u003e\n\u003cli\u003eMapReduce writes most data to disk after each Map and Reduce operation.\u003c/li\u003e\n\u003cli\u003eSpark keeps most of the data in memory something like RAM after each Transformation. Spark can spill over to \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eHDD (Private)\u003c/a\u003e if you don't have enough \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eRAM (Private)\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"spark-rdds\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-rdds\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSpark RDDs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAt the core of Spark is the idea of a Resilient Distributed Dataset(RDD).\u003c/li\u003e\n\u003cli\u003eResilient Distributed Dataset (RDD) has 4 main features:\n\u003cul\u003e\n\u003cli\u003eDistrbuted Collections of Data\u003c/li\u003e\n\u003cli\u003eFault-tolerant\u003c/li\u003e\n\u003cli\u003eParallel operation - ability to be partitioned\u003c/li\u003e\n\u003cli\u003eAbility to use many data sources\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAt the abstract level it looks like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.ewfeqm3kqzs.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAt physical level:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.3f0c49iwkep.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/2vn2g9mhmi624b83rl5se0k\"\u003eRDD\u003c/a\u003es are immutable, lazily evaluated and cacheable\u003c/li\u003e\n\u003cli\u003eThere are two types of Spark operations:\n\u003cul\u003e\n\u003cli\u003eTransformations\u003c/li\u003e\n\u003cli\u003eActions\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTransformations are basically a recipe to follow.\u003c/li\u003e\n\u003cli\u003eActions actually perform what the recipe says to do and returns something back.\u003c/li\u003e\n\u003cli\u003eThis behavior carries over to the syntax when coding.\u003c/li\u003e\n\u003cli\u003eA lot of times you will write a method call off of a \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eDataFrame (Private)\u003c/a\u003e you won't see anything as a result until you call the action.\u003c/li\u003e\n\u003cli\u003eThis makes sense because with a large dataset, you don't want to calculate all the transformations until you are sure you want to perform them!\u003c/li\u003e\n\u003cli\u003eWhen discussing Spark syntax you'll often see RDD versus DataFrame syntax show up.\u003c/li\u003e\n\u003cli\u003eWith the release of Spark 2.0, Spark is moving towards a DataFrame based syntax, but keep in mind that the way files are being distributed can still be thought of as RDDs(it is still happening in RDD at the pyhsical level), it is just the typed out syntax that is changing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"spark-dataframe-privates\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#spark-dataframe-privates\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSpark \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eDataFrame (Private)\u003c/a\u003es\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSpark DataFrames are also now the standard way of using Spark's Machine Learning capabilities.\u003c/li\u003e\n\u003cli\u003eSpark DataFrame documentation is still pretty new and can be sparse, it can be found on \u003ca href=\"https://spark.apache.org/docs/latest/\"\u003espark.apache.org\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"python-and-spark\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#python-and-spark\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePython and Spark\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRealistically Spark won't be running on a single machine, it will run on a cluster on a service, like \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eAWS (Private)\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eThese cluster services will pretty much always be a \u003ca href=\"/notes/owoutsv5dicylguol2odc3e\"\u003eLinux\u003c/a\u003e) based system.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"miscellaneous-notes\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#miscellaneous-notes\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMiscellaneous Notes\u003c/h2\u003e\n\u003ch3 id=\"scalable-cluster-computing\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#scalable-cluster-computing\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eScalable Cluster Computing\u003c/h3\u003e\n\u003cp\u003eScalable Cluster Computing is a method of combining discrete computing resources(e.g servers) to increase the performance of a single application.\u003c/p\u003e\n\u003cp\u003eDoes not work for applications! (i.e., not all applications \"scale\")\u003c/p\u003e\n\u003ch3 id=\"of-servers-nodes-and-daemons\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#of-servers-nodes-and-daemons\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eOf Servers, Nodes and Daemons\u003c/h3\u003e\n\u003ch4 id=\"servernode\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#servernode\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eServer/Node\u003c/h4\u003e\n\u003cp\u003eA Cluster is a collection of servers working together on one problem.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eServers come with cores, memory, storage and network\u003c/li\u003e\n\u003cli\u003eServers can be called a \"\u003cstrong\u003eNODE\u003c/strong\u003e\" in a cluster of servers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"daemon\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#daemon\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eDaemon\u003c/h4\u003e\n\u003cp\u003eA daemon is a program that is constantly running and responding to requests (e.g., web server)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eServers can have one or multiple daemons running on them\u003c/li\u003e\n\u003cli\u003eDaemons can communicate with other daemons and form a network within and between servers\u003c/li\u003e\n\u003cli\u003eEach daemon can also be called a \"\u003cstrong\u003eNODE\u003c/strong\u003e\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"scalable-behaviour\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#scalable-behaviour\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eScalable Behaviour\u003c/h3\u003e\n\u003cp\u003eof Scalable Systems:\nThe ability to apply resources (servers, VM instances) to a problem as a means to increase performance(green line) over a single server(red line).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.t6kfn1dy0om.png\"\u003e\u003c/p\u003e\n\u003cp\u003eBut, initially performance may lag due to overhead(the setup of our infrastructure).\u003c/p\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/lz322a84xc913sor5r5c71u\"\u003eData Warehouse\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/ffhm4kotv5wbd139ml9ch44\"\u003eETL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/hr5l5ukjr3mfsnsn0fkw6w5\"\u003eODS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/6k7g4x7ws565lditv1n8hxm\"\u003eYARN\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"3nfl4nvv516muyzozhcwrw8","title":"/root","desc":"","updated":1655559901157,"created":1637610830605,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"581715455a6f0f7a699209e8521b4acf","links":[{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"my","position":{"start":{"line":4,"column":9,"offset":37},"end":{"line":4,"column":29,"offset":57},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":20,"column":111,"offset":1051},"end":{"line":20,"column":117,"offset":1057},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":21,"column":3,"offset":1198},"end":{"line":21,"column":9,"offset":1204},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes","alias":"swipes","position":{"start":{"line":27,"column":3,"offset":1724},"end":{"line":27,"column":13,"offset":1734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.quotes","alias":"quotes","position":{"start":{"line":27,"column":48,"offset":1769},"end":{"line":27,"column":72,"offset":1793},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.quotes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.excerpts","alias":"excerpts","position":{"start":{"line":27,"column":74,"offset":1795},"end":{"line":27,"column":102,"offset":1823},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.excerpts"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.sayings","alias":"sayings","position":{"start":{"line":27,"column":104,"offset":1825},"end":{"line":27,"column":130,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.sayings"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.phrases","alias":"phrases","position":{"start":{"line":27,"column":132,"offset":1853},"end":{"line":27,"column":158,"offset":1879},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.phrases"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"resources.people","alias":"others","position":{"start":{"line":27,"column":214,"offset":1935},"end":{"line":27,"column":241,"offset":1962},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"resources.people"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"inbox.webmark","alias":"webmark","position":{"start":{"line":31,"column":235,"offset":2463},"end":{"line":31,"column":260,"offset":2488},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"inbox.webmark"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"slipbox.Ontology","alias":"slipbox.Ontology","position":{"start":{"line":55,"column":3,"offset":3735},"end":{"line":55,"column":23,"offset":3755},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"slipbox.Ontology"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"About me","position":{"start":{"line":60,"column":3,"offset":3963},"end":{"line":60,"column":29,"offset":3989},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}}],"anchors":{"welcome-to-noetic-noggin":{"type":"header","text":"Welcome to Noetic Noggin","value":"welcome-to-noetic-noggin","line":8,"column":0,"depth":1},"principles":{"type":"header","text":"Principles","value":"principles","line":18,"column":0,"depth":2},"all-notes-should-be-relative-to-me":{"type":"header","text":"All notes should be relative to me.","value":"all-notes-should-be-relative-to-me","line":20,"column":0,"depth":3},"gotta-capture-em-all":{"type":"header","text":"Gotta capture 'em all","value":"gotta-capture-em-all","line":30,"column":0,"depth":3},"dont-force-evolution":{"type":"header","text":"Don't force evolution","value":"dont-force-evolution","line":40,"column":0,"depth":3},"noise--signal":{"type":"header","text":"Noise \u0026 Signal","value":"noise--signal","line":45,"column":0,"depth":3},"why-do-any-of-this":{"type":"header","text":"Why do any of this?","value":"why-do-any-of-this","line":50,"column":0,"depth":3},"structure-of-this-wiki":{"type":"header","text":"Structure of this wiki","value":"structure-of-this-wiki","line":59,"column":0,"depth":2},"quicklinks":{"type":"header","text":"Quicklinks","value":"quicklinks","line":64,"column":0,"depth":2}},"children":["0yay2om15bsg2li2p6qgux7","05c4nnjqa92zx11ld6o0ytn","9gtn7g40cvqui0sifl1s7t5","ftbd1hknsd3ocd7jao26tn3","a1kmkdbpclaz5p6sykaw6kc","z121gkmqfo09m8r7jgnpfgn","gkqrr7xbt18xhi93dmjrwzj","ja2x4lrgejr9o9wvit0bd0d","luv39odkfibx3wdosvigwvy","vtvk3bi6o72w58oima9xzf3","yy652kvqrkfn9ipk07m40h4"],"parent":null,"data":{},"body":"\n# Welcome to Noetic Noggin\n\nThis is [[my|archive.about]] personal wiki and a commonplace book; notes by me, for me.\n\n Permanently under construction \n\n![](https://res.cloudinary.com/zubayr/image/upload/v1658499909/wiki/ajevkuyebljlxiblyst2.png)\n\nThis wiki was made possible with [dendron.so](https://dendron.so) and [obisidian.md](https://obsidian.md). Stored on [Github Repository](https://github.com/zubayrrr/dendron) and hosted on [Netlify](https://netlify.com) for free.\n\n## Principles\n\n### All notes should be relative to me.\n\n- All notes in principle are written for me; what I know about a subject, how I feel about a particular thing.\n- Opinions are fine as long as I feel strong epistemic confidence in the given opinion.\n- Don't over explain a note if it's not necessary, remember, these notes are for you and are relative to whatever knowledge you posses about the subject.\n- Read books and make an dedicated notes for them.\n- Listen podcasts but capture them inside a \"subject specific\" note or \"Map of Concept\" note or a note tagged #areas. Because making notes from podcasts can be tedious as they're not as well structured as books for consumption.(Whose merit is debatable.)\n- #areas are basically \"Map of Concept\" notes but I have recently come to the realization that its better to maintain them [Nikita Voloboev style](https://wiki.nikiv.dev/) but with heavy usage of transclusion and backlinking.\n  - \"Resources\" should be first processed and then mentioned inside the note, otherwise they should be left in inbox.\n\n### Gotta capture 'em all\n\n- Hog whatever information tickles your pickle([anything that gratifies one's intellectual curiosity](https://news.ycombinator.com/newsguidelines.html)).\n- [[swipes]] are interesting/useful bits of... [[quotes|swipes.quotes]], [[excerpts|swipes.excerpts]], [[sayings|swipes.sayings]], [[phrases|swipes.phrases]]. Essentially, ideas, opinions that are swiped off from [[others|resources.people]].\n- Make no distinction between \"your\" ideas and ideas of \"others\", because if you vibe with an idea; it's already yours.\n- But also remember \"If you've time to consume, you've time to produce.\".\n- Use [raindrop.io](https://raindrop.io) to manage your URL bookmarks.\n- If you need to bookmark a webpage or an article all together, use [MarkDownload](https://chrome.google.com/webstore/detail/markdownload-markdown-web/pcmpcfapbekmbjjkdalcgopdkipoggdi?hl=en-GB) to rip the entire page. Let's call it a [[webmark|inbox.webmark]]; it belong in the `/inbox`.\n- Similar process is employed for capturing tweets using [tweet-to-markdown](https://github.com/kbravh/tweet-to-markdown) and it also belongs in the `/inbox`.\n\n### Don't force evolution\n\n- Let your second brain evolve at it's own pace.\n- The structure should never be _too_ rigid because its meant to take form by itself.\n\n### Noise \u0026 Signal\n\n- While capturing ideas left and right is recommended, make sure you're not harming your periods of focus.\n- Have impenetrable focus periods (use Pomodoro method) where you only care about the work on hand and nothing else.\n\n### Why do any of this?\n\n- Because I can't remember everything - there's a lot of information around that interests me and there isn't enough working memory installed in me.\n- So, I make notes - to remember, to create, to meditate, to think.\n- A bodybuilder's portfolio is their body - my portfolio is my wiki.\n- Not only am I making - whatever I know - tangible by writing it down. I know exactly where to look if I ever forget something.\n- I am at the beginning of my learning adventures. When I look back at it, I will know where I came from and how my thoughts evolved over time.\n- Plus, its really fun to nerd out.\n\n## Structure of this wiki\n\n- [[slipbox.Ontology]] explains the structure of this wiki and the tags, backlinks used in it.\n- Dendron takes care of the structure and hierarchy(mostly), but I insist on using tags for backwards compatibility.\n\n## Quicklinks\n\n- [[About me|archive.about]]\n- [Github](https://github.com/zubayrrr)\n- [Twitter](https://twitter.com/zoobhalu)\n- [Blog](https://zubayrali.in)\n- [Guestbook](https://www.yourworldoftext.com/~zubayrali/)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template","insertNote":{"initialValue":"templates"}},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Noetic Noggin","description":"Personal Wiki / Digital Garden","author":"Zubayr Ali","twitter":"zoobhalu"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteUrl":"localhost:3000","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"s5t9uscswvpo3p6gebui1ea"},"buildId":"4oDSInv8WeDX6cVSprjyl","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>