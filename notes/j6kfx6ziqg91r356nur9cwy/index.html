<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>MapReduce</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal Wiki / Digital Garden"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="zoobhalu"/><meta name="twitter:creator" content="zoobhalu"/><meta property="og:title" content="MapReduce"/><meta property="og:description" content="Personal Wiki / Digital Garden"/><meta property="og:url" content="localhost:3000/notes/j6kfx6ziqg91r356nur9cwy/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="Invalid DateTime"/><meta property="article:modified_time" content="5/23/2022"/><link rel="canonical" href="localhost:3000/notes/j6kfx6ziqg91r356nur9cwy/"/><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/cc2307f6fd3a2fec.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cc2307f6fd3a2fec.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-709abf7ab5f510de.js" defer=""></script><script src="/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/_next/static/chunks/main-c4b0e551a2150d17.js" defer=""></script><script src="/_next/static/chunks/pages/_app-62c5b93605efada7.js" defer=""></script><script src="/_next/static/chunks/78-13ae6acd5ce7ca5b.js" defer=""></script><script src="/_next/static/chunks/373-2f3879190a46a3d9.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-69449972c2a725d8.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_buildManifest.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_ssgManifest.js" defer=""></script><script src="/_next/static/4oDSInv8WeDX6cVSprjyl/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout site-layout" style="margin-top:64px"><section class="ant-layout site-layout"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="mapreduce"><a aria-hidden="true" class="anchor-heading" href="#mapreduce"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>MapReduce</h1>
<ul>
<li>MapReduce is a simple algorithm</li>
<li>It uses distinct steps and one-way communication</li>
<li><strong>Map</strong> first then <strong>Reduce</strong>, can be combined into multiple layers</li>
</ul>
<h3 id="example-of-mapreduce-but-non-parallel"><a aria-hidden="true" class="anchor-heading" href="#example-of-mapreduce-but-non-parallel"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Example of MapReduce (but non parallel)</h3>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.uyhzcdyhtl.png"></p>
<h3 id="mapreduce-design-principles"><a aria-hidden="true" class="anchor-heading" href="#mapreduce-design-principles"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>MapReduce Design Principles</h3>
<p><strong>"Can we read parts(<em>slices</em>) of the file at the same time on different systems?"</strong></p>
<ul>
<li>For certain types of problems the answer is "Yes"</li>
<li>Slice data and spread across multiple HDDs on multiple servers (<a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a>)</li>
<li>Obtain performance from reading slices at the same time</li>
<li>Moving computation to data, if possible(cheaper than moving data to computation)</li>
<li>Allows scalable software that can hide (most) execution details from the user</li>
<li>The ability to be fault tolerant (map steps crashing, restarting) which can be extended to reducers.</li>
</ul>
<h3 id="mapreduce-big-picture"><a aria-hidden="true" class="anchor-heading" href="#mapreduce-big-picture"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>MapReduce Big Picture</h3>
<ul>
<li>Slice data and place each part on a different server(node)?</li>
<li>Improve performance by mapping (processing) each slice independently</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.2eclq7p550p.png"></p>
<ul>
<li>Slicing happens automatically, the administrator can change the size of the slice</li>
<li>Slice is blind to the data that is inside the file(s), we're not slicing based on structure but based on size.</li>
</ul>
<h3 id="slices-and-splits"><a aria-hidden="true" class="anchor-heading" href="#slices-and-splits"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Slices and Splits</h3>
<ul>
<li>Each slice has programmatically defined data splits that independent of slice boundaries.</li>
<li><strong>A map is applied to each split in the slice.</strong></li>
<li>Edge splits may span two slices.</li>
<li>There can be multiple maps occuring on the same slice.
<ul>
<li>If a split doesn't fit on a slice, edge splits may span to other slices(not a lot of data and MapReduce algo will account for it automagically)</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.wstyvft6d0s.png"></p>
<h3 id="scalable-mapping-step"><a aria-hidden="true" class="anchor-heading" href="#scalable-mapping-step"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Scalable Mapping Step</h3>
<ol>
<li>Slice data into parts</li>
<li>Apply the same Mapping function to <strong>Input list</strong> (user defined splits)</li>
<li>Each step provides a separate set of results(<strong>Output list</strong>)</li>
</ol>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.2glgo0k8cyt.png"></p>
<h3 id="reduction-step"><a aria-hidden="true" class="anchor-heading" href="#reduction-step"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Reduction Step</h3>
<ul>
<li>Results from each map(as applied to each split) are combined and reduced to a single Output value.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.w1uoldg3if.png"></p>
<h3 id="summary"><a aria-hidden="true" class="anchor-heading" href="#summary"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Summary</h3>
<ul>
<li><a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a> provides a MapReduce engine as part of the core components (<a href="/notes/kdddo1f7ltfsuwhexj4s535">HDFS</a>, <a href="/notes/6k7g4x7ws565lditv1n8hxm">YARN</a> and <a href="/notes/j6kfx6ziqg91r356nur9cwy">MapReduce</a>)</li>
<li>Hadoop MapReduce can be used directly or by higher level applications(e.g. <a href="/notes/30isnfzmqrvmmiii03d0chj">Hive</a>)</li>
<li>Original MapReduce was somewhat slow because intermediate results are written to disk.</li>
<li>Current MapReduce uses <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Tez (Private)</a> acceleration to keep intermediate results in memory (very fast)</li>
<li>Operate on "Bigger than Database" amounts of data.</li>
</ul>
<h3 id="advantages-of-hadoop-mapreduce"><a aria-hidden="true" class="anchor-heading" href="#advantages-of-hadoop-mapreduce"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Advantages of Hadoop MapReduce</h3>
<ul>
<li>Uses a functional approach (original data is unchanged)</li>
<li>Restricted to one-way communication path</li>
<li>Provides the following features:
<ul>
<li>Highly scalable(same user code)</li>
<li>Easily managed workflow</li>
<li>Hardware fault tolerant</li>
</ul>
</li>
<li>MapReduce is powerful paradigm for solving problems</li>
<li>Often referred to as a "Data Parallel" or <strong>S</strong>ingle <strong>I</strong>nstrution <strong>M</strong>ultiple <strong>D</strong>ata problem (SIMD)</li>
</ul>
<h3 id="programming-hadoop-mapreduce"><a aria-hidden="true" class="anchor-heading" href="#programming-hadoop-mapreduce"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Programming Hadoop MapReduce</h3>
<ul>
<li><strong>Program Natively</strong> using Java API (Either <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Tez (Private)</a> or MapReduce)</li>
<li>Uses other languages like Python with the Streaming interface(stdin, stdout and text only)</li>
<li>Use the C++ Pipes interface.</li>
<li>Use higher level tools (e.g., <a href="/notes/30isnfzmqrvmmiii03d0chj">Hive</a>, Pig) on top of MapReduce or Tez acceleration</li>
<li>MapReduce applications can scale with no change to the application</li>
<li>Computation is moved close to the data in HDFS</li>
<li>There is no need to manage side-effects or process state</li>
<li>SW/HW faults in HDFS and YARN are handled by automatic restart of tasks, transparent to the application.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.bnsj6fk5m18.png"></p>
<h2 id="examples-mapreduce-in-parallel"><a aria-hidden="true" class="anchor-heading" href="#examples-mapreduce-in-parallel"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Examples (MapReduce in Parallel)</h2>
<h3 id="word-count"><a aria-hidden="true" class="anchor-heading" href="#word-count"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Word Count</h3>
<p>Determine how many times unique words are used in the following text.</p>
<ul>
<li><code>see spot run</code></li>
<li><code>run spot run</code></li>
<li><code>see the cat</code></li>
</ul>
<p>A trivial task. How does MapReduce perform this at scale when a collection of document has 50 billion words?</p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.upf9zn7wm2.png"></p>
<p>We can add improvements to our mapper using a combiner</p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.x9crxdaozq.png"></p>
<h3 id="working-example-of-word-count"><a aria-hidden="true" class="anchor-heading" href="#working-example-of-word-count"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Working Example of Word Count</h3>
<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar > words.txt</code></p>
<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar wordcount wordcount/in wordcount/out</code></p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.vkmgyisvvbc.png"></p>
<p>Check the output that was created</p>
<p><code>hdfs dfs -cat wordcount/out/part-r-00000</code></p>
<p><img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.8velr6e7whs.png"></p>
<h3 id="calculate-pi"><a aria-hidden="true" class="anchor-heading" href="#calculate-pi"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Calculate pi</h3>
<ul>
<li>Simple MapReduce program to run, good for quick tests.</li>
<li>The following is run on the 4-node cluster</li>
</ul>
<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar pi 16 100000</code><br>
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.ip1wh7ittuc.png">
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.op5t2hnmh7l.png"></p>
<ul>
<li>The following is a short test that will run on the LHM-VM (If you increase</li>
<li>the number of maps (8) or the number of guesses (1000) then the</li>
<li>application will take longer.</li>
</ul>
<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar pi 8 1000</code></p>
<h3 id="the-terasort-benchmark"><a aria-hidden="true" class="anchor-heading" href="#the-terasort-benchmark"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>The Terasort Benchmark</h3>
<p><strong>IMPORTANT: The test will not run on the LHM-VM. Indeed, as run below,
the Teragen portion will create a 50GB file (by default the LHM-VM has
70 GB of file space. The actual Terasort step will create another
50 GB file. You can try to reduce the file size from 500000000 to
5000 so the application can finish.</strong>
Terasort is used to measure the raw sorting power of Hadoop MapReduce, though it is of no practical use, it can provide an indication of how fast your cluster can process data.</p>
<p>The following steps are for the the 4-node cluster mentioned above, using the HDFS account <code>/user/deadline</code>.</p>
<p>There are three steps required to run the complete test:</p>
<ol>
<li>Generate the table</li>
<li>Sort the table</li>
<li>Validate the sort</li>
</ol>
<!-- end list -->
<ul>
<li>
<p>rows are 100 bytes long, thus the total amount of data written is 100 times the</p>
</li>
<li>
<p>number of rows (i.e. to write a 100 GBytes of data, use 1000000000 rows). You</p>
</li>
<li>
<p>will also need to specify input and output directories in HDFS.</p>
</li>
<li>
<p>1. Run teragen to generate 500 MB of data, 500,000 rows of random data to sort</p>
</li>
</ul>
<p><code>yarn jar $$EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar teragen 5000000 /user/hands-on/TeraGen-500MB</code>
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.dhhwu16cazc.png"> * Check if the data was created; this will be your input for the next step in the terasort benchmark
<code>hdfs dfs -ls TeraGen-500MB</code>
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.brz6odys9hv.png"></p>
<ul>
<li>2. Run terasort to sort the database</li>
</ul>
<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar terasort /user/hands-on/TeraGen-500MB /user/hands-on/TeraSort-500MB</code>
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.idartp3zxj.png">
Check the output that was created for <code>TeraSort-500MB</code> (which was the output dir)
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.bohj41tce1a.png"></p>
<p>A combined single file was created</p>
<ul>
<li>3. Run teravalidate to validate the sort Teragen</li>
</ul>
<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar teravalidate /user/hands-on/TeraSort-500MB /user/hands-on/TeraValid-500MB</code>
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.m6uaknqoch.png">
Check the results that was created and the checksum of the file
<img src="https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.4ch3o9j2lo7.png"> * Interpret the results: * Measure the time it takes to complete the terasort application. Results are * usually reported in Database Size in Seconds (or Minutes).</p>
<ul>
<li>The performance can be increased by increasing the number of reducers (default is one)</li>
<li>add the option -Dmapred.reduce.tasks=NUMBER_OF_REDUCERS</li>
<li>The command below uses 4 reducers.</li>
</ul>
<p><code>yarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar terasort -Dmapred.reduce.tasks=4 /user/hands-on/TeraGen-500MB /user/hands-on/TeraSort-500MB</code></p>
<ul>
<li>Don't for get to delete you files in HDFS before the next run!</li>
</ul>
<p><code>hdfs dfs -rm -r -skipTrash Tera*</code></p>
<h3 id="miscellaneous-notes"><a aria-hidden="true" class="anchor-heading" href="#miscellaneous-notes"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Miscellaneous notes</h3>
<p><a href="/notes/85w31vcdf3bjnm0yxh72ygf">Hadoop</a> and <a href="/notes/f2kecna72pmc7re3wh1ugk4">Apache Spark</a> usually work with "INPUT DIRECTORIES" not files. Note that the argument given to word count in the above example was a directory NOT the "words.txt" file. We can add more files to the input directory and <code>wordcount</code> will ingest all the files in finds.</p>
<h2 id="simulating-map-and-reduce-using-python-and-bash-scripts"><a aria-hidden="true" class="anchor-heading" href="#simulating-map-and-reduce-using-python-and-bash-scripts"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Simulating Map and Reduce using Python and Bash scripts</h2>
<p>Source: <a href="https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></p>
<p>There are several methods to program a Hadoop MapReduce. The most direct and lowest level is using the Java Hadoop API or the C++ Pipes library.</p>
<p>Many users prefer the flexibility of the Streams Interface that allows a variety of programming language to be used.</p>
<p>This method will work with any program that can read and write TEXT DATA to <a href="/notes/n4ernfsrloq7zjrscm8t5rc">Redirecting stdin &#x26; stderr</a> and <a href="/notes/do68y261xeu98bpq3vl5cn9">stdout</a>.</p>
<h3 id="developing-a-hadoop-streaming-word-counting-application"><a aria-hidden="true" class="anchor-heading" href="#developing-a-hadoop-streaming-word-counting-application"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Developing a Hadoop Streaming word counting application.</h3>
<ul>
<li>pymapper.py - a python word count mapper</li>
<li>shuffer.sh - a simulated shuffle step using bash script</li>
<li>pyreducer.py - a python word count reducer</li>
</ul>
<!-- end list -->
<pre><code>#!/usr/bin/env python
"""pymapper.py"""

import sys

# input comes from STDIN (standard input)
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()
    # split the line into words
    words = line.split()
    # increase counters
    for word in words:
        # write the results to STDOUT (standard output);
        # what we output here will be the input for the
        # Reduce step, i.e. the input for reducer.py
        #
        # tab-delimited; the trivial word count is 1
        print '%s\t%s' % (word, 1)

#!/bin/bash
# shuffle.sh
sort -k1,1

#!/usr/bin/env python
"""pyreducer.py"""

from operator import itemgetter
import sys

current_word = None
current_count = 0
word = None

# input comes from STDIN
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()

    # parse the input we got from mapper.py
    word, count = line.split('\t', 1)

    # convert count (currently a string) to int
    try:
        count = int(count)
    except ValueError:
        # count was not a number, so silently
        # ignore/discard this line
        continue

    # this IF-switch only works because Hadoop sorts map output
    # by key (here: word) before it is passed to the reducer
    if current_word == word:
        current_count += count
    else:
        if current_word:
            # write result to STDOUT
            print ('%s\t%s' % (current_word, current_count))
        current_count = count
        current_word = word

# do not forget to output the last word if needed!
if current_word == word:
    print ('%s\t%s' % (current_word, current_count))
</code></pre>
<p>1. Run the <code>pymapper.py</code> with some simple data and change the input words to key value pairs.</p>
<pre><code>echo "see spot run run spot run see the cat" | ./pymapper.py
</code></pre>
<p>2. Sort the data passed through <code>pymapper.py</code></p>
<pre><code>echo "see spot run run spot run see the cat" | ./pymapper.py | ./shuffle.sh
</code></pre>
<p>3. Add the reducer stage and count all the same keys</p>
<pre><code>echo "see spot run run spot run see the cat" | ./pymapper.py | ./shuffle.sh | ./pyreducer.py
</code></pre>
<p>See also: <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Tez (Private)</a>, <a href="/notes/f2kecna72pmc7re3wh1ugk4">Apache Spark</a></p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/f2kecna72pmc7re3wh1ugk4">Apache Spark</a></li>
<li><a href="/notes/s5t9uscswvpo3p6gebui1ea">Big Data</a></li>
<li><a href="/notes/bojmgq30yfxpn44hgo0ofcf">DAG</a></li>
<li><a href="/notes/2fz1tdl3yy2s78wdmo7ql56">Data Engineering Roadmap</a></li>
<li><a href="/notes/30isnfzmqrvmmiii03d0chj">Hive</a></li>
<li><a href="/notes/e92qvq2zwoa9wyn0p0187l3">HQL</a></li>
<li><a href="/notes/7v21vh269qxy069is0fkrzn">Tez</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#example-of-mapreduce-but-non-parallel" title="Example of MapReduce (but non parallel)">Example of MapReduce (but non parallel)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#mapreduce-design-principles" title="MapReduce Design Principles">MapReduce Design Principles</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#mapreduce-big-picture" title="MapReduce Big Picture">MapReduce Big Picture</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#slices-and-splits" title="Slices and Splits">Slices and Splits</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#scalable-mapping-step" title="Scalable Mapping Step">Scalable Mapping Step</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#reduction-step" title="Reduction Step">Reduction Step</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#summary" title="Summary">Summary</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#advantages-of-hadoop-mapreduce" title="Advantages of Hadoop MapReduce">Advantages of Hadoop MapReduce</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#programming-hadoop-mapreduce" title="Programming Hadoop MapReduce">Programming Hadoop MapReduce</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#examples-mapreduce-in-parallel" title="Examples (MapReduce in Parallel)">Examples (MapReduce in Parallel)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#word-count" title="Word Count">Word Count</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#working-example-of-word-count" title="Working Example of Word Count">Working Example of Word Count</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#calculate-pi" title="Calculate pi">Calculate pi</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#the-terasort-benchmark" title="The Terasort Benchmark">The Terasort Benchmark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#miscellaneous-notes" title="Miscellaneous notes">Miscellaneous notes</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#simulating-map-and-reduce-using-python-and-bash-scripts" title="Simulating Map and Reduce using Python and Bash scripts">Simulating Map and Reduce using Python and Bash scripts</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#developing-a-hadoop-streaming-word-counting-application" title="Developing a Hadoop Streaming word counting application.">Developing a Hadoop Streaming word counting application.</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"j6kfx6ziqg91r356nur9cwy","title":"MapReduce","desc":"","updated":1653305310654,"created":20211122214632356,"custom":{},"fname":"devlog.mapreduce","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"d4f29c70a2f34823f3bf9eb5c082f7d3","links":[{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":15,"column":67,"offset":524},"end":{"line":15,"column":82,"offset":539},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":57,"column":3,"offset":2308},"end":{"line":57,"column":20,"offset":2325},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hdfs","alias":"devlog.hdfs","position":{"start":{"line":57,"column":81,"offset":2386},"end":{"line":57,"column":96,"offset":2401},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hdfs"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.yarn","alias":"devlog.yarn","position":{"start":{"line":57,"column":98,"offset":2403},"end":{"line":57,"column":113,"offset":2418},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.yarn"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.mapreduce","alias":"devlog.mapreduce","position":{"start":{"line":57,"column":118,"offset":2423},"end":{"line":57,"column":138,"offset":2443},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.mapreduce"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hive","alias":"devlog.hive","position":{"start":{"line":58,"column":78,"offset":2522},"end":{"line":58,"column":93,"offset":2537},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hive"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"Tez","alias":"Tez","position":{"start":{"line":60,"column":26,"offset":2653},"end":{"line":60,"column":33,"offset":2660},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Tez"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"Tez","alias":"Tez","position":{"start":{"line":76,"column":47,"offset":3285},"end":{"line":76,"column":54,"offset":3292},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Tez"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hive","alias":"devlog.hive","position":{"start":{"line":79,"column":33,"offset":3463},"end":{"line":79,"column":48,"offset":3478},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hive"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.hadoop","alias":"devlog.hadoop","position":{"start":{"line":192,"column":1,"offset":8456},"end":{"line":192,"column":18,"offset":8473},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.hadoop"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.apache spark","alias":"devlog.apache spark","position":{"start":{"line":192,"column":23,"offset":8478},"end":{"line":192,"column":46,"offset":8501},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.apache spark"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.Redirecting stdin \u0026 stderr","alias":"devlog.Redirecting stdin \u0026 stderr","position":{"start":{"line":202,"column":77,"offset":9252},"end":{"line":202,"column":114,"offset":9289},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.Redirecting stdin \u0026 stderr"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.stdout","alias":"devlog.stdout","position":{"start":{"line":202,"column":119,"offset":9294},"end":{"line":202,"column":136,"offset":9311},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.stdout"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"Tez","alias":"Tez","position":{"start":{"line":289,"column":11,"offset":11852},"end":{"line":289,"column":18,"offset":11859},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"Tez"}},{"type":"wiki","from":{"fname":"devlog.mapreduce","id":"j6kfx6ziqg91r356nur9cwy","vaultName":"Dendron"},"value":"devlog.apache spark","alias":"devlog.apache spark","position":{"start":{"line":289,"column":20,"offset":11861},"end":{"line":289,"column":43,"offset":11884},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"devlog.apache spark"}},{"from":{"fname":"devlog.apache spark","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":69,"column":25,"offset":1938},"end":{"line":69,"column":45,"offset":1958},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.apache spark","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":191,"column":123,"offset":8334},"end":{"line":191,"column":143,"offset":8354},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.big data","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":64,"column":21,"offset":2405},"end":{"line":64,"column":41,"offset":2425},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.big data","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":127,"column":32,"offset":6072},"end":{"line":127,"column":52,"offset":6092},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.dag","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":10,"column":123,"offset":315},"end":{"line":10,"column":143,"offset":335},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.data engineering roadmap","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":18,"column":3,"offset":558},"end":{"line":18,"column":23,"offset":578},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.hive","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":18,"column":39,"offset":1003},"end":{"line":18,"column":59,"offset":1023},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.hive","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":20,"column":56,"offset":1080},"end":{"line":20,"column":76,"offset":1100},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.hql","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":54,"column":22,"offset":1608},"end":{"line":54,"column":42,"offset":1628},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"},{"from":{"fname":"devlog.tez","vaultName":"Dendron"},"type":"backlink","position":{"start":{"line":2,"column":21,"offset":21},"end":{"line":2,"column":41,"offset":41},"indent":[]},"value":"devlog.mapreduce","alias":"devlog.mapreduce"}],"anchors":{"example-of-mapreduce-but-non-parallel":{"type":"header","text":"Example of MapReduce (but non parallel)","value":"example-of-mapreduce-but-non-parallel","line":12,"column":0,"depth":3},"mapreduce-design-principles":{"type":"header","text":"MapReduce Design Principles","value":"mapreduce-design-principles","line":16,"column":0,"depth":3},"mapreduce-big-picture":{"type":"header","text":"MapReduce Big Picture","value":"mapreduce-big-picture","line":27,"column":0,"depth":3},"slices-and-splits":{"type":"header","text":"Slices and Splits","value":"slices-and-splits","line":37,"column":0,"depth":3},"scalable-mapping-step":{"type":"header","text":"Scalable Mapping Step","value":"scalable-mapping-step","line":47,"column":0,"depth":3},"reduction-step":{"type":"header","text":"Reduction Step","value":"reduction-step","line":55,"column":0,"depth":3},"summary":{"type":"header","text":"Summary","value":"summary","line":61,"column":0,"depth":3},"advantages-of-hadoop-mapreduce":{"type":"header","text":"Advantages of Hadoop MapReduce","value":"advantages-of-hadoop-mapreduce","line":69,"column":0,"depth":3},"programming-hadoop-mapreduce":{"type":"header","text":"Programming Hadoop MapReduce","value":"programming-hadoop-mapreduce","line":80,"column":0,"depth":3},"examples-mapreduce-in-parallel":{"type":"header","text":"Examples (MapReduce in Parallel)","value":"examples-mapreduce-in-parallel","line":93,"column":0,"depth":2},"word-count":{"type":"header","text":"Word Count","value":"word-count","line":95,"column":0,"depth":3},"working-example-of-word-count":{"type":"header","text":"Working Example of Word Count","value":"working-example-of-word-count","line":111,"column":0,"depth":3},"calculate-pi":{"type":"header","text":"Calculate pi","value":"calculate-pi","line":125,"column":0,"depth":3},"the-terasort-benchmark":{"type":"header","text":"The Terasort Benchmark","value":"the-terasort-benchmark","line":140,"column":0,"depth":3},"miscellaneous-notes":{"type":"header","text":"Miscellaneous notes","value":"miscellaneous-notes","line":196,"column":0,"depth":3},"simulating-map-and-reduce-using-python-and-bash-scripts":{"type":"header","text":"Simulating Map and Reduce using Python and Bash scripts","value":"simulating-map-and-reduce-using-python-and-bash-scripts","line":200,"column":0,"depth":2},"developing-a-hadoop-streaming-word-counting-application":{"type":"header","text":"Developing a Hadoop Streaming word counting application.","value":"developing-a-hadoop-streaming-word-counting-application","line":210,"column":0,"depth":3}},"children":[],"parent":"9gtn7g40cvqui0sifl1s7t5","data":{}},"body":"\u003ch1 id=\"mapreduce\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mapreduce\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMapReduce\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eMapReduce is a simple algorithm\u003c/li\u003e\n\u003cli\u003eIt uses distinct steps and one-way communication\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMap\u003c/strong\u003e first then \u003cstrong\u003eReduce\u003c/strong\u003e, can be combined into multiple layers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"example-of-mapreduce-but-non-parallel\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#example-of-mapreduce-but-non-parallel\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eExample of MapReduce (but non parallel)\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.uyhzcdyhtl.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"mapreduce-design-principles\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mapreduce-design-principles\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMapReduce Design Principles\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\"Can we read parts(\u003cem\u003eslices\u003c/em\u003e) of the file at the same time on different systems?\"\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor certain types of problems the answer is \"Yes\"\u003c/li\u003e\n\u003cli\u003eSlice data and spread across multiple HDDs on multiple servers (\u003ca href=\"/notes/kdddo1f7ltfsuwhexj4s535\"\u003eHDFS\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eObtain performance from reading slices at the same time\u003c/li\u003e\n\u003cli\u003eMoving computation to data, if possible(cheaper than moving data to computation)\u003c/li\u003e\n\u003cli\u003eAllows scalable software that can hide (most) execution details from the user\u003c/li\u003e\n\u003cli\u003eThe ability to be fault tolerant (map steps crashing, restarting) which can be extended to reducers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"mapreduce-big-picture\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#mapreduce-big-picture\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMapReduce Big Picture\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSlice data and place each part on a different server(node)?\u003c/li\u003e\n\u003cli\u003eImprove performance by mapping (processing) each slice independently\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.2eclq7p550p.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSlicing happens automatically, the administrator can change the size of the slice\u003c/li\u003e\n\u003cli\u003eSlice is blind to the data that is inside the file(s), we're not slicing based on structure but based on size.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"slices-and-splits\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#slices-and-splits\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSlices and Splits\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEach slice has programmatically defined data splits that independent of slice boundaries.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eA map is applied to each split in the slice.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eEdge splits may span two slices.\u003c/li\u003e\n\u003cli\u003eThere can be multiple maps occuring on the same slice.\n\u003cul\u003e\n\u003cli\u003eIf a split doesn't fit on a slice, edge splits may span to other slices(not a lot of data and MapReduce algo will account for it automagically)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.wstyvft6d0s.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"scalable-mapping-step\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#scalable-mapping-step\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eScalable Mapping Step\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eSlice data into parts\u003c/li\u003e\n\u003cli\u003eApply the same Mapping function to \u003cstrong\u003eInput list\u003c/strong\u003e (user defined splits)\u003c/li\u003e\n\u003cli\u003eEach step provides a separate set of results(\u003cstrong\u003eOutput list\u003c/strong\u003e)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.2glgo0k8cyt.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"reduction-step\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#reduction-step\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eReduction Step\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eResults from each map(as applied to each split) are combined and reduced to a single Output value.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.w1uoldg3if.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"summary\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#summary\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSummary\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/85w31vcdf3bjnm0yxh72ygf\"\u003eHadoop\u003c/a\u003e provides a MapReduce engine as part of the core components (\u003ca href=\"/notes/kdddo1f7ltfsuwhexj4s535\"\u003eHDFS\u003c/a\u003e, \u003ca href=\"/notes/6k7g4x7ws565lditv1n8hxm\"\u003eYARN\u003c/a\u003e and \u003ca href=\"/notes/j6kfx6ziqg91r356nur9cwy\"\u003eMapReduce\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eHadoop MapReduce can be used directly or by higher level applications(e.g. \u003ca href=\"/notes/30isnfzmqrvmmiii03d0chj\"\u003eHive\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eOriginal MapReduce was somewhat slow because intermediate results are written to disk.\u003c/li\u003e\n\u003cli\u003eCurrent MapReduce uses \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eTez (Private)\u003c/a\u003e acceleration to keep intermediate results in memory (very fast)\u003c/li\u003e\n\u003cli\u003eOperate on \"Bigger than Database\" amounts of data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"advantages-of-hadoop-mapreduce\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#advantages-of-hadoop-mapreduce\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eAdvantages of Hadoop MapReduce\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUses a functional approach (original data is unchanged)\u003c/li\u003e\n\u003cli\u003eRestricted to one-way communication path\u003c/li\u003e\n\u003cli\u003eProvides the following features:\n\u003cul\u003e\n\u003cli\u003eHighly scalable(same user code)\u003c/li\u003e\n\u003cli\u003eEasily managed workflow\u003c/li\u003e\n\u003cli\u003eHardware fault tolerant\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMapReduce is powerful paradigm for solving problems\u003c/li\u003e\n\u003cli\u003eOften referred to as a \"Data Parallel\" or \u003cstrong\u003eS\u003c/strong\u003eingle \u003cstrong\u003eI\u003c/strong\u003enstrution \u003cstrong\u003eM\u003c/strong\u003eultiple \u003cstrong\u003eD\u003c/strong\u003eata problem (SIMD)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"programming-hadoop-mapreduce\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#programming-hadoop-mapreduce\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eProgramming Hadoop MapReduce\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eProgram Natively\u003c/strong\u003e using Java API (Either \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eTez (Private)\u003c/a\u003e or MapReduce)\u003c/li\u003e\n\u003cli\u003eUses other languages like Python with the Streaming interface(stdin, stdout and text only)\u003c/li\u003e\n\u003cli\u003eUse the C++ Pipes interface.\u003c/li\u003e\n\u003cli\u003eUse higher level tools (e.g., \u003ca href=\"/notes/30isnfzmqrvmmiii03d0chj\"\u003eHive\u003c/a\u003e, Pig) on top of MapReduce or Tez acceleration\u003c/li\u003e\n\u003cli\u003eMapReduce applications can scale with no change to the application\u003c/li\u003e\n\u003cli\u003eComputation is moved close to the data in HDFS\u003c/li\u003e\n\u003cli\u003eThere is no need to manage side-effects or process state\u003c/li\u003e\n\u003cli\u003eSW/HW faults in HDFS and YARN are handled by automatic restart of tasks, transparent to the application.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.bnsj6fk5m18.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"examples-mapreduce-in-parallel\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#examples-mapreduce-in-parallel\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eExamples (MapReduce in Parallel)\u003c/h2\u003e\n\u003ch3 id=\"word-count\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#word-count\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eWord Count\u003c/h3\u003e\n\u003cp\u003eDetermine how many times unique words are used in the following text.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esee spot run\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erun spot run\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esee the cat\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA trivial task. How does MapReduce perform this at scale when a collection of document has 50 billion words?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.upf9zn7wm2.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can add improvements to our mapper using a combiner\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.x9crxdaozq.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"working-example-of-word-count\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#working-example-of-word-count\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eWorking Example of Word Count\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eyarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar \u003e words.txt\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eyarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar wordcount wordcount/in wordcount/out\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.vkmgyisvvbc.png\"\u003e\u003c/p\u003e\n\u003cp\u003eCheck the output that was created\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ehdfs dfs -cat wordcount/out/part-r-00000\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.8velr6e7whs.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"calculate-pi\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#calculate-pi\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eCalculate pi\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSimple MapReduce program to run, good for quick tests.\u003c/li\u003e\n\u003cli\u003eThe following is run on the 4-node cluster\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eyarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar pi 16 100000\u003c/code\u003e\u003cbr\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.ip1wh7ittuc.png\"\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.op5t2hnmh7l.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe following is a short test that will run on the LHM-VM (If you increase\u003c/li\u003e\n\u003cli\u003ethe number of maps (8) or the number of guesses (1000) then the\u003c/li\u003e\n\u003cli\u003eapplication will take longer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eyarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar pi 8 1000\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"the-terasort-benchmark\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#the-terasort-benchmark\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eThe Terasort Benchmark\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eIMPORTANT: The test will not run on the LHM-VM. Indeed, as run below,\nthe Teragen portion will create a 50GB file (by default the LHM-VM has\n70 GB of file space. The actual Terasort step will create another\n50 GB file. You can try to reduce the file size from 500000000 to\n5000 so the application can finish.\u003c/strong\u003e\nTerasort is used to measure the raw sorting power of Hadoop MapReduce, though it is of no practical use, it can provide an indication of how fast your cluster can process data.\u003c/p\u003e\n\u003cp\u003eThe following steps are for the the 4-node cluster mentioned above, using the HDFS account \u003ccode\u003e/user/deadline\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThere are three steps required to run the complete test:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGenerate the table\u003c/li\u003e\n\u003cli\u003eSort the table\u003c/li\u003e\n\u003cli\u003eValidate the sort\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- end list --\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003erows are 100 bytes long, thus the total amount of data written is 100 times the\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003enumber of rows (i.e. to write a 100 GBytes of data, use 1000000000 rows). You\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ewill also need to specify input and output directories in HDFS.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e1. Run teragen to generate 500 MB of data, 500,000 rows of random data to sort\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eyarn jar $$EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar teragen 5000000 /user/hands-on/TeraGen-500MB\u003c/code\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.dhhwu16cazc.png\"\u003e * Check if the data was created; this will be your input for the next step in the terasort benchmark\n\u003ccode\u003ehdfs dfs -ls TeraGen-500MB\u003c/code\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.brz6odys9hv.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2. Run terasort to sort the database\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eyarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar terasort /user/hands-on/TeraGen-500MB /user/hands-on/TeraSort-500MB\u003c/code\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.idartp3zxj.png\"\u003e\nCheck the output that was created for \u003ccode\u003eTeraSort-500MB\u003c/code\u003e (which was the output dir)\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.bohj41tce1a.png\"\u003e\u003c/p\u003e\n\u003cp\u003eA combined single file was created\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e3. Run teravalidate to validate the sort Teragen\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eyarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar teravalidate /user/hands-on/TeraSort-500MB /user/hands-on/TeraValid-500MB\u003c/code\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.m6uaknqoch.png\"\u003e\nCheck the results that was created and the checksum of the file\n\u003cimg src=\"https://raw.githubusercontent.com/zubayrrr/twiki/main/bin/image.4ch3o9j2lo7.png\"\u003e * Interpret the results: * Measure the time it takes to complete the terasort application. Results are * usually reported in Database Size in Seconds (or Minutes).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe performance can be increased by increasing the number of reducers (default is one)\u003c/li\u003e\n\u003cli\u003eadd the option -Dmapred.reduce.tasks=NUMBER_OF_REDUCERS\u003c/li\u003e\n\u003cli\u003eThe command below uses 4 reducers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eyarn jar $EXAMPLES/hadoop-mapreduce-examples-3.3.0.jar terasort -Dmapred.reduce.tasks=4 /user/hands-on/TeraGen-500MB /user/hands-on/TeraSort-500MB\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDon't for get to delete you files in HDFS before the next run!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003ehdfs dfs -rm -r -skipTrash Tera*\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"miscellaneous-notes\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#miscellaneous-notes\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMiscellaneous notes\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"/notes/85w31vcdf3bjnm0yxh72ygf\"\u003eHadoop\u003c/a\u003e and \u003ca href=\"/notes/f2kecna72pmc7re3wh1ugk4\"\u003eApache Spark\u003c/a\u003e usually work with \"INPUT DIRECTORIES\" not files. Note that the argument given to word count in the above example was a directory NOT the \"words.txt\" file. We can add more files to the input directory and \u003ccode\u003ewordcount\u003c/code\u003e will ingest all the files in finds.\u003c/p\u003e\n\u003ch2 id=\"simulating-map-and-reduce-using-python-and-bash-scripts\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#simulating-map-and-reduce-using-python-and-bash-scripts\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSimulating Map and Reduce using Python and Bash scripts\u003c/h2\u003e\n\u003cp\u003eSource: \u003ca href=\"https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\"\u003ehttps://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThere are several methods to program a Hadoop MapReduce. The most direct and lowest level is using the Java Hadoop API or the C++ Pipes library.\u003c/p\u003e\n\u003cp\u003eMany users prefer the flexibility of the Streams Interface that allows a variety of programming language to be used.\u003c/p\u003e\n\u003cp\u003eThis method will work with any program that can read and write TEXT DATA to \u003ca href=\"/notes/n4ernfsrloq7zjrscm8t5rc\"\u003eRedirecting stdin \u0026#x26; stderr\u003c/a\u003e and \u003ca href=\"/notes/do68y261xeu98bpq3vl5cn9\"\u003estdout\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"developing-a-hadoop-streaming-word-counting-application\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#developing-a-hadoop-streaming-word-counting-application\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eDeveloping a Hadoop Streaming word counting application.\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003epymapper.py - a python word count mapper\u003c/li\u003e\n\u003cli\u003eshuffer.sh - a simulated shuffle step using bash script\u003c/li\u003e\n\u003cli\u003epyreducer.py - a python word count reducer\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- end list --\u003e\n\u003cpre\u003e\u003ccode\u003e#!/usr/bin/env python\n\"\"\"pymapper.py\"\"\"\n\nimport sys\n\n# input comes from STDIN (standard input)\nfor line in sys.stdin:\n    # remove leading and trailing whitespace\n    line = line.strip()\n    # split the line into words\n    words = line.split()\n    # increase counters\n    for word in words:\n        # write the results to STDOUT (standard output);\n        # what we output here will be the input for the\n        # Reduce step, i.e. the input for reducer.py\n        #\n        # tab-delimited; the trivial word count is 1\n        print '%s\\t%s' % (word, 1)\n\n#!/bin/bash\n# shuffle.sh\nsort -k1,1\n\n#!/usr/bin/env python\n\"\"\"pyreducer.py\"\"\"\n\nfrom operator import itemgetter\nimport sys\n\ncurrent_word = None\ncurrent_count = 0\nword = None\n\n# input comes from STDIN\nfor line in sys.stdin:\n    # remove leading and trailing whitespace\n    line = line.strip()\n\n    # parse the input we got from mapper.py\n    word, count = line.split('\\t', 1)\n\n    # convert count (currently a string) to int\n    try:\n        count = int(count)\n    except ValueError:\n        # count was not a number, so silently\n        # ignore/discard this line\n        continue\n\n    # this IF-switch only works because Hadoop sorts map output\n    # by key (here: word) before it is passed to the reducer\n    if current_word == word:\n        current_count += count\n    else:\n        if current_word:\n            # write result to STDOUT\n            print ('%s\\t%s' % (current_word, current_count))\n        current_count = count\n        current_word = word\n\n# do not forget to output the last word if needed!\nif current_word == word:\n    print ('%s\\t%s' % (current_word, current_count))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e1. Run the \u003ccode\u003epymapper.py\u003c/code\u003e with some simple data and change the input words to key value pairs.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eecho \"see spot run run spot run see the cat\" | ./pymapper.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2. Sort the data passed through \u003ccode\u003epymapper.py\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eecho \"see spot run run spot run see the cat\" | ./pymapper.py | ./shuffle.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e3. Add the reducer stage and count all the same keys\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eecho \"see spot run run spot run see the cat\" | ./pymapper.py | ./shuffle.sh | ./pyreducer.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee also: \u003ca title=\"Private\" style=\"color: brown\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\"\u003eTez (Private)\u003c/a\u003e, \u003ca href=\"/notes/f2kecna72pmc7re3wh1ugk4\"\u003eApache Spark\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/f2kecna72pmc7re3wh1ugk4\"\u003eApache Spark\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/s5t9uscswvpo3p6gebui1ea\"\u003eBig Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/bojmgq30yfxpn44hgo0ofcf\"\u003eDAG\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/2fz1tdl3yy2s78wdmo7ql56\"\u003eData Engineering Roadmap\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/30isnfzmqrvmmiii03d0chj\"\u003eHive\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/e92qvq2zwoa9wyn0p0187l3\"\u003eHQL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/7v21vh269qxy069is0fkrzn\"\u003eTez\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"3nfl4nvv516muyzozhcwrw8","title":"/root","desc":"","updated":1655559901157,"created":1637610830605,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"Dendron"},"contentHash":"581715455a6f0f7a699209e8521b4acf","links":[{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"my","position":{"start":{"line":4,"column":9,"offset":37},"end":{"line":4,"column":29,"offset":57},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":20,"column":111,"offset":1051},"end":{"line":20,"column":117,"offset":1057},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"tags.areas","alias":"#areas","position":{"start":{"line":21,"column":3,"offset":1198},"end":{"line":21,"column":9,"offset":1204},"indent":[]},"xvault":false,"to":{"fname":"tags.areas"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes","alias":"swipes","position":{"start":{"line":27,"column":3,"offset":1724},"end":{"line":27,"column":13,"offset":1734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.quotes","alias":"quotes","position":{"start":{"line":27,"column":48,"offset":1769},"end":{"line":27,"column":72,"offset":1793},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.quotes"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.excerpts","alias":"excerpts","position":{"start":{"line":27,"column":74,"offset":1795},"end":{"line":27,"column":102,"offset":1823},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.excerpts"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.sayings","alias":"sayings","position":{"start":{"line":27,"column":104,"offset":1825},"end":{"line":27,"column":130,"offset":1851},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.sayings"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"swipes.phrases","alias":"phrases","position":{"start":{"line":27,"column":132,"offset":1853},"end":{"line":27,"column":158,"offset":1879},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"swipes.phrases"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"resources.people","alias":"others","position":{"start":{"line":27,"column":214,"offset":1935},"end":{"line":27,"column":241,"offset":1962},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"resources.people"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"inbox.webmark","alias":"webmark","position":{"start":{"line":31,"column":235,"offset":2463},"end":{"line":31,"column":260,"offset":2488},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"inbox.webmark"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"slipbox.Ontology","alias":"slipbox.Ontology","position":{"start":{"line":55,"column":3,"offset":3735},"end":{"line":55,"column":23,"offset":3755},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"slipbox.Ontology"}},{"type":"wiki","from":{"fname":"root","id":"3nfl4nvv516muyzozhcwrw8","vaultName":"Dendron"},"value":"archive.about","alias":"About me","position":{"start":{"line":60,"column":3,"offset":3963},"end":{"line":60,"column":29,"offset":3989},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"archive.about"}}],"anchors":{"welcome-to-noetic-noggin":{"type":"header","text":"Welcome to Noetic Noggin","value":"welcome-to-noetic-noggin","line":8,"column":0,"depth":1},"principles":{"type":"header","text":"Principles","value":"principles","line":18,"column":0,"depth":2},"all-notes-should-be-relative-to-me":{"type":"header","text":"All notes should be relative to me.","value":"all-notes-should-be-relative-to-me","line":20,"column":0,"depth":3},"gotta-capture-em-all":{"type":"header","text":"Gotta capture 'em all","value":"gotta-capture-em-all","line":30,"column":0,"depth":3},"dont-force-evolution":{"type":"header","text":"Don't force evolution","value":"dont-force-evolution","line":40,"column":0,"depth":3},"noise--signal":{"type":"header","text":"Noise \u0026 Signal","value":"noise--signal","line":45,"column":0,"depth":3},"why-do-any-of-this":{"type":"header","text":"Why do any of this?","value":"why-do-any-of-this","line":50,"column":0,"depth":3},"structure-of-this-wiki":{"type":"header","text":"Structure of this wiki","value":"structure-of-this-wiki","line":59,"column":0,"depth":2},"quicklinks":{"type":"header","text":"Quicklinks","value":"quicklinks","line":64,"column":0,"depth":2}},"children":["0yay2om15bsg2li2p6qgux7","05c4nnjqa92zx11ld6o0ytn","9gtn7g40cvqui0sifl1s7t5","ftbd1hknsd3ocd7jao26tn3","a1kmkdbpclaz5p6sykaw6kc","z121gkmqfo09m8r7jgnpfgn","gkqrr7xbt18xhi93dmjrwzj","ja2x4lrgejr9o9wvit0bd0d","luv39odkfibx3wdosvigwvy","vtvk3bi6o72w58oima9xzf3","yy652kvqrkfn9ipk07m40h4"],"parent":null,"data":{},"body":"\n# Welcome to Noetic Noggin\n\nThis is [[my|archive.about]] personal wiki and a commonplace book; notes by me, for me.\n\n🚧 Permanently under construction 🚧\n\n![](https://res.cloudinary.com/zubayr/image/upload/v1658499909/wiki/ajevkuyebljlxiblyst2.png)\n\nThis wiki was made possible with [dendron.so](https://dendron.so) and [obisidian.md](https://obsidian.md). Stored on [Github Repository](https://github.com/zubayrrr/dendron) and hosted on [Netlify](https://netlify.com) for free.\n\n## Principles\n\n### All notes should be relative to me.\n\n- All notes in principle are written for me; what I know about a subject, how I feel about a particular thing.\n- Opinions are fine as long as I feel strong epistemic confidence in the given opinion.\n- Don't over explain a note if it's not necessary, remember, these notes are for you and are relative to whatever knowledge you posses about the subject.\n- Read books and make an dedicated notes for them.\n- Listen podcasts but capture them inside a \"subject specific\" note or \"Map of Concept\" note or a note tagged #areas. Because making notes from podcasts can be tedious as they're not as well structured as books for consumption.(Whose merit is debatable.)\n- #areas are basically \"Map of Concept\" notes but I have recently come to the realization that its better to maintain them [Nikita Voloboev style](https://wiki.nikiv.dev/) but with heavy usage of transclusion and backlinking.\n  - \"Resources\" should be first processed and then mentioned inside the note, otherwise they should be left in inbox.\n\n### Gotta capture 'em all\n\n- Hog whatever information tickles your pickle([anything that gratifies one's intellectual curiosity](https://news.ycombinator.com/newsguidelines.html)).\n- [[swipes]] are interesting/useful bits of... [[quotes|swipes.quotes]], [[excerpts|swipes.excerpts]], [[sayings|swipes.sayings]], [[phrases|swipes.phrases]]. Essentially, ideas, opinions that are swiped off from [[others|resources.people]].\n- Make no distinction between \"your\" ideas and ideas of \"others\", because if you vibe with an idea; it's already yours.\n- But also remember \"If you've time to consume, you've time to produce.\".\n- Use [raindrop.io](https://raindrop.io) to manage your URL bookmarks.\n- If you need to bookmark a webpage or an article all together, use [MarkDownload](https://chrome.google.com/webstore/detail/markdownload-markdown-web/pcmpcfapbekmbjjkdalcgopdkipoggdi?hl=en-GB) to rip the entire page. Let's call it a [[webmark|inbox.webmark]]; it belong in the `/inbox`.\n- Similar process is employed for capturing tweets using [tweet-to-markdown](https://github.com/kbravh/tweet-to-markdown) and it also belongs in the `/inbox`.\n\n### Don't force evolution\n\n- Let your second brain evolve at it's own pace.\n- The structure should never be _too_ rigid because its meant to take form by itself.\n\n### Noise \u0026 Signal\n\n- While capturing ideas left and right is recommended, make sure you're not harming your periods of focus.\n- Have impenetrable focus periods (use Pomodoro method) where you only care about the work on hand and nothing else.\n\n### Why do any of this?\n\n- Because I can't remember everything - there's a lot of information around that interests me and there isn't enough working memory installed in me.\n- So, I make notes - to remember, to create, to meditate, to think.\n- A bodybuilder's portfolio is their body - my portfolio is my wiki.\n- Not only am I making - whatever I know - tangible by writing it down. I know exactly where to look if I ever forget something.\n- I am at the beginning of my learning adventures. When I look back at it, I will know where I came from and how my thoughts evolved over time.\n- Plus, its really fun to nerd out.\n\n## Structure of this wiki\n\n- [[slipbox.Ontology]] explains the structure of this wiki and the tags, backlinks used in it.\n- Dendron takes care of the structure and hierarchy(mostly), but I insist on using tags for backwards compatibility.\n\n## Quicklinks\n\n- [[About me|archive.about]]\n- [Github](https://github.com/zubayrrr)\n- [Twitter](https://twitter.com/zoobhalu)\n- [Blog](https://zubayrali.in)\n- [Guestbook](https://www.yourworldoftext.com/~zubayrali/)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template","insertNote":{"initialValue":"templates"}},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"Dendron"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Noetic Noggin","description":"Personal Wiki / Digital Garden","author":"Zubayr Ali","twitter":"zoobhalu"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteUrl":"localhost:3000","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"j6kfx6ziqg91r356nur9cwy"},"buildId":"4oDSInv8WeDX6cVSprjyl","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>